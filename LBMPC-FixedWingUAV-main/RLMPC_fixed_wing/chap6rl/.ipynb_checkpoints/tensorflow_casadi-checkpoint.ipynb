{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:46:07.200552Z",
     "start_time": "2021-01-18T10:46:07.184872Z"
    }
   },
   "outputs": [],
   "source": [
    "from casadi import*\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:43:28.150489Z",
     "start_time": "2021-01-18T10:43:26.983456Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T12:50:31.027497Z",
     "start_time": "2021-01-23T12:50:30.001931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevator(radian)</th>\n",
       "      <th>thrustf1</th>\n",
       "      <th>thrustf2</th>\n",
       "      <th>u</th>\n",
       "      <th>w</th>\n",
       "      <th>q(radian/s)</th>\n",
       "      <th>theta(radian)</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>24.837090</td>\n",
       "      <td>1.620222</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.063913</td>\n",
       "      <td>99.999790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999997</td>\n",
       "      <td>19.999997</td>\n",
       "      <td>24.857969</td>\n",
       "      <td>1.505181</td>\n",
       "      <td>-0.903366</td>\n",
       "      <td>0.060313</td>\n",
       "      <td>99.999508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999996</td>\n",
       "      <td>19.999996</td>\n",
       "      <td>24.887396</td>\n",
       "      <td>1.222918</td>\n",
       "      <td>-1.739143</td>\n",
       "      <td>0.049742</td>\n",
       "      <td>99.999409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999995</td>\n",
       "      <td>19.999995</td>\n",
       "      <td>24.919247</td>\n",
       "      <td>0.792520</td>\n",
       "      <td>-2.505189</td>\n",
       "      <td>0.032765</td>\n",
       "      <td>99.999417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999994</td>\n",
       "      <td>19.999994</td>\n",
       "      <td>24.945717</td>\n",
       "      <td>0.232311</td>\n",
       "      <td>-3.202840</td>\n",
       "      <td>0.009933</td>\n",
       "      <td>99.999414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999992</td>\n",
       "      <td>19.999992</td>\n",
       "      <td>24.958255</td>\n",
       "      <td>-0.439982</td>\n",
       "      <td>-3.833588</td>\n",
       "      <td>-0.018213</td>\n",
       "      <td>99.999243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999990</td>\n",
       "      <td>19.999990</td>\n",
       "      <td>24.948333</td>\n",
       "      <td>-1.207096</td>\n",
       "      <td>-4.399042</td>\n",
       "      <td>-0.051143</td>\n",
       "      <td>99.998716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999987</td>\n",
       "      <td>19.999987</td>\n",
       "      <td>24.908056</td>\n",
       "      <td>-2.052183</td>\n",
       "      <td>-4.900914</td>\n",
       "      <td>-0.088343</td>\n",
       "      <td>99.997618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999984</td>\n",
       "      <td>19.999984</td>\n",
       "      <td>24.830630</td>\n",
       "      <td>-2.958820</td>\n",
       "      <td>-5.341024</td>\n",
       "      <td>-0.129311</td>\n",
       "      <td>99.995711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999979</td>\n",
       "      <td>19.999979</td>\n",
       "      <td>24.710702</td>\n",
       "      <td>-3.911101</td>\n",
       "      <td>-5.721307</td>\n",
       "      <td>-0.173560</td>\n",
       "      <td>99.992738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999973</td>\n",
       "      <td>19.999973</td>\n",
       "      <td>24.544577</td>\n",
       "      <td>-4.893769</td>\n",
       "      <td>-6.043834</td>\n",
       "      <td>-0.220621</td>\n",
       "      <td>99.988433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.898630</td>\n",
       "      <td>19.999968</td>\n",
       "      <td>19.999968</td>\n",
       "      <td>24.333856</td>\n",
       "      <td>-5.878587</td>\n",
       "      <td>-6.188694</td>\n",
       "      <td>-0.269551</td>\n",
       "      <td>99.982512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.844726</td>\n",
       "      <td>19.999964</td>\n",
       "      <td>19.999964</td>\n",
       "      <td>24.129501</td>\n",
       "      <td>-6.685311</td>\n",
       "      <td>-4.869081</td>\n",
       "      <td>-0.313782</td>\n",
       "      <td>99.974601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.997444</td>\n",
       "      <td>19.999958</td>\n",
       "      <td>19.999958</td>\n",
       "      <td>23.982843</td>\n",
       "      <td>-7.203378</td>\n",
       "      <td>-3.509622</td>\n",
       "      <td>-0.347297</td>\n",
       "      <td>99.964371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.682204</td>\n",
       "      <td>19.999950</td>\n",
       "      <td>19.999950</td>\n",
       "      <td>23.898958</td>\n",
       "      <td>-7.482665</td>\n",
       "      <td>-2.496990</td>\n",
       "      <td>-0.371323</td>\n",
       "      <td>99.951690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.445234</td>\n",
       "      <td>19.999940</td>\n",
       "      <td>19.999940</td>\n",
       "      <td>23.866976</td>\n",
       "      <td>-7.589612</td>\n",
       "      <td>-1.744851</td>\n",
       "      <td>-0.388290</td>\n",
       "      <td>99.936513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.325041</td>\n",
       "      <td>19.999929</td>\n",
       "      <td>19.999929</td>\n",
       "      <td>23.876613</td>\n",
       "      <td>-7.567711</td>\n",
       "      <td>-1.143053</td>\n",
       "      <td>-0.399842</td>\n",
       "      <td>99.918831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.272491</td>\n",
       "      <td>19.999916</td>\n",
       "      <td>19.999916</td>\n",
       "      <td>23.920198</td>\n",
       "      <td>-7.442627</td>\n",
       "      <td>-0.629080</td>\n",
       "      <td>-0.406931</td>\n",
       "      <td>99.898662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.250518</td>\n",
       "      <td>19.999902</td>\n",
       "      <td>19.999902</td>\n",
       "      <td>23.991531</td>\n",
       "      <td>-7.230716</td>\n",
       "      <td>-0.174442</td>\n",
       "      <td>-0.410145</td>\n",
       "      <td>99.876045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.240110</td>\n",
       "      <td>19.999886</td>\n",
       "      <td>19.999885</td>\n",
       "      <td>24.084966</td>\n",
       "      <td>-6.944102</td>\n",
       "      <td>0.232328</td>\n",
       "      <td>-0.409913</td>\n",
       "      <td>99.851038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.233708</td>\n",
       "      <td>19.999867</td>\n",
       "      <td>19.999866</td>\n",
       "      <td>24.195073</td>\n",
       "      <td>-6.593013</td>\n",
       "      <td>0.596064</td>\n",
       "      <td>-0.406599</td>\n",
       "      <td>99.823721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.229747</td>\n",
       "      <td>19.999843</td>\n",
       "      <td>19.999843</td>\n",
       "      <td>24.316662</td>\n",
       "      <td>-6.186626</td>\n",
       "      <td>0.920234</td>\n",
       "      <td>-0.400534</td>\n",
       "      <td>99.794186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.229104</td>\n",
       "      <td>19.999812</td>\n",
       "      <td>19.999811</td>\n",
       "      <td>24.444943</td>\n",
       "      <td>-5.733187</td>\n",
       "      <td>1.209015</td>\n",
       "      <td>-0.392017</td>\n",
       "      <td>99.762544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.232827</td>\n",
       "      <td>19.999766</td>\n",
       "      <td>19.999765</td>\n",
       "      <td>24.575649</td>\n",
       "      <td>-5.239941</td>\n",
       "      <td>1.467379</td>\n",
       "      <td>-0.381312</td>\n",
       "      <td>99.728915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.240545</td>\n",
       "      <td>19.999686</td>\n",
       "      <td>19.999685</td>\n",
       "      <td>24.705070</td>\n",
       "      <td>-4.713151</td>\n",
       "      <td>1.699865</td>\n",
       "      <td>-0.368643</td>\n",
       "      <td>99.693430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.249012</td>\n",
       "      <td>19.999507</td>\n",
       "      <td>19.999504</td>\n",
       "      <td>24.829976</td>\n",
       "      <td>-4.158483</td>\n",
       "      <td>1.908188</td>\n",
       "      <td>-0.354210</td>\n",
       "      <td>99.656231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.250199</td>\n",
       "      <td>19.998684</td>\n",
       "      <td>19.998670</td>\n",
       "      <td>24.947461</td>\n",
       "      <td>-3.582023</td>\n",
       "      <td>2.087379</td>\n",
       "      <td>-0.338228</td>\n",
       "      <td>99.617466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.228579</td>\n",
       "      <td>17.041343</td>\n",
       "      <td>17.017016</td>\n",
       "      <td>25.051405</td>\n",
       "      <td>-2.992891</td>\n",
       "      <td>2.218383</td>\n",
       "      <td>-0.321005</td>\n",
       "      <td>99.577297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.158910</td>\n",
       "      <td>9.520442</td>\n",
       "      <td>9.507820</td>\n",
       "      <td>25.134073</td>\n",
       "      <td>-2.406019</td>\n",
       "      <td>2.263881</td>\n",
       "      <td>-0.303076</td>\n",
       "      <td>99.535911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>25.191336</td>\n",
       "      <td>-1.845756</td>\n",
       "      <td>2.163346</td>\n",
       "      <td>-0.285367</td>\n",
       "      <td>99.493507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    elevator(radian)   thrustf1   thrustf2          u         w  q(radian/s)  \\\n",
       "0           0.000000   6.000000   6.000000  24.837090  1.620222     0.003248   \n",
       "1           1.047198  19.999997  19.999997  24.857969  1.505181    -0.903366   \n",
       "2           1.047198  19.999996  19.999996  24.887396  1.222918    -1.739143   \n",
       "3           1.047198  19.999995  19.999995  24.919247  0.792520    -2.505189   \n",
       "4           1.047198  19.999994  19.999994  24.945717  0.232311    -3.202840   \n",
       "5           1.047198  19.999992  19.999992  24.958255 -0.439982    -3.833588   \n",
       "6           1.047198  19.999990  19.999990  24.948333 -1.207096    -4.399042   \n",
       "7           1.047197  19.999987  19.999987  24.908056 -2.052183    -4.900914   \n",
       "8           1.047197  19.999984  19.999984  24.830630 -2.958820    -5.341024   \n",
       "9           1.047197  19.999979  19.999979  24.710702 -3.911101    -5.721307   \n",
       "10          1.047197  19.999973  19.999973  24.544577 -4.893769    -6.043834   \n",
       "11          0.898630  19.999968  19.999968  24.333856 -5.878587    -6.188694   \n",
       "12         -0.844726  19.999964  19.999964  24.129501 -6.685311    -4.869081   \n",
       "13         -0.997444  19.999958  19.999958  23.982843 -7.203378    -3.509622   \n",
       "14         -0.682204  19.999950  19.999950  23.898958 -7.482665    -2.496990   \n",
       "15         -0.445234  19.999940  19.999940  23.866976 -7.589612    -1.744851   \n",
       "16         -0.325041  19.999929  19.999929  23.876613 -7.567711    -1.143053   \n",
       "17         -0.272491  19.999916  19.999916  23.920198 -7.442627    -0.629080   \n",
       "18         -0.250518  19.999902  19.999902  23.991531 -7.230716    -0.174442   \n",
       "19         -0.240110  19.999886  19.999885  24.084966 -6.944102     0.232328   \n",
       "20         -0.233708  19.999867  19.999866  24.195073 -6.593013     0.596064   \n",
       "21         -0.229747  19.999843  19.999843  24.316662 -6.186626     0.920234   \n",
       "22         -0.229104  19.999812  19.999811  24.444943 -5.733187     1.209015   \n",
       "23         -0.232827  19.999766  19.999765  24.575649 -5.239941     1.467379   \n",
       "24         -0.240545  19.999686  19.999685  24.705070 -4.713151     1.699865   \n",
       "25         -0.249012  19.999507  19.999504  24.829976 -4.158483     1.908188   \n",
       "26         -0.250199  19.998684  19.998670  24.947461 -3.582023     2.087379   \n",
       "27         -0.228579  17.041343  17.017016  25.051405 -2.992891     2.218383   \n",
       "28         -0.158910   9.520442   9.507820  25.134073 -2.406019     2.263881   \n",
       "29          0.000000   0.169645   0.169645  25.191336 -1.845756     2.163346   \n",
       "\n",
       "    theta(radian)     height  \n",
       "0        0.063913  99.999790  \n",
       "1        0.060313  99.999508  \n",
       "2        0.049742  99.999409  \n",
       "3        0.032765  99.999417  \n",
       "4        0.009933  99.999414  \n",
       "5       -0.018213  99.999243  \n",
       "6       -0.051143  99.998716  \n",
       "7       -0.088343  99.997618  \n",
       "8       -0.129311  99.995711  \n",
       "9       -0.173560  99.992738  \n",
       "10      -0.220621  99.988433  \n",
       "11      -0.269551  99.982512  \n",
       "12      -0.313782  99.974601  \n",
       "13      -0.347297  99.964371  \n",
       "14      -0.371323  99.951690  \n",
       "15      -0.388290  99.936513  \n",
       "16      -0.399842  99.918831  \n",
       "17      -0.406931  99.898662  \n",
       "18      -0.410145  99.876045  \n",
       "19      -0.409913  99.851038  \n",
       "20      -0.406599  99.823721  \n",
       "21      -0.400534  99.794186  \n",
       "22      -0.392017  99.762544  \n",
       "23      -0.381312  99.728915  \n",
       "24      -0.368643  99.693430  \n",
       "25      -0.354210  99.656231  \n",
       "26      -0.338228  99.617466  \n",
       "27      -0.321005  99.577297  \n",
       "28      -0.303076  99.535911  \n",
       "29      -0.285367  99.493507  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "system_data = pd.read_csv('system_data.csv') \n",
    "#data.to_csv(\"vtol_data.csv\", header=[\"delta_e\", \"delta_tf1\", \"delta_tf2\",\"u\", \"w\", \"q\",\"theta\", \"height\"], index=False)\n",
    "#data.to_csv(\"system_data.csv\", header=[\"elevator\",\"tf1\",\"tf2\",\"u\",\"w\",  \"q\",\"theta\", \"height\" ], index=False)\n",
    "system_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T12:51:55.614691Z",
     "start_time": "2021-01-23T12:51:55.589201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevator(radian)</th>\n",
       "      <th>thrustf1</th>\n",
       "      <th>thrustf2</th>\n",
       "      <th>u</th>\n",
       "      <th>w</th>\n",
       "      <th>q(radian/s)</th>\n",
       "      <th>theta(radian)</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.880791e-37</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999997</td>\n",
       "      <td>19.999997</td>\n",
       "      <td>25.023323</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>-3.212336e-03</td>\n",
       "      <td>100.117885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999996</td>\n",
       "      <td>19.999996</td>\n",
       "      <td>25.275402</td>\n",
       "      <td>-0.191817</td>\n",
       "      <td>-0.849133</td>\n",
       "      <td>-9.038246e-03</td>\n",
       "      <td>99.983279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999995</td>\n",
       "      <td>19.999995</td>\n",
       "      <td>25.560125</td>\n",
       "      <td>-0.741759</td>\n",
       "      <td>-1.551485</td>\n",
       "      <td>-2.330760e-02</td>\n",
       "      <td>99.666646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999994</td>\n",
       "      <td>19.999994</td>\n",
       "      <td>25.894711</td>\n",
       "      <td>-1.381820</td>\n",
       "      <td>-2.104490</td>\n",
       "      <td>-4.468255e-02</td>\n",
       "      <td>99.128755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999992</td>\n",
       "      <td>19.999992</td>\n",
       "      <td>26.277970</td>\n",
       "      <td>-2.038750</td>\n",
       "      <td>-2.518448</td>\n",
       "      <td>-7.152977e-02</td>\n",
       "      <td>98.377192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.047198</td>\n",
       "      <td>19.999990</td>\n",
       "      <td>19.999990</td>\n",
       "      <td>26.695048</td>\n",
       "      <td>-2.666052</td>\n",
       "      <td>-2.813697</td>\n",
       "      <td>-1.022155e-01</td>\n",
       "      <td>97.458461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999987</td>\n",
       "      <td>19.999987</td>\n",
       "      <td>27.125607</td>\n",
       "      <td>-3.240381</td>\n",
       "      <td>-3.014749</td>\n",
       "      <td>-1.353300e-01</td>\n",
       "      <td>96.436210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999984</td>\n",
       "      <td>19.999984</td>\n",
       "      <td>27.550513</td>\n",
       "      <td>-3.754009</td>\n",
       "      <td>-3.145190</td>\n",
       "      <td>-1.697757e-01</td>\n",
       "      <td>95.371892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999979</td>\n",
       "      <td>19.999979</td>\n",
       "      <td>27.955191</td>\n",
       "      <td>-4.207835</td>\n",
       "      <td>-3.224922</td>\n",
       "      <td>-2.047536e-01</td>\n",
       "      <td>94.314486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.047197</td>\n",
       "      <td>19.999973</td>\n",
       "      <td>19.999973</td>\n",
       "      <td>28.330257</td>\n",
       "      <td>-4.606636</td>\n",
       "      <td>-3.269410</td>\n",
       "      <td>-2.397037e-01</td>\n",
       "      <td>93.298043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.898630</td>\n",
       "      <td>19.999968</td>\n",
       "      <td>19.999968</td>\n",
       "      <td>28.670761</td>\n",
       "      <td>-4.956423</td>\n",
       "      <td>-3.290055</td>\n",
       "      <td>-2.742399e-01</td>\n",
       "      <td>92.343479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.844726</td>\n",
       "      <td>19.999964</td>\n",
       "      <td>19.999964</td>\n",
       "      <td>28.937030</td>\n",
       "      <td>-5.246327</td>\n",
       "      <td>-3.198933</td>\n",
       "      <td>-3.096080e-01</td>\n",
       "      <td>91.521007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.997444</td>\n",
       "      <td>19.999958</td>\n",
       "      <td>19.999958</td>\n",
       "      <td>28.785526</td>\n",
       "      <td>-5.154781</td>\n",
       "      <td>-1.906724</td>\n",
       "      <td>-3.592981e-01</td>\n",
       "      <td>91.250101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.682204</td>\n",
       "      <td>19.999950</td>\n",
       "      <td>19.999950</td>\n",
       "      <td>28.644065</td>\n",
       "      <td>-4.742896</td>\n",
       "      <td>-0.721728</td>\n",
       "      <td>-4.006583e-01</td>\n",
       "      <td>91.085842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.445234</td>\n",
       "      <td>19.999940</td>\n",
       "      <td>19.999940</td>\n",
       "      <td>28.598270</td>\n",
       "      <td>-4.150331</td>\n",
       "      <td>0.033911</td>\n",
       "      <td>-4.285941e-01</td>\n",
       "      <td>90.946046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.325041</td>\n",
       "      <td>19.999929</td>\n",
       "      <td>19.999929</td>\n",
       "      <td>28.613071</td>\n",
       "      <td>-3.472815</td>\n",
       "      <td>0.483422</td>\n",
       "      <td>-4.459224e-01</td>\n",
       "      <td>90.827847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.272491</td>\n",
       "      <td>19.999916</td>\n",
       "      <td>19.999916</td>\n",
       "      <td>28.653511</td>\n",
       "      <td>-2.765619</td>\n",
       "      <td>0.760577</td>\n",
       "      <td>-4.557234e-01</td>\n",
       "      <td>90.730312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.250518</td>\n",
       "      <td>19.999902</td>\n",
       "      <td>19.999902</td>\n",
       "      <td>28.699719</td>\n",
       "      <td>-2.060409</td>\n",
       "      <td>0.942181</td>\n",
       "      <td>-4.600346e-01</td>\n",
       "      <td>90.652338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.240110</td>\n",
       "      <td>19.999886</td>\n",
       "      <td>19.999885</td>\n",
       "      <td>28.742031</td>\n",
       "      <td>-1.376594</td>\n",
       "      <td>1.065528</td>\n",
       "      <td>-4.601422e-01</td>\n",
       "      <td>90.593071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.233708</td>\n",
       "      <td>19.999867</td>\n",
       "      <td>19.999866</td>\n",
       "      <td>28.776242</td>\n",
       "      <td>-0.726970</td>\n",
       "      <td>1.148092</td>\n",
       "      <td>-4.569390e-01</td>\n",
       "      <td>90.551234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.229747</td>\n",
       "      <td>19.999843</td>\n",
       "      <td>19.999843</td>\n",
       "      <td>28.800807</td>\n",
       "      <td>-0.119872</td>\n",
       "      <td>1.199299</td>\n",
       "      <td>-4.511224e-01</td>\n",
       "      <td>90.524823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.229104</td>\n",
       "      <td>19.999812</td>\n",
       "      <td>19.999811</td>\n",
       "      <td>28.815284</td>\n",
       "      <td>0.440196</td>\n",
       "      <td>1.226313</td>\n",
       "      <td>-4.432717e-01</td>\n",
       "      <td>90.511477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.232827</td>\n",
       "      <td>19.999766</td>\n",
       "      <td>19.999765</td>\n",
       "      <td>28.819522</td>\n",
       "      <td>0.952218</td>\n",
       "      <td>1.236167</td>\n",
       "      <td>-4.338630e-01</td>\n",
       "      <td>90.509172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.240545</td>\n",
       "      <td>19.999686</td>\n",
       "      <td>19.999685</td>\n",
       "      <td>28.813349</td>\n",
       "      <td>1.418061</td>\n",
       "      <td>1.235784</td>\n",
       "      <td>-4.232658e-01</td>\n",
       "      <td>90.516766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.249012</td>\n",
       "      <td>19.999507</td>\n",
       "      <td>19.999504</td>\n",
       "      <td>28.796676</td>\n",
       "      <td>1.841347</td>\n",
       "      <td>1.230776</td>\n",
       "      <td>-4.117441e-01</td>\n",
       "      <td>90.534018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.250199</td>\n",
       "      <td>19.998684</td>\n",
       "      <td>19.998670</td>\n",
       "      <td>28.770061</td>\n",
       "      <td>2.225626</td>\n",
       "      <td>1.223298</td>\n",
       "      <td>-3.994750e-01</td>\n",
       "      <td>90.560807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.228579</td>\n",
       "      <td>17.041343</td>\n",
       "      <td>17.017016</td>\n",
       "      <td>28.735858</td>\n",
       "      <td>2.571505</td>\n",
       "      <td>1.208769</td>\n",
       "      <td>-3.865934e-01</td>\n",
       "      <td>90.595233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.158910</td>\n",
       "      <td>9.520442</td>\n",
       "      <td>9.507820</td>\n",
       "      <td>28.696071</td>\n",
       "      <td>2.871426</td>\n",
       "      <td>1.172362</td>\n",
       "      <td>-3.720398e-01</td>\n",
       "      <td>90.654199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>0.169645</td>\n",
       "      <td>28.656544</td>\n",
       "      <td>3.104192</td>\n",
       "      <td>1.082223</td>\n",
       "      <td>-3.542735e-01</td>\n",
       "      <td>90.758703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    elevator(radian)   thrustf1   thrustf2          u         w  q(radian/s)  \\\n",
       "0           0.000000   6.000000   6.000000  25.000000  0.100000     0.000000   \n",
       "1           1.047198  19.999997  19.999997  25.023323  0.173100     0.000433   \n",
       "2           1.047198  19.999996  19.999996  25.275402 -0.191817    -0.849133   \n",
       "3           1.047198  19.999995  19.999995  25.560125 -0.741759    -1.551485   \n",
       "4           1.047198  19.999994  19.999994  25.894711 -1.381820    -2.104490   \n",
       "5           1.047198  19.999992  19.999992  26.277970 -2.038750    -2.518448   \n",
       "6           1.047198  19.999990  19.999990  26.695048 -2.666052    -2.813697   \n",
       "7           1.047197  19.999987  19.999987  27.125607 -3.240381    -3.014749   \n",
       "8           1.047197  19.999984  19.999984  27.550513 -3.754009    -3.145190   \n",
       "9           1.047197  19.999979  19.999979  27.955191 -4.207835    -3.224922   \n",
       "10          1.047197  19.999973  19.999973  28.330257 -4.606636    -3.269410   \n",
       "11          0.898630  19.999968  19.999968  28.670761 -4.956423    -3.290055   \n",
       "12         -0.844726  19.999964  19.999964  28.937030 -5.246327    -3.198933   \n",
       "13         -0.997444  19.999958  19.999958  28.785526 -5.154781    -1.906724   \n",
       "14         -0.682204  19.999950  19.999950  28.644065 -4.742896    -0.721728   \n",
       "15         -0.445234  19.999940  19.999940  28.598270 -4.150331     0.033911   \n",
       "16         -0.325041  19.999929  19.999929  28.613071 -3.472815     0.483422   \n",
       "17         -0.272491  19.999916  19.999916  28.653511 -2.765619     0.760577   \n",
       "18         -0.250518  19.999902  19.999902  28.699719 -2.060409     0.942181   \n",
       "19         -0.240110  19.999886  19.999885  28.742031 -1.376594     1.065528   \n",
       "20         -0.233708  19.999867  19.999866  28.776242 -0.726970     1.148092   \n",
       "21         -0.229747  19.999843  19.999843  28.800807 -0.119872     1.199299   \n",
       "22         -0.229104  19.999812  19.999811  28.815284  0.440196     1.226313   \n",
       "23         -0.232827  19.999766  19.999765  28.819522  0.952218     1.236167   \n",
       "24         -0.240545  19.999686  19.999685  28.813349  1.418061     1.235784   \n",
       "25         -0.249012  19.999507  19.999504  28.796676  1.841347     1.230776   \n",
       "26         -0.250199  19.998684  19.998670  28.770061  2.225626     1.223298   \n",
       "27         -0.228579  17.041343  17.017016  28.735858  2.571505     1.208769   \n",
       "28         -0.158910   9.520442   9.507820  28.696071  2.871426     1.172362   \n",
       "29          0.000000   0.169645   0.169645  28.656544  3.104192     1.082223   \n",
       "\n",
       "    theta(radian)      height  \n",
       "0   -1.880791e-37  100.000000  \n",
       "1   -3.212336e-03  100.117885  \n",
       "2   -9.038246e-03   99.983279  \n",
       "3   -2.330760e-02   99.666646  \n",
       "4   -4.468255e-02   99.128755  \n",
       "5   -7.152977e-02   98.377192  \n",
       "6   -1.022155e-01   97.458461  \n",
       "7   -1.353300e-01   96.436210  \n",
       "8   -1.697757e-01   95.371892  \n",
       "9   -2.047536e-01   94.314486  \n",
       "10  -2.397037e-01   93.298043  \n",
       "11  -2.742399e-01   92.343479  \n",
       "12  -3.096080e-01   91.521007  \n",
       "13  -3.592981e-01   91.250101  \n",
       "14  -4.006583e-01   91.085842  \n",
       "15  -4.285941e-01   90.946046  \n",
       "16  -4.459224e-01   90.827847  \n",
       "17  -4.557234e-01   90.730312  \n",
       "18  -4.600346e-01   90.652338  \n",
       "19  -4.601422e-01   90.593071  \n",
       "20  -4.569390e-01   90.551234  \n",
       "21  -4.511224e-01   90.524823  \n",
       "22  -4.432717e-01   90.511477  \n",
       "23  -4.338630e-01   90.509172  \n",
       "24  -4.232658e-01   90.516766  \n",
       "25  -4.117441e-01   90.534018  \n",
       "26  -3.994750e-01   90.560807  \n",
       "27  -3.865934e-01   90.595233  \n",
       "28  -3.720398e-01   90.654199  \n",
       "29  -3.542735e-01   90.758703  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpc_data = pd.read_csv('mpc_data.csv') \n",
    "\n",
    "mpc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:24:47.186340Z",
     "start_time": "2021-01-17T18:24:45.966997Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "json_file = open('model_next_simple.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_next_simple.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\",  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:28:28.002569Z",
     "start_time": "2021-01-19T07:28:27.126875Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data.csv') \n",
    "#data.to_csv(\"vtol_data.csv\", header=[\"delta_e\", \"delta_tf1\", \"delta_tf2\",\"u\", \"w\", \"q\",\"theta\", \"height\"], index=False)\n",
    "#data.to_csv(\"index_data.csv\", header=[\"prev_u\",\"prev_w\",  \"prev_q\",\"prev_theta\", \"prev_height\",\"elevator\", \"tf1\",\"tf2\",\"u\",\"w\",  \"q\",\"theta\", \"height\"], index=False)\n",
    "#data = pd.read_csv('index_data.csv')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#v = preprocessing.scale(data)\n",
    "v=data.to_numpy();\n",
    "Xtrain = v[0:1000,0:8]\n",
    "Ytrain = v[0:1000,8:]\n",
    "Xtest = v[2000:2100,0:8]\n",
    "Ytest = v[2000:2100,8:]\n",
    "from sklearn import preprocessing\n",
    "scaler_x  = preprocessing.StandardScaler().fit(Xtrain)\n",
    "scaler_y = preprocessing.StandardScaler().fit(Ytrain)\n",
    "print(\"x_mean\",scaler_x.mean_)\n",
    "print(\"x_scale\",scaler_x.scale_)\n",
    "print(\"y_mean\",scaler_y.mean_)\n",
    "print(\"y_scale\",scaler_y.scale_)\n",
    "X_train_scaled = scaler_x.transform(Xtrain)\n",
    "Y_train_scaled = scaler_y.transform(Ytrain)\n",
    "X_test_scaled = scaler_x.transform(Xtest)\n",
    "Y_test_scaled = scaler_y.transform(Ytest)\n",
    "y_predic = loaded_model.predict(X_test_scaled)\n",
    "for i in range(20):\n",
    "    print(\"predicted output\", np.multiply(scaler_y.scale_, y_predic[0]) + scaler_y.mean_)\n",
    "    #print(np.multiply(scaler_y.scale_, Y_train_scaled[0]) + scaler_y.mean_)\n",
    "    print(\"target= \" , Ytest[i])\n",
    "    print(\"***********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:28:59.438463Z",
     "start_time": "2021-01-19T07:28:58.814410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_mean [ 2.62093608e+01  1.07930032e+00 -1.15915999e-02  3.36124745e-02\n",
      "  1.02489054e+02 -2.37640851e-02  8.80369495e+00  8.80369495e+00]\n",
      "x_scale [3.31746745 2.23230049 0.45922519 0.2943049  7.69364428 0.11576688\n",
      " 9.03824927 9.03824927]\n",
      "y_mean [ 2.62101511e+01  1.07905936e+00 -1.34161592e-02  3.35119831e-02\n",
      "  1.02488059e+02]\n",
      "y_scale [3.318481   2.23180528 0.45831533 0.29437104 7.69570136]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler_x  = preprocessing.StandardScaler().fit(Xtrain)\n",
    "scaler_y = preprocessing.StandardScaler().fit(Ytrain)\n",
    "print(\"x_mean\",scaler_x.mean_)\n",
    "print(\"x_scale\",scaler_x.scale_)\n",
    "print(\"y_mean\",scaler_y.mean_)\n",
    "print(\"y_scale\",scaler_y.scale_)\n",
    "X_train_scaled = scaler_x.transform(Xtrain)\n",
    "Y_train_scaled = scaler_y.transform(Ytrain)\n",
    "X_test_scaled = scaler_x.transform(Xtest)\n",
    "Y_test_scaled = scaler_y.transform(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T20:42:24.151890Z",
     "start_time": "2021-01-17T20:42:24.144433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(scaler_x.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:29:45.928204Z",
     "start_time": "2021-01-19T07:29:28.017954Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/500\n",
      "1000/1000 - 0s - loss: 2.4698 - acc: 0.6120 - val_loss: 2.0889 - val_acc: 0.7300\n",
      "Epoch 2/500\n",
      "1000/1000 - 0s - loss: 2.0147 - acc: 0.8450 - val_loss: 1.8577 - val_acc: 0.8400\n",
      "Epoch 3/500\n",
      "1000/1000 - 0s - loss: 1.7757 - acc: 0.8680 - val_loss: 1.6430 - val_acc: 0.9200\n",
      "Epoch 4/500\n",
      "1000/1000 - 0s - loss: 1.5636 - acc: 0.8910 - val_loss: 1.4446 - val_acc: 0.9700\n",
      "Epoch 5/500\n",
      "1000/1000 - 0s - loss: 1.3700 - acc: 0.9420 - val_loss: 1.2653 - val_acc: 0.9700\n",
      "Epoch 6/500\n",
      "1000/1000 - 0s - loss: 1.1951 - acc: 0.9440 - val_loss: 1.1018 - val_acc: 0.9700\n",
      "Epoch 7/500\n",
      "1000/1000 - 0s - loss: 1.0384 - acc: 0.9620 - val_loss: 0.9554 - val_acc: 0.9800\n",
      "Epoch 8/500\n",
      "1000/1000 - 0s - loss: 0.8997 - acc: 0.9600 - val_loss: 0.8265 - val_acc: 0.9800\n",
      "Epoch 9/500\n",
      "1000/1000 - 0s - loss: 0.7781 - acc: 0.9670 - val_loss: 0.7138 - val_acc: 0.9700\n",
      "Epoch 10/500\n",
      "1000/1000 - 0s - loss: 0.6724 - acc: 0.9590 - val_loss: 0.6168 - val_acc: 0.9700\n",
      "Epoch 11/500\n",
      "1000/1000 - 0s - loss: 0.5812 - acc: 0.9650 - val_loss: 0.5334 - val_acc: 0.9700\n",
      "Epoch 12/500\n",
      "1000/1000 - 0s - loss: 0.5029 - acc: 0.9640 - val_loss: 0.4613 - val_acc: 0.9700\n",
      "Epoch 13/500\n",
      "1000/1000 - 0s - loss: 0.4359 - acc: 0.9660 - val_loss: 0.4000 - val_acc: 0.9700\n",
      "Epoch 14/500\n",
      "1000/1000 - 0s - loss: 0.3787 - acc: 0.9650 - val_loss: 0.3479 - val_acc: 0.9700\n",
      "Epoch 15/500\n",
      "1000/1000 - 0s - loss: 0.3302 - acc: 0.9670 - val_loss: 0.3037 - val_acc: 0.9700\n",
      "Epoch 16/500\n",
      "1000/1000 - 0s - loss: 0.2891 - acc: 0.9620 - val_loss: 0.2663 - val_acc: 0.9800\n",
      "Epoch 17/500\n",
      "1000/1000 - 0s - loss: 0.2542 - acc: 0.9640 - val_loss: 0.2345 - val_acc: 0.9800\n",
      "Epoch 18/500\n",
      "1000/1000 - 0s - loss: 0.2246 - acc: 0.9660 - val_loss: 0.2077 - val_acc: 0.9700\n",
      "Epoch 19/500\n",
      "1000/1000 - 0s - loss: 0.1996 - acc: 0.9630 - val_loss: 0.1848 - val_acc: 0.9900\n",
      "Epoch 20/500\n",
      "1000/1000 - 0s - loss: 0.1784 - acc: 0.9670 - val_loss: 0.1656 - val_acc: 0.9700\n",
      "Epoch 21/500\n",
      "1000/1000 - 0s - loss: 0.1605 - acc: 0.9630 - val_loss: 0.1492 - val_acc: 0.9600\n",
      "Epoch 22/500\n",
      "1000/1000 - 0s - loss: 0.1452 - acc: 0.9650 - val_loss: 0.1353 - val_acc: 0.9800\n",
      "Epoch 23/500\n",
      "1000/1000 - 0s - loss: 0.1322 - acc: 0.9670 - val_loss: 0.1232 - val_acc: 0.9800\n",
      "Epoch 24/500\n",
      "1000/1000 - 0s - loss: 0.1212 - acc: 0.9670 - val_loss: 0.1134 - val_acc: 0.9700\n",
      "Epoch 25/500\n",
      "1000/1000 - 0s - loss: 0.1118 - acc: 0.9670 - val_loss: 0.1046 - val_acc: 0.9800\n",
      "Epoch 26/500\n",
      "1000/1000 - 0s - loss: 0.1037 - acc: 0.9660 - val_loss: 0.0974 - val_acc: 0.9700\n",
      "Epoch 27/500\n",
      "1000/1000 - 0s - loss: 0.0968 - acc: 0.9710 - val_loss: 0.0909 - val_acc: 0.9800\n",
      "Epoch 28/500\n",
      "1000/1000 - 0s - loss: 0.0908 - acc: 0.9610 - val_loss: 0.0852 - val_acc: 0.9700\n",
      "Epoch 29/500\n",
      "1000/1000 - 0s - loss: 0.0857 - acc: 0.9660 - val_loss: 0.0806 - val_acc: 0.9700\n",
      "Epoch 30/500\n",
      "1000/1000 - 0s - loss: 0.0813 - acc: 0.9670 - val_loss: 0.0766 - val_acc: 0.9700\n",
      "Epoch 31/500\n",
      "1000/1000 - 0s - loss: 0.0774 - acc: 0.9620 - val_loss: 0.0733 - val_acc: 0.9700\n",
      "Epoch 32/500\n",
      "1000/1000 - 0s - loss: 0.0741 - acc: 0.9680 - val_loss: 0.0698 - val_acc: 0.9800\n",
      "Epoch 33/500\n",
      "1000/1000 - 0s - loss: 0.0711 - acc: 0.9640 - val_loss: 0.0672 - val_acc: 0.9700\n",
      "Epoch 34/500\n",
      "1000/1000 - 0s - loss: 0.0686 - acc: 0.9690 - val_loss: 0.0646 - val_acc: 0.9800\n",
      "Epoch 35/500\n",
      "1000/1000 - 0s - loss: 0.0664 - acc: 0.9620 - val_loss: 0.0626 - val_acc: 0.9600\n",
      "Epoch 36/500\n",
      "1000/1000 - 0s - loss: 0.0642 - acc: 0.9600 - val_loss: 0.0609 - val_acc: 0.9700\n",
      "Epoch 37/500\n",
      "1000/1000 - 0s - loss: 0.0624 - acc: 0.9680 - val_loss: 0.0590 - val_acc: 0.9700\n",
      "Epoch 38/500\n",
      "1000/1000 - 0s - loss: 0.0608 - acc: 0.9640 - val_loss: 0.0575 - val_acc: 0.9700\n",
      "Epoch 39/500\n",
      "1000/1000 - 0s - loss: 0.0594 - acc: 0.9610 - val_loss: 0.0562 - val_acc: 0.9700\n",
      "Epoch 40/500\n",
      "1000/1000 - 0s - loss: 0.0581 - acc: 0.9590 - val_loss: 0.0551 - val_acc: 0.9700\n",
      "Epoch 41/500\n",
      "1000/1000 - 0s - loss: 0.0568 - acc: 0.9640 - val_loss: 0.0539 - val_acc: 0.9700\n",
      "Epoch 42/500\n",
      "1000/1000 - 0s - loss: 0.0558 - acc: 0.9630 - val_loss: 0.0527 - val_acc: 0.9900\n",
      "Epoch 43/500\n",
      "1000/1000 - 0s - loss: 0.0548 - acc: 0.9640 - val_loss: 0.0519 - val_acc: 0.9700\n",
      "Epoch 44/500\n",
      "1000/1000 - 0s - loss: 0.0539 - acc: 0.9600 - val_loss: 0.0508 - val_acc: 0.9600\n",
      "Epoch 45/500\n",
      "1000/1000 - 0s - loss: 0.0531 - acc: 0.9640 - val_loss: 0.0501 - val_acc: 0.9700\n",
      "Epoch 46/500\n",
      "1000/1000 - 0s - loss: 0.0524 - acc: 0.9610 - val_loss: 0.0495 - val_acc: 0.9800\n",
      "Epoch 47/500\n",
      "1000/1000 - 0s - loss: 0.0517 - acc: 0.9660 - val_loss: 0.0488 - val_acc: 0.9700\n",
      "Epoch 48/500\n",
      "1000/1000 - 0s - loss: 0.0510 - acc: 0.9660 - val_loss: 0.0485 - val_acc: 0.9700\n",
      "Epoch 49/500\n",
      "1000/1000 - 0s - loss: 0.0503 - acc: 0.9630 - val_loss: 0.0477 - val_acc: 0.9700\n",
      "Epoch 50/500\n",
      "1000/1000 - 0s - loss: 0.0498 - acc: 0.9600 - val_loss: 0.0473 - val_acc: 0.9700\n",
      "Epoch 51/500\n",
      "1000/1000 - 0s - loss: 0.0492 - acc: 0.9670 - val_loss: 0.0465 - val_acc: 0.9700\n",
      "Epoch 52/500\n",
      "1000/1000 - 0s - loss: 0.0486 - acc: 0.9640 - val_loss: 0.0460 - val_acc: 0.9700\n",
      "Epoch 53/500\n",
      "1000/1000 - 0s - loss: 0.0481 - acc: 0.9580 - val_loss: 0.0461 - val_acc: 0.9700\n",
      "Epoch 54/500\n",
      "1000/1000 - 0s - loss: 0.0477 - acc: 0.9610 - val_loss: 0.0452 - val_acc: 0.9900\n",
      "Epoch 55/500\n",
      "1000/1000 - 0s - loss: 0.0471 - acc: 0.9680 - val_loss: 0.0445 - val_acc: 0.9900\n",
      "Epoch 56/500\n",
      "1000/1000 - 0s - loss: 0.0467 - acc: 0.9600 - val_loss: 0.0444 - val_acc: 0.9700\n",
      "Epoch 57/500\n",
      "1000/1000 - 0s - loss: 0.0462 - acc: 0.9690 - val_loss: 0.0439 - val_acc: 0.9800\n",
      "Epoch 58/500\n",
      "1000/1000 - 0s - loss: 0.0458 - acc: 0.9700 - val_loss: 0.0437 - val_acc: 0.9700\n",
      "Epoch 59/500\n",
      "1000/1000 - 0s - loss: 0.0455 - acc: 0.9620 - val_loss: 0.0430 - val_acc: 0.9700\n",
      "Epoch 60/500\n",
      "1000/1000 - 0s - loss: 0.0451 - acc: 0.9690 - val_loss: 0.0426 - val_acc: 0.9800\n",
      "Epoch 61/500\n",
      "1000/1000 - 0s - loss: 0.0448 - acc: 0.9600 - val_loss: 0.0424 - val_acc: 0.9800\n",
      "Epoch 62/500\n",
      "1000/1000 - 0s - loss: 0.0443 - acc: 0.9610 - val_loss: 0.0421 - val_acc: 0.9800\n",
      "Epoch 63/500\n",
      "1000/1000 - 0s - loss: 0.0439 - acc: 0.9660 - val_loss: 0.0418 - val_acc: 0.9700\n",
      "Epoch 64/500\n",
      "1000/1000 - 0s - loss: 0.0437 - acc: 0.9620 - val_loss: 0.0414 - val_acc: 0.9700\n",
      "Epoch 65/500\n",
      "1000/1000 - 0s - loss: 0.0433 - acc: 0.9650 - val_loss: 0.0412 - val_acc: 0.9800\n",
      "Epoch 66/500\n",
      "1000/1000 - 0s - loss: 0.0429 - acc: 0.9620 - val_loss: 0.0409 - val_acc: 0.9800\n",
      "Epoch 67/500\n",
      "1000/1000 - 0s - loss: 0.0426 - acc: 0.9640 - val_loss: 0.0405 - val_acc: 0.9800\n",
      "Epoch 68/500\n",
      "1000/1000 - 0s - loss: 0.0424 - acc: 0.9570 - val_loss: 0.0403 - val_acc: 0.9700\n",
      "Epoch 69/500\n",
      "1000/1000 - 0s - loss: 0.0420 - acc: 0.9610 - val_loss: 0.0398 - val_acc: 0.9900\n",
      "Epoch 70/500\n",
      "1000/1000 - 0s - loss: 0.0417 - acc: 0.9700 - val_loss: 0.0398 - val_acc: 0.9800\n",
      "Epoch 71/500\n",
      "1000/1000 - 0s - loss: 0.0414 - acc: 0.9680 - val_loss: 0.0396 - val_acc: 0.9800\n",
      "Epoch 72/500\n",
      "1000/1000 - 0s - loss: 0.0411 - acc: 0.9690 - val_loss: 0.0392 - val_acc: 0.9800\n",
      "Epoch 73/500\n",
      "1000/1000 - 0s - loss: 0.0408 - acc: 0.9690 - val_loss: 0.0390 - val_acc: 0.9800\n",
      "Epoch 74/500\n",
      "1000/1000 - 0s - loss: 0.0405 - acc: 0.9680 - val_loss: 0.0384 - val_acc: 0.9900\n",
      "Epoch 75/500\n",
      "1000/1000 - 0s - loss: 0.0402 - acc: 0.9640 - val_loss: 0.0383 - val_acc: 0.9900\n",
      "Epoch 76/500\n",
      "1000/1000 - 0s - loss: 0.0401 - acc: 0.9670 - val_loss: 0.0381 - val_acc: 0.9800\n",
      "Epoch 77/500\n",
      "1000/1000 - 0s - loss: 0.0399 - acc: 0.9700 - val_loss: 0.0378 - val_acc: 0.9700\n",
      "Epoch 78/500\n",
      "1000/1000 - 0s - loss: 0.0395 - acc: 0.9630 - val_loss: 0.0377 - val_acc: 0.9800\n",
      "Epoch 79/500\n",
      "1000/1000 - 0s - loss: 0.0393 - acc: 0.9680 - val_loss: 0.0375 - val_acc: 0.9900\n",
      "Epoch 80/500\n",
      "1000/1000 - 0s - loss: 0.0390 - acc: 0.9690 - val_loss: 0.0373 - val_acc: 0.9700\n",
      "Epoch 81/500\n",
      "1000/1000 - 0s - loss: 0.0388 - acc: 0.9650 - val_loss: 0.0367 - val_acc: 0.9700\n",
      "Epoch 82/500\n",
      "1000/1000 - 0s - loss: 0.0386 - acc: 0.9670 - val_loss: 0.0368 - val_acc: 0.9700\n",
      "Epoch 83/500\n",
      "1000/1000 - 0s - loss: 0.0383 - acc: 0.9680 - val_loss: 0.0366 - val_acc: 0.9700\n",
      "Epoch 84/500\n",
      "1000/1000 - 0s - loss: 0.0380 - acc: 0.9740 - val_loss: 0.0363 - val_acc: 0.9700\n",
      "Epoch 85/500\n",
      "1000/1000 - 0s - loss: 0.0378 - acc: 0.9740 - val_loss: 0.0361 - val_acc: 0.9700\n",
      "Epoch 86/500\n",
      "1000/1000 - 0s - loss: 0.0375 - acc: 0.9610 - val_loss: 0.0361 - val_acc: 0.9700\n",
      "Epoch 87/500\n",
      "1000/1000 - 0s - loss: 0.0374 - acc: 0.9660 - val_loss: 0.0357 - val_acc: 0.9800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/500\n",
      "1000/1000 - 0s - loss: 0.0370 - acc: 0.9740 - val_loss: 0.0356 - val_acc: 0.9700\n",
      "Epoch 89/500\n",
      "1000/1000 - 0s - loss: 0.0368 - acc: 0.9730 - val_loss: 0.0352 - val_acc: 0.9800\n",
      "Epoch 90/500\n",
      "1000/1000 - 0s - loss: 0.0366 - acc: 0.9730 - val_loss: 0.0350 - val_acc: 0.9800\n",
      "Epoch 91/500\n",
      "1000/1000 - 0s - loss: 0.0364 - acc: 0.9730 - val_loss: 0.0348 - val_acc: 0.9800\n",
      "Epoch 92/500\n",
      "1000/1000 - 0s - loss: 0.0363 - acc: 0.9630 - val_loss: 0.0346 - val_acc: 0.9600\n",
      "Epoch 93/500\n",
      "1000/1000 - 0s - loss: 0.0361 - acc: 0.9680 - val_loss: 0.0346 - val_acc: 0.9800\n",
      "Epoch 94/500\n",
      "1000/1000 - 0s - loss: 0.0358 - acc: 0.9640 - val_loss: 0.0342 - val_acc: 0.9900\n",
      "Epoch 95/500\n",
      "1000/1000 - 0s - loss: 0.0356 - acc: 0.9700 - val_loss: 0.0341 - val_acc: 0.9700\n",
      "Epoch 96/500\n",
      "1000/1000 - 0s - loss: 0.0355 - acc: 0.9640 - val_loss: 0.0343 - val_acc: 0.9700\n",
      "Epoch 97/500\n",
      "1000/1000 - 0s - loss: 0.0353 - acc: 0.9660 - val_loss: 0.0340 - val_acc: 0.9700\n",
      "Epoch 98/500\n",
      "1000/1000 - 0s - loss: 0.0350 - acc: 0.9700 - val_loss: 0.0335 - val_acc: 0.9900\n",
      "Epoch 99/500\n",
      "1000/1000 - 0s - loss: 0.0349 - acc: 0.9720 - val_loss: 0.0332 - val_acc: 0.9800\n",
      "Epoch 100/500\n",
      "1000/1000 - 0s - loss: 0.0347 - acc: 0.9660 - val_loss: 0.0334 - val_acc: 0.9700\n",
      "Epoch 101/500\n",
      "1000/1000 - 0s - loss: 0.0346 - acc: 0.9730 - val_loss: 0.0329 - val_acc: 0.9900\n",
      "Epoch 102/500\n",
      "1000/1000 - 0s - loss: 0.0343 - acc: 0.9690 - val_loss: 0.0328 - val_acc: 0.9900\n",
      "Epoch 103/500\n",
      "1000/1000 - 0s - loss: 0.0341 - acc: 0.9750 - val_loss: 0.0325 - val_acc: 0.9900\n",
      "Epoch 104/500\n",
      "1000/1000 - 0s - loss: 0.0340 - acc: 0.9700 - val_loss: 0.0325 - val_acc: 0.9800\n",
      "Epoch 105/500\n",
      "1000/1000 - 0s - loss: 0.0338 - acc: 0.9720 - val_loss: 0.0324 - val_acc: 0.9800\n",
      "Epoch 106/500\n",
      "1000/1000 - 0s - loss: 0.0336 - acc: 0.9680 - val_loss: 0.0324 - val_acc: 0.9900\n",
      "Epoch 107/500\n",
      "1000/1000 - 0s - loss: 0.0334 - acc: 0.9700 - val_loss: 0.0321 - val_acc: 0.9800\n",
      "Epoch 108/500\n",
      "1000/1000 - 0s - loss: 0.0332 - acc: 0.9740 - val_loss: 0.0317 - val_acc: 0.9800\n",
      "Epoch 109/500\n",
      "1000/1000 - 0s - loss: 0.0330 - acc: 0.9720 - val_loss: 0.0316 - val_acc: 0.9700\n",
      "Epoch 110/500\n",
      "1000/1000 - 0s - loss: 0.0329 - acc: 0.9700 - val_loss: 0.0318 - val_acc: 0.9900\n",
      "Epoch 111/500\n",
      "1000/1000 - 0s - loss: 0.0327 - acc: 0.9770 - val_loss: 0.0312 - val_acc: 0.9800\n",
      "Epoch 112/500\n",
      "1000/1000 - 0s - loss: 0.0325 - acc: 0.9670 - val_loss: 0.0314 - val_acc: 0.9800\n",
      "Epoch 113/500\n",
      "1000/1000 - 0s - loss: 0.0325 - acc: 0.9700 - val_loss: 0.0313 - val_acc: 0.9800\n",
      "Epoch 114/500\n",
      "1000/1000 - 0s - loss: 0.0326 - acc: 0.9700 - val_loss: 0.0309 - val_acc: 0.9700\n",
      "Epoch 115/500\n",
      "1000/1000 - 0s - loss: 0.0324 - acc: 0.9630 - val_loss: 0.0308 - val_acc: 0.9900\n",
      "Epoch 116/500\n",
      "1000/1000 - 0s - loss: 0.0320 - acc: 0.9720 - val_loss: 0.0307 - val_acc: 0.9900\n",
      "Epoch 117/500\n",
      "1000/1000 - 0s - loss: 0.0318 - acc: 0.9680 - val_loss: 0.0306 - val_acc: 0.9700\n",
      "Epoch 118/500\n",
      "1000/1000 - 0s - loss: 0.0316 - acc: 0.9720 - val_loss: 0.0304 - val_acc: 0.9900\n",
      "Epoch 119/500\n",
      "1000/1000 - 0s - loss: 0.0315 - acc: 0.9740 - val_loss: 0.0303 - val_acc: 0.9900\n",
      "Epoch 120/500\n",
      "1000/1000 - 0s - loss: 0.0313 - acc: 0.9780 - val_loss: 0.0301 - val_acc: 0.9900\n",
      "Epoch 121/500\n",
      "1000/1000 - 0s - loss: 0.0312 - acc: 0.9720 - val_loss: 0.0299 - val_acc: 0.9800\n",
      "Epoch 122/500\n",
      "1000/1000 - 0s - loss: 0.0311 - acc: 0.9770 - val_loss: 0.0298 - val_acc: 0.9900\n",
      "Epoch 123/500\n",
      "1000/1000 - 0s - loss: 0.0312 - acc: 0.9680 - val_loss: 0.0299 - val_acc: 0.9700\n",
      "Epoch 124/500\n",
      "1000/1000 - 0s - loss: 0.0312 - acc: 0.9680 - val_loss: 0.0296 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "1000/1000 - 0s - loss: 0.0310 - acc: 0.9660 - val_loss: 0.0294 - val_acc: 0.9800\n",
      "Epoch 126/500\n",
      "1000/1000 - 0s - loss: 0.0307 - acc: 0.9680 - val_loss: 0.0293 - val_acc: 0.9900\n",
      "Epoch 127/500\n",
      "1000/1000 - 0s - loss: 0.0304 - acc: 0.9700 - val_loss: 0.0290 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "1000/1000 - 0s - loss: 0.0302 - acc: 0.9660 - val_loss: 0.0292 - val_acc: 0.9700\n",
      "Epoch 129/500\n",
      "1000/1000 - 0s - loss: 0.0301 - acc: 0.9740 - val_loss: 0.0290 - val_acc: 0.9700\n",
      "Epoch 130/500\n",
      "1000/1000 - 0s - loss: 0.0300 - acc: 0.9740 - val_loss: 0.0287 - val_acc: 0.9900\n",
      "Epoch 131/500\n",
      "1000/1000 - 0s - loss: 0.0300 - acc: 0.9760 - val_loss: 0.0289 - val_acc: 0.9600\n",
      "Epoch 132/500\n",
      "1000/1000 - 0s - loss: 0.0297 - acc: 0.9700 - val_loss: 0.0284 - val_acc: 0.9900\n",
      "Epoch 133/500\n",
      "1000/1000 - 0s - loss: 0.0296 - acc: 0.9750 - val_loss: 0.0286 - val_acc: 0.9900\n",
      "Epoch 134/500\n",
      "1000/1000 - 0s - loss: 0.0294 - acc: 0.9700 - val_loss: 0.0281 - val_acc: 0.9800\n",
      "Epoch 135/500\n",
      "1000/1000 - 0s - loss: 0.0293 - acc: 0.9730 - val_loss: 0.0281 - val_acc: 0.9900\n",
      "Epoch 136/500\n",
      "1000/1000 - 0s - loss: 0.0292 - acc: 0.9700 - val_loss: 0.0281 - val_acc: 0.9800\n",
      "Epoch 137/500\n",
      "1000/1000 - 0s - loss: 0.0291 - acc: 0.9750 - val_loss: 0.0279 - val_acc: 0.9700\n",
      "Epoch 138/500\n",
      "1000/1000 - 0s - loss: 0.0289 - acc: 0.9740 - val_loss: 0.0277 - val_acc: 0.9800\n",
      "Epoch 139/500\n",
      "1000/1000 - 0s - loss: 0.0289 - acc: 0.9720 - val_loss: 0.0278 - val_acc: 0.9900\n",
      "Epoch 140/500\n",
      "1000/1000 - 0s - loss: 0.0288 - acc: 0.9710 - val_loss: 0.0275 - val_acc: 0.9800\n",
      "Epoch 141/500\n",
      "1000/1000 - 0s - loss: 0.0286 - acc: 0.9760 - val_loss: 0.0275 - val_acc: 0.9900\n",
      "Epoch 142/500\n",
      "1000/1000 - 0s - loss: 0.0285 - acc: 0.9710 - val_loss: 0.0275 - val_acc: 0.9800\n",
      "Epoch 143/500\n",
      "1000/1000 - 0s - loss: 0.0284 - acc: 0.9710 - val_loss: 0.0272 - val_acc: 0.9900\n",
      "Epoch 144/500\n",
      "1000/1000 - 0s - loss: 0.0282 - acc: 0.9780 - val_loss: 0.0273 - val_acc: 0.9900\n",
      "Epoch 145/500\n",
      "1000/1000 - 0s - loss: 0.0281 - acc: 0.9700 - val_loss: 0.0271 - val_acc: 0.9900\n",
      "Epoch 146/500\n",
      "1000/1000 - 0s - loss: 0.0280 - acc: 0.9720 - val_loss: 0.0269 - val_acc: 0.9900\n",
      "Epoch 147/500\n",
      "1000/1000 - 0s - loss: 0.0279 - acc: 0.9710 - val_loss: 0.0268 - val_acc: 0.9700\n",
      "Epoch 148/500\n",
      "1000/1000 - 0s - loss: 0.0279 - acc: 0.9720 - val_loss: 0.0266 - val_acc: 0.9800\n",
      "Epoch 149/500\n",
      "1000/1000 - 0s - loss: 0.0279 - acc: 0.9670 - val_loss: 0.0267 - val_acc: 0.9700\n",
      "Epoch 150/500\n",
      "1000/1000 - 0s - loss: 0.0276 - acc: 0.9730 - val_loss: 0.0266 - val_acc: 0.9900\n",
      "Epoch 151/500\n",
      "1000/1000 - 0s - loss: 0.0275 - acc: 0.9760 - val_loss: 0.0265 - val_acc: 0.9700\n",
      "Epoch 152/500\n",
      "1000/1000 - 0s - loss: 0.0274 - acc: 0.9730 - val_loss: 0.0261 - val_acc: 0.9700\n",
      "Epoch 153/500\n",
      "1000/1000 - 0s - loss: 0.0275 - acc: 0.9670 - val_loss: 0.0264 - val_acc: 0.9800\n",
      "Epoch 154/500\n",
      "1000/1000 - 0s - loss: 0.0273 - acc: 0.9730 - val_loss: 0.0263 - val_acc: 0.9800\n",
      "Epoch 155/500\n",
      "1000/1000 - 0s - loss: 0.0270 - acc: 0.9780 - val_loss: 0.0259 - val_acc: 0.9900\n",
      "Epoch 156/500\n",
      "1000/1000 - 0s - loss: 0.0270 - acc: 0.9720 - val_loss: 0.0258 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "1000/1000 - 0s - loss: 0.0268 - acc: 0.9770 - val_loss: 0.0259 - val_acc: 0.9600\n",
      "Epoch 158/500\n",
      "1000/1000 - 0s - loss: 0.0268 - acc: 0.9780 - val_loss: 0.0258 - val_acc: 0.9900\n",
      "Epoch 159/500\n",
      "1000/1000 - 0s - loss: 0.0266 - acc: 0.9760 - val_loss: 0.0257 - val_acc: 0.9800\n",
      "Epoch 160/500\n",
      "1000/1000 - 0s - loss: 0.0265 - acc: 0.9760 - val_loss: 0.0255 - val_acc: 0.9900\n",
      "Epoch 161/500\n",
      "1000/1000 - 0s - loss: 0.0265 - acc: 0.9700 - val_loss: 0.0257 - val_acc: 0.9800\n",
      "Epoch 162/500\n",
      "1000/1000 - 0s - loss: 0.0264 - acc: 0.9700 - val_loss: 0.0254 - val_acc: 0.9900\n",
      "Epoch 163/500\n",
      "1000/1000 - 0s - loss: 0.0263 - acc: 0.9660 - val_loss: 0.0252 - val_acc: 0.9800\n",
      "Epoch 164/500\n",
      "1000/1000 - 0s - loss: 0.0262 - acc: 0.9780 - val_loss: 0.0253 - val_acc: 0.9900\n",
      "Epoch 165/500\n",
      "1000/1000 - 0s - loss: 0.0260 - acc: 0.9740 - val_loss: 0.0250 - val_acc: 0.9900\n",
      "Epoch 166/500\n",
      "1000/1000 - 0s - loss: 0.0260 - acc: 0.9780 - val_loss: 0.0249 - val_acc: 0.9900\n",
      "Epoch 167/500\n",
      "1000/1000 - 0s - loss: 0.0258 - acc: 0.9750 - val_loss: 0.0247 - val_acc: 0.9900\n",
      "Epoch 168/500\n",
      "1000/1000 - 0s - loss: 0.0258 - acc: 0.9800 - val_loss: 0.0249 - val_acc: 0.9800\n",
      "Epoch 169/500\n",
      "1000/1000 - 0s - loss: 0.0256 - acc: 0.9680 - val_loss: 0.0246 - val_acc: 0.9800\n",
      "Epoch 170/500\n",
      "1000/1000 - 0s - loss: 0.0256 - acc: 0.9740 - val_loss: 0.0247 - val_acc: 0.9900\n",
      "Epoch 171/500\n",
      "1000/1000 - 0s - loss: 0.0255 - acc: 0.9720 - val_loss: 0.0243 - val_acc: 0.9900\n",
      "Epoch 172/500\n",
      "1000/1000 - 0s - loss: 0.0254 - acc: 0.9750 - val_loss: 0.0245 - val_acc: 0.9800\n",
      "Epoch 173/500\n",
      "1000/1000 - 0s - loss: 0.0253 - acc: 0.9790 - val_loss: 0.0245 - val_acc: 0.9900\n",
      "Epoch 174/500\n",
      "1000/1000 - 0s - loss: 0.0252 - acc: 0.9790 - val_loss: 0.0244 - val_acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/500\n",
      "1000/1000 - 0s - loss: 0.0251 - acc: 0.9690 - val_loss: 0.0241 - val_acc: 0.9800\n",
      "Epoch 176/500\n",
      "1000/1000 - 0s - loss: 0.0250 - acc: 0.9720 - val_loss: 0.0241 - val_acc: 0.9800\n",
      "Epoch 177/500\n",
      "1000/1000 - 0s - loss: 0.0250 - acc: 0.9710 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "1000/1000 - 0s - loss: 0.0251 - acc: 0.9710 - val_loss: 0.0239 - val_acc: 0.9600\n",
      "Epoch 179/500\n",
      "1000/1000 - 0s - loss: 0.0249 - acc: 0.9710 - val_loss: 0.0240 - val_acc: 0.9900\n",
      "Epoch 180/500\n",
      "1000/1000 - 0s - loss: 0.0249 - acc: 0.9700 - val_loss: 0.0237 - val_acc: 0.9900\n",
      "Epoch 181/500\n",
      "1000/1000 - 0s - loss: 0.0252 - acc: 0.9710 - val_loss: 0.0241 - val_acc: 0.9700\n",
      "Epoch 182/500\n",
      "1000/1000 - 0s - loss: 0.0249 - acc: 0.9690 - val_loss: 0.0236 - val_acc: 0.9800\n",
      "Epoch 183/500\n",
      "1000/1000 - 0s - loss: 0.0245 - acc: 0.9740 - val_loss: 0.0236 - val_acc: 0.9900\n",
      "Epoch 184/500\n",
      "1000/1000 - 0s - loss: 0.0244 - acc: 0.9690 - val_loss: 0.0234 - val_acc: 0.9900\n",
      "Epoch 185/500\n",
      "1000/1000 - 0s - loss: 0.0244 - acc: 0.9770 - val_loss: 0.0234 - val_acc: 0.9800\n",
      "Epoch 186/500\n",
      "1000/1000 - 0s - loss: 0.0243 - acc: 0.9680 - val_loss: 0.0233 - val_acc: 0.9900\n",
      "Epoch 187/500\n",
      "1000/1000 - 0s - loss: 0.0241 - acc: 0.9790 - val_loss: 0.0234 - val_acc: 0.9900\n",
      "Epoch 188/500\n",
      "1000/1000 - 0s - loss: 0.0240 - acc: 0.9740 - val_loss: 0.0231 - val_acc: 0.9900\n",
      "Epoch 189/500\n",
      "1000/1000 - 0s - loss: 0.0240 - acc: 0.9810 - val_loss: 0.0232 - val_acc: 0.9700\n",
      "Epoch 190/500\n",
      "1000/1000 - 0s - loss: 0.0240 - acc: 0.9760 - val_loss: 0.0231 - val_acc: 0.9800\n",
      "Epoch 191/500\n",
      "1000/1000 - 0s - loss: 0.0239 - acc: 0.9730 - val_loss: 0.0229 - val_acc: 0.9900\n",
      "Epoch 192/500\n",
      "1000/1000 - 0s - loss: 0.0238 - acc: 0.9730 - val_loss: 0.0230 - val_acc: 0.9700\n",
      "Epoch 193/500\n",
      "1000/1000 - 0s - loss: 0.0236 - acc: 0.9720 - val_loss: 0.0228 - val_acc: 0.9800\n",
      "Epoch 194/500\n",
      "1000/1000 - 0s - loss: 0.0236 - acc: 0.9780 - val_loss: 0.0229 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "1000/1000 - 0s - loss: 0.0235 - acc: 0.9830 - val_loss: 0.0227 - val_acc: 0.9700\n",
      "Epoch 196/500\n",
      "1000/1000 - 0s - loss: 0.0235 - acc: 0.9730 - val_loss: 0.0225 - val_acc: 0.9800\n",
      "Epoch 197/500\n",
      "1000/1000 - 0s - loss: 0.0234 - acc: 0.9750 - val_loss: 0.0227 - val_acc: 0.9900\n",
      "Epoch 198/500\n",
      "1000/1000 - 0s - loss: 0.0233 - acc: 0.9780 - val_loss: 0.0223 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "1000/1000 - 0s - loss: 0.0232 - acc: 0.9730 - val_loss: 0.0225 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "1000/1000 - 0s - loss: 0.0233 - acc: 0.9750 - val_loss: 0.0223 - val_acc: 0.9800\n",
      "Epoch 201/500\n",
      "1000/1000 - 0s - loss: 0.0232 - acc: 0.9740 - val_loss: 0.0223 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "1000/1000 - 0s - loss: 0.0230 - acc: 0.9780 - val_loss: 0.0223 - val_acc: 0.9800\n",
      "Epoch 203/500\n",
      "1000/1000 - 0s - loss: 0.0229 - acc: 0.9760 - val_loss: 0.0221 - val_acc: 0.9600\n",
      "Epoch 204/500\n",
      "1000/1000 - 0s - loss: 0.0230 - acc: 0.9710 - val_loss: 0.0221 - val_acc: 0.9900\n",
      "Epoch 205/500\n",
      "1000/1000 - 0s - loss: 0.0227 - acc: 0.9780 - val_loss: 0.0220 - val_acc: 0.9900\n",
      "Epoch 206/500\n",
      "1000/1000 - 0s - loss: 0.0227 - acc: 0.9770 - val_loss: 0.0219 - val_acc: 0.9700\n",
      "Epoch 207/500\n",
      "1000/1000 - 0s - loss: 0.0227 - acc: 0.9730 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "1000/1000 - 0s - loss: 0.0226 - acc: 0.9800 - val_loss: 0.0218 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "1000/1000 - 0s - loss: 0.0227 - acc: 0.9750 - val_loss: 0.0218 - val_acc: 0.9900\n",
      "Epoch 210/500\n",
      "1000/1000 - 0s - loss: 0.0225 - acc: 0.9730 - val_loss: 0.0220 - val_acc: 0.9700\n",
      "Epoch 211/500\n",
      "1000/1000 - 0s - loss: 0.0225 - acc: 0.9750 - val_loss: 0.0217 - val_acc: 0.9800\n",
      "Epoch 212/500\n",
      "1000/1000 - 0s - loss: 0.0222 - acc: 0.9740 - val_loss: 0.0216 - val_acc: 0.9900\n",
      "Epoch 213/500\n",
      "1000/1000 - 0s - loss: 0.0222 - acc: 0.9790 - val_loss: 0.0215 - val_acc: 0.9900\n",
      "Epoch 214/500\n",
      "1000/1000 - 0s - loss: 0.0222 - acc: 0.9820 - val_loss: 0.0217 - val_acc: 0.9800\n",
      "Epoch 215/500\n",
      "1000/1000 - 0s - loss: 0.0221 - acc: 0.9760 - val_loss: 0.0212 - val_acc: 0.9900\n",
      "Epoch 216/500\n",
      "1000/1000 - 0s - loss: 0.0221 - acc: 0.9780 - val_loss: 0.0213 - val_acc: 0.9800\n",
      "Epoch 217/500\n",
      "1000/1000 - 0s - loss: 0.0221 - acc: 0.9710 - val_loss: 0.0214 - val_acc: 0.9900\n",
      "Epoch 218/500\n",
      "1000/1000 - 0s - loss: 0.0220 - acc: 0.9760 - val_loss: 0.0210 - val_acc: 0.9800\n",
      "Epoch 219/500\n",
      "1000/1000 - 0s - loss: 0.0219 - acc: 0.9770 - val_loss: 0.0212 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "1000/1000 - 0s - loss: 0.0220 - acc: 0.9750 - val_loss: 0.0211 - val_acc: 0.9800\n",
      "Epoch 221/500\n",
      "1000/1000 - 0s - loss: 0.0219 - acc: 0.9650 - val_loss: 0.0211 - val_acc: 0.9800\n",
      "Epoch 222/500\n",
      "1000/1000 - 0s - loss: 0.0219 - acc: 0.9720 - val_loss: 0.0209 - val_acc: 0.9800\n",
      "Epoch 223/500\n",
      "1000/1000 - 0s - loss: 0.0218 - acc: 0.9710 - val_loss: 0.0212 - val_acc: 0.9700\n",
      "Epoch 224/500\n",
      "1000/1000 - 0s - loss: 0.0217 - acc: 0.9760 - val_loss: 0.0212 - val_acc: 0.9700\n",
      "Epoch 225/500\n",
      "1000/1000 - 0s - loss: 0.0217 - acc: 0.9740 - val_loss: 0.0207 - val_acc: 0.9700\n",
      "Epoch 226/500\n",
      "1000/1000 - 0s - loss: 0.0216 - acc: 0.9670 - val_loss: 0.0208 - val_acc: 0.9900\n",
      "Epoch 227/500\n",
      "1000/1000 - 0s - loss: 0.0215 - acc: 0.9810 - val_loss: 0.0207 - val_acc: 0.9900\n",
      "Epoch 228/500\n",
      "1000/1000 - 0s - loss: 0.0214 - acc: 0.9800 - val_loss: 0.0206 - val_acc: 0.9800\n",
      "Epoch 229/500\n",
      "1000/1000 - 0s - loss: 0.0214 - acc: 0.9800 - val_loss: 0.0205 - val_acc: 0.9900\n",
      "Epoch 230/500\n",
      "1000/1000 - 0s - loss: 0.0212 - acc: 0.9700 - val_loss: 0.0205 - val_acc: 0.9900\n",
      "Epoch 231/500\n",
      "1000/1000 - 0s - loss: 0.0211 - acc: 0.9790 - val_loss: 0.0203 - val_acc: 0.9900\n",
      "Epoch 232/500\n",
      "1000/1000 - 0s - loss: 0.0211 - acc: 0.9760 - val_loss: 0.0205 - val_acc: 0.9900\n",
      "Epoch 233/500\n",
      "1000/1000 - 0s - loss: 0.0211 - acc: 0.9770 - val_loss: 0.0204 - val_acc: 0.9800\n",
      "Epoch 234/500\n",
      "1000/1000 - 0s - loss: 0.0209 - acc: 0.9750 - val_loss: 0.0203 - val_acc: 0.9800\n",
      "Epoch 235/500\n",
      "1000/1000 - 0s - loss: 0.0209 - acc: 0.9790 - val_loss: 0.0204 - val_acc: 0.9800\n",
      "Epoch 236/500\n",
      "1000/1000 - 0s - loss: 0.0209 - acc: 0.9820 - val_loss: 0.0202 - val_acc: 0.9800\n",
      "Epoch 237/500\n",
      "1000/1000 - 0s - loss: 0.0208 - acc: 0.9780 - val_loss: 0.0202 - val_acc: 0.9800\n",
      "Epoch 238/500\n",
      "1000/1000 - 0s - loss: 0.0207 - acc: 0.9710 - val_loss: 0.0200 - val_acc: 0.9800\n",
      "Epoch 239/500\n",
      "1000/1000 - 0s - loss: 0.0208 - acc: 0.9780 - val_loss: 0.0200 - val_acc: 0.9900\n",
      "Epoch 240/500\n",
      "1000/1000 - 0s - loss: 0.0208 - acc: 0.9770 - val_loss: 0.0199 - val_acc: 0.9800\n",
      "Epoch 241/500\n",
      "1000/1000 - 0s - loss: 0.0205 - acc: 0.9750 - val_loss: 0.0200 - val_acc: 0.9900\n",
      "Epoch 242/500\n",
      "1000/1000 - 0s - loss: 0.0205 - acc: 0.9800 - val_loss: 0.0199 - val_acc: 0.9900\n",
      "Epoch 243/500\n",
      "1000/1000 - 0s - loss: 0.0205 - acc: 0.9770 - val_loss: 0.0200 - val_acc: 0.9800\n",
      "Epoch 244/500\n",
      "1000/1000 - 0s - loss: 0.0204 - acc: 0.9740 - val_loss: 0.0197 - val_acc: 0.9800\n",
      "Epoch 245/500\n",
      "1000/1000 - 0s - loss: 0.0205 - acc: 0.9790 - val_loss: 0.0198 - val_acc: 0.9900\n",
      "Epoch 246/500\n",
      "1000/1000 - 0s - loss: 0.0204 - acc: 0.9760 - val_loss: 0.0196 - val_acc: 0.9900\n",
      "Epoch 247/500\n",
      "1000/1000 - 0s - loss: 0.0202 - acc: 0.9740 - val_loss: 0.0195 - val_acc: 0.9900\n",
      "Epoch 248/500\n",
      "1000/1000 - 0s - loss: 0.0202 - acc: 0.9760 - val_loss: 0.0197 - val_acc: 0.9900\n",
      "Epoch 249/500\n",
      "1000/1000 - 0s - loss: 0.0201 - acc: 0.9820 - val_loss: 0.0194 - val_acc: 0.9900\n",
      "Epoch 250/500\n",
      "1000/1000 - 0s - loss: 0.0202 - acc: 0.9800 - val_loss: 0.0195 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "1000/1000 - 0s - loss: 0.0201 - acc: 0.9760 - val_loss: 0.0194 - val_acc: 0.9700\n",
      "Epoch 252/500\n",
      "1000/1000 - 0s - loss: 0.0200 - acc: 0.9780 - val_loss: 0.0194 - val_acc: 0.9800\n",
      "Epoch 253/500\n",
      "1000/1000 - 0s - loss: 0.0200 - acc: 0.9750 - val_loss: 0.0195 - val_acc: 0.9700\n",
      "Epoch 254/500\n",
      "1000/1000 - 0s - loss: 0.0200 - acc: 0.9740 - val_loss: 0.0192 - val_acc: 0.9900\n",
      "Epoch 255/500\n",
      "1000/1000 - 0s - loss: 0.0200 - acc: 0.9740 - val_loss: 0.0192 - val_acc: 0.9800\n",
      "Epoch 256/500\n",
      "1000/1000 - 0s - loss: 0.0199 - acc: 0.9770 - val_loss: 0.0192 - val_acc: 0.9800\n",
      "Epoch 257/500\n",
      "1000/1000 - 0s - loss: 0.0197 - acc: 0.9810 - val_loss: 0.0190 - val_acc: 0.9800\n",
      "Epoch 258/500\n",
      "1000/1000 - 0s - loss: 0.0197 - acc: 0.9770 - val_loss: 0.0192 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "1000/1000 - 0s - loss: 0.0201 - acc: 0.9690 - val_loss: 0.0191 - val_acc: 0.9500\n",
      "Epoch 260/500\n",
      "1000/1000 - 0s - loss: 0.0199 - acc: 0.9670 - val_loss: 0.0190 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "1000/1000 - 0s - loss: 0.0199 - acc: 0.9750 - val_loss: 0.0190 - val_acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262/500\n",
      "1000/1000 - 0s - loss: 0.0197 - acc: 0.9660 - val_loss: 0.0188 - val_acc: 0.9800\n",
      "Epoch 263/500\n",
      "1000/1000 - 0s - loss: 0.0196 - acc: 0.9700 - val_loss: 0.0187 - val_acc: 0.9900\n",
      "Epoch 264/500\n",
      "1000/1000 - 0s - loss: 0.0194 - acc: 0.9770 - val_loss: 0.0188 - val_acc: 0.9800\n",
      "Epoch 265/500\n",
      "1000/1000 - 0s - loss: 0.0194 - acc: 0.9790 - val_loss: 0.0187 - val_acc: 0.9900\n",
      "Epoch 266/500\n",
      "1000/1000 - 0s - loss: 0.0196 - acc: 0.9820 - val_loss: 0.0187 - val_acc: 0.9900\n",
      "Epoch 267/500\n",
      "1000/1000 - 0s - loss: 0.0196 - acc: 0.9760 - val_loss: 0.0189 - val_acc: 0.9900\n",
      "Epoch 268/500\n",
      "1000/1000 - 0s - loss: 0.0194 - acc: 0.9710 - val_loss: 0.0187 - val_acc: 0.9800\n",
      "Epoch 269/500\n",
      "1000/1000 - 0s - loss: 0.0194 - acc: 0.9720 - val_loss: 0.0186 - val_acc: 0.9900\n",
      "Epoch 270/500\n",
      "1000/1000 - 0s - loss: 0.0191 - acc: 0.9830 - val_loss: 0.0184 - val_acc: 0.9900\n",
      "Epoch 271/500\n",
      "1000/1000 - 0s - loss: 0.0191 - acc: 0.9730 - val_loss: 0.0183 - val_acc: 0.9900\n",
      "Epoch 272/500\n",
      "1000/1000 - 0s - loss: 0.0190 - acc: 0.9760 - val_loss: 0.0184 - val_acc: 0.9800\n",
      "Epoch 273/500\n",
      "1000/1000 - 0s - loss: 0.0190 - acc: 0.9770 - val_loss: 0.0183 - val_acc: 0.9800\n",
      "Epoch 274/500\n",
      "1000/1000 - 0s - loss: 0.0190 - acc: 0.9720 - val_loss: 0.0184 - val_acc: 0.9900\n",
      "Epoch 275/500\n",
      "1000/1000 - 0s - loss: 0.0189 - acc: 0.9750 - val_loss: 0.0181 - val_acc: 0.9800\n",
      "Epoch 276/500\n",
      "1000/1000 - 0s - loss: 0.0188 - acc: 0.9810 - val_loss: 0.0182 - val_acc: 0.9900\n",
      "Epoch 277/500\n",
      "1000/1000 - 0s - loss: 0.0187 - acc: 0.9830 - val_loss: 0.0182 - val_acc: 0.9900\n",
      "Epoch 278/500\n",
      "1000/1000 - 0s - loss: 0.0186 - acc: 0.9820 - val_loss: 0.0180 - val_acc: 0.9900\n",
      "Epoch 279/500\n",
      "1000/1000 - 0s - loss: 0.0187 - acc: 0.9750 - val_loss: 0.0181 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "1000/1000 - 0s - loss: 0.0186 - acc: 0.9790 - val_loss: 0.0180 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "1000/1000 - 0s - loss: 0.0187 - acc: 0.9750 - val_loss: 0.0180 - val_acc: 0.9900\n",
      "Epoch 282/500\n",
      "1000/1000 - 0s - loss: 0.0186 - acc: 0.9790 - val_loss: 0.0180 - val_acc: 0.9900\n",
      "Epoch 283/500\n",
      "1000/1000 - 0s - loss: 0.0186 - acc: 0.9690 - val_loss: 0.0180 - val_acc: 0.9900\n",
      "Epoch 284/500\n",
      "1000/1000 - 0s - loss: 0.0186 - acc: 0.9740 - val_loss: 0.0179 - val_acc: 0.9900\n",
      "Epoch 285/500\n",
      "1000/1000 - 0s - loss: 0.0185 - acc: 0.9830 - val_loss: 0.0179 - val_acc: 0.9900\n",
      "Epoch 286/500\n",
      "1000/1000 - 0s - loss: 0.0185 - acc: 0.9780 - val_loss: 0.0179 - val_acc: 0.9700\n",
      "Epoch 287/500\n",
      "1000/1000 - 0s - loss: 0.0184 - acc: 0.9800 - val_loss: 0.0178 - val_acc: 0.9800\n",
      "Epoch 288/500\n",
      "1000/1000 - 0s - loss: 0.0184 - acc: 0.9740 - val_loss: 0.0177 - val_acc: 0.9800\n",
      "Epoch 289/500\n",
      "1000/1000 - 0s - loss: 0.0184 - acc: 0.9770 - val_loss: 0.0177 - val_acc: 0.9900\n",
      "Epoch 290/500\n",
      "1000/1000 - 0s - loss: 0.0185 - acc: 0.9690 - val_loss: 0.0178 - val_acc: 0.9700\n",
      "Epoch 291/500\n",
      "1000/1000 - 0s - loss: 0.0184 - acc: 0.9700 - val_loss: 0.0177 - val_acc: 0.9700\n",
      "Epoch 292/500\n",
      "1000/1000 - 0s - loss: 0.0183 - acc: 0.9740 - val_loss: 0.0177 - val_acc: 0.9800\n",
      "Epoch 293/500\n",
      "1000/1000 - 0s - loss: 0.0182 - acc: 0.9790 - val_loss: 0.0175 - val_acc: 0.9900\n",
      "Epoch 294/500\n",
      "1000/1000 - 0s - loss: 0.0181 - acc: 0.9810 - val_loss: 0.0175 - val_acc: 0.9800\n",
      "Epoch 295/500\n",
      "1000/1000 - 0s - loss: 0.0181 - acc: 0.9800 - val_loss: 0.0173 - val_acc: 0.9900\n",
      "Epoch 296/500\n",
      "1000/1000 - 0s - loss: 0.0179 - acc: 0.9790 - val_loss: 0.0173 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "1000/1000 - 0s - loss: 0.0179 - acc: 0.9840 - val_loss: 0.0173 - val_acc: 0.9900\n",
      "Epoch 298/500\n",
      "1000/1000 - 0s - loss: 0.0179 - acc: 0.9810 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "1000/1000 - 0s - loss: 0.0180 - acc: 0.9750 - val_loss: 0.0173 - val_acc: 0.9800\n",
      "Epoch 300/500\n",
      "1000/1000 - 0s - loss: 0.0180 - acc: 0.9720 - val_loss: 0.0172 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "1000/1000 - 0s - loss: 0.0179 - acc: 0.9700 - val_loss: 0.0172 - val_acc: 0.9700\n",
      "Epoch 302/500\n",
      "1000/1000 - 0s - loss: 0.0178 - acc: 0.9720 - val_loss: 0.0173 - val_acc: 0.9800\n",
      "Epoch 303/500\n",
      "1000/1000 - 0s - loss: 0.0177 - acc: 0.9780 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "1000/1000 - 0s - loss: 0.0177 - acc: 0.9740 - val_loss: 0.0171 - val_acc: 0.9800\n",
      "Epoch 305/500\n",
      "1000/1000 - 0s - loss: 0.0176 - acc: 0.9840 - val_loss: 0.0170 - val_acc: 0.9900\n",
      "Epoch 306/500\n",
      "1000/1000 - 0s - loss: 0.0177 - acc: 0.9800 - val_loss: 0.0170 - val_acc: 0.9900\n",
      "Epoch 307/500\n",
      "1000/1000 - 0s - loss: 0.0177 - acc: 0.9770 - val_loss: 0.0170 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "1000/1000 - 0s - loss: 0.0177 - acc: 0.9720 - val_loss: 0.0170 - val_acc: 0.9800\n",
      "Epoch 309/500\n",
      "1000/1000 - 0s - loss: 0.0176 - acc: 0.9770 - val_loss: 0.0170 - val_acc: 0.9700\n",
      "Epoch 310/500\n",
      "1000/1000 - 0s - loss: 0.0177 - acc: 0.9800 - val_loss: 0.0170 - val_acc: 0.9800\n",
      "Epoch 311/500\n",
      "1000/1000 - 0s - loss: 0.0175 - acc: 0.9830 - val_loss: 0.0169 - val_acc: 0.9800\n",
      "Epoch 312/500\n",
      "1000/1000 - 0s - loss: 0.0173 - acc: 0.9820 - val_loss: 0.0170 - val_acc: 0.9900\n",
      "Epoch 313/500\n",
      "1000/1000 - 0s - loss: 0.0173 - acc: 0.9840 - val_loss: 0.0167 - val_acc: 0.9900\n",
      "Epoch 314/500\n",
      "1000/1000 - 0s - loss: 0.0173 - acc: 0.9780 - val_loss: 0.0167 - val_acc: 0.9900\n",
      "Epoch 315/500\n",
      "1000/1000 - 0s - loss: 0.0173 - acc: 0.9730 - val_loss: 0.0166 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "1000/1000 - 0s - loss: 0.0172 - acc: 0.9760 - val_loss: 0.0167 - val_acc: 0.9800\n",
      "Epoch 317/500\n",
      "1000/1000 - 0s - loss: 0.0172 - acc: 0.9780 - val_loss: 0.0167 - val_acc: 0.9900\n",
      "Epoch 318/500\n",
      "1000/1000 - 0s - loss: 0.0172 - acc: 0.9790 - val_loss: 0.0167 - val_acc: 0.9900\n",
      "Epoch 319/500\n",
      "1000/1000 - 0s - loss: 0.0171 - acc: 0.9780 - val_loss: 0.0166 - val_acc: 0.9800\n",
      "Epoch 320/500\n",
      "1000/1000 - 0s - loss: 0.0173 - acc: 0.9730 - val_loss: 0.0165 - val_acc: 0.9800\n",
      "Epoch 321/500\n",
      "1000/1000 - 0s - loss: 0.0170 - acc: 0.9840 - val_loss: 0.0164 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "1000/1000 - 0s - loss: 0.0169 - acc: 0.9790 - val_loss: 0.0164 - val_acc: 0.9900\n",
      "Epoch 323/500\n",
      "1000/1000 - 0s - loss: 0.0168 - acc: 0.9810 - val_loss: 0.0164 - val_acc: 0.9800\n",
      "Epoch 324/500\n",
      "1000/1000 - 0s - loss: 0.0170 - acc: 0.9780 - val_loss: 0.0164 - val_acc: 0.9900\n",
      "Epoch 325/500\n",
      "1000/1000 - 0s - loss: 0.0170 - acc: 0.9750 - val_loss: 0.0163 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "1000/1000 - 0s - loss: 0.0170 - acc: 0.9710 - val_loss: 0.0165 - val_acc: 0.9700\n",
      "Epoch 327/500\n",
      "1000/1000 - 0s - loss: 0.0170 - acc: 0.9830 - val_loss: 0.0162 - val_acc: 0.9900\n",
      "Epoch 328/500\n",
      "1000/1000 - 0s - loss: 0.0168 - acc: 0.9680 - val_loss: 0.0161 - val_acc: 0.9900\n",
      "Epoch 329/500\n",
      "1000/1000 - 0s - loss: 0.0167 - acc: 0.9810 - val_loss: 0.0163 - val_acc: 0.9700\n",
      "Epoch 330/500\n",
      "1000/1000 - 0s - loss: 0.0167 - acc: 0.9740 - val_loss: 0.0163 - val_acc: 0.9900\n",
      "Epoch 331/500\n",
      "1000/1000 - 0s - loss: 0.0167 - acc: 0.9860 - val_loss: 0.0162 - val_acc: 0.9700\n",
      "Epoch 332/500\n",
      "1000/1000 - 0s - loss: 0.0166 - acc: 0.9770 - val_loss: 0.0161 - val_acc: 0.9800\n",
      "Epoch 333/500\n",
      "1000/1000 - 0s - loss: 0.0166 - acc: 0.9840 - val_loss: 0.0162 - val_acc: 0.9900\n",
      "Epoch 334/500\n",
      "1000/1000 - 0s - loss: 0.0166 - acc: 0.9760 - val_loss: 0.0159 - val_acc: 0.9700\n",
      "Epoch 335/500\n",
      "1000/1000 - 0s - loss: 0.0165 - acc: 0.9820 - val_loss: 0.0161 - val_acc: 0.9900\n",
      "Epoch 336/500\n",
      "1000/1000 - 0s - loss: 0.0164 - acc: 0.9820 - val_loss: 0.0158 - val_acc: 0.9800\n",
      "Epoch 337/500\n",
      "1000/1000 - 0s - loss: 0.0165 - acc: 0.9760 - val_loss: 0.0159 - val_acc: 0.9800\n",
      "Epoch 338/500\n",
      "1000/1000 - 0s - loss: 0.0165 - acc: 0.9810 - val_loss: 0.0160 - val_acc: 0.9800\n",
      "Epoch 339/500\n",
      "1000/1000 - 0s - loss: 0.0165 - acc: 0.9780 - val_loss: 0.0159 - val_acc: 0.9700\n",
      "Epoch 340/500\n",
      "1000/1000 - 0s - loss: 0.0163 - acc: 0.9810 - val_loss: 0.0158 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "1000/1000 - 0s - loss: 0.0163 - acc: 0.9750 - val_loss: 0.0158 - val_acc: 0.9900\n",
      "Epoch 342/500\n",
      "1000/1000 - 0s - loss: 0.0164 - acc: 0.9810 - val_loss: 0.0157 - val_acc: 0.9700\n",
      "Epoch 343/500\n",
      "1000/1000 - 0s - loss: 0.0163 - acc: 0.9760 - val_loss: 0.0160 - val_acc: 0.9700\n",
      "Epoch 344/500\n",
      "1000/1000 - 0s - loss: 0.0162 - acc: 0.9820 - val_loss: 0.0156 - val_acc: 0.9900\n",
      "Epoch 345/500\n",
      "1000/1000 - 0s - loss: 0.0163 - acc: 0.9730 - val_loss: 0.0158 - val_acc: 0.9800\n",
      "Epoch 346/500\n",
      "1000/1000 - 0s - loss: 0.0166 - acc: 0.9710 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "1000/1000 - 0s - loss: 0.0162 - acc: 0.9800 - val_loss: 0.0157 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "1000/1000 - 0s - loss: 0.0161 - acc: 0.9740 - val_loss: 0.0156 - val_acc: 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349/500\n",
      "1000/1000 - 0s - loss: 0.0161 - acc: 0.9740 - val_loss: 0.0156 - val_acc: 0.9800\n",
      "Epoch 350/500\n",
      "1000/1000 - 0s - loss: 0.0160 - acc: 0.9820 - val_loss: 0.0154 - val_acc: 0.9900\n",
      "Epoch 351/500\n",
      "1000/1000 - 0s - loss: 0.0162 - acc: 0.9750 - val_loss: 0.0157 - val_acc: 0.9600\n",
      "Epoch 352/500\n",
      "1000/1000 - 0s - loss: 0.0161 - acc: 0.9710 - val_loss: 0.0155 - val_acc: 0.9800\n",
      "Epoch 353/500\n",
      "1000/1000 - 0s - loss: 0.0160 - acc: 0.9830 - val_loss: 0.0154 - val_acc: 0.9800\n",
      "Epoch 354/500\n",
      "1000/1000 - 0s - loss: 0.0160 - acc: 0.9720 - val_loss: 0.0154 - val_acc: 0.9800\n",
      "Epoch 355/500\n",
      "1000/1000 - 0s - loss: 0.0160 - acc: 0.9790 - val_loss: 0.0153 - val_acc: 0.9900\n",
      "Epoch 356/500\n",
      "1000/1000 - 0s - loss: 0.0158 - acc: 0.9830 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "1000/1000 - 0s - loss: 0.0158 - acc: 0.9740 - val_loss: 0.0152 - val_acc: 0.9900\n",
      "Epoch 358/500\n",
      "1000/1000 - 0s - loss: 0.0158 - acc: 0.9830 - val_loss: 0.0152 - val_acc: 0.9900\n",
      "Epoch 359/500\n",
      "1000/1000 - 0s - loss: 0.0159 - acc: 0.9790 - val_loss: 0.0152 - val_acc: 0.9800\n",
      "Epoch 360/500\n",
      "1000/1000 - 0s - loss: 0.0159 - acc: 0.9740 - val_loss: 0.0151 - val_acc: 0.9900\n",
      "Epoch 361/500\n",
      "1000/1000 - 0s - loss: 0.0157 - acc: 0.9830 - val_loss: 0.0153 - val_acc: 0.9900\n",
      "Epoch 362/500\n",
      "1000/1000 - 0s - loss: 0.0156 - acc: 0.9850 - val_loss: 0.0151 - val_acc: 0.9800\n",
      "Epoch 363/500\n",
      "1000/1000 - 0s - loss: 0.0158 - acc: 0.9760 - val_loss: 0.0151 - val_acc: 0.9900\n",
      "Epoch 364/500\n",
      "1000/1000 - 0s - loss: 0.0158 - acc: 0.9730 - val_loss: 0.0152 - val_acc: 0.9700\n",
      "Epoch 365/500\n",
      "1000/1000 - 0s - loss: 0.0157 - acc: 0.9800 - val_loss: 0.0150 - val_acc: 0.9900\n",
      "Epoch 366/500\n",
      "1000/1000 - 0s - loss: 0.0157 - acc: 0.9790 - val_loss: 0.0150 - val_acc: 0.9700\n",
      "Epoch 367/500\n",
      "1000/1000 - 0s - loss: 0.0157 - acc: 0.9770 - val_loss: 0.0152 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "1000/1000 - 0s - loss: 0.0156 - acc: 0.9780 - val_loss: 0.0149 - val_acc: 0.9800\n",
      "Epoch 369/500\n",
      "1000/1000 - 0s - loss: 0.0158 - acc: 0.9740 - val_loss: 0.0150 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "1000/1000 - 0s - loss: 0.0159 - acc: 0.9740 - val_loss: 0.0150 - val_acc: 0.9900\n",
      "Epoch 371/500\n",
      "1000/1000 - 0s - loss: 0.0154 - acc: 0.9780 - val_loss: 0.0149 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "1000/1000 - 0s - loss: 0.0154 - acc: 0.9840 - val_loss: 0.0148 - val_acc: 0.9900\n",
      "Epoch 373/500\n",
      "1000/1000 - 0s - loss: 0.0153 - acc: 0.9850 - val_loss: 0.0149 - val_acc: 0.9800\n",
      "Epoch 374/500\n",
      "1000/1000 - 0s - loss: 0.0152 - acc: 0.9840 - val_loss: 0.0148 - val_acc: 0.9900\n",
      "Epoch 375/500\n",
      "1000/1000 - 0s - loss: 0.0152 - acc: 0.9780 - val_loss: 0.0148 - val_acc: 0.9900\n",
      "Epoch 376/500\n",
      "1000/1000 - 0s - loss: 0.0152 - acc: 0.9800 - val_loss: 0.0146 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9810 - val_loss: 0.0148 - val_acc: 0.9900\n",
      "Epoch 378/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9800 - val_loss: 0.0146 - val_acc: 0.9800\n",
      "Epoch 379/500\n",
      "1000/1000 - 0s - loss: 0.0152 - acc: 0.9780 - val_loss: 0.0147 - val_acc: 0.9900\n",
      "Epoch 380/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9780 - val_loss: 0.0147 - val_acc: 0.9800\n",
      "Epoch 381/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9830 - val_loss: 0.0146 - val_acc: 0.9900\n",
      "Epoch 382/500\n",
      "1000/1000 - 0s - loss: 0.0150 - acc: 0.9770 - val_loss: 0.0145 - val_acc: 0.9800\n",
      "Epoch 383/500\n",
      "1000/1000 - 0s - loss: 0.0150 - acc: 0.9820 - val_loss: 0.0145 - val_acc: 0.9900\n",
      "Epoch 384/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9810 - val_loss: 0.0145 - val_acc: 0.9900\n",
      "Epoch 385/500\n",
      "1000/1000 - 0s - loss: 0.0149 - acc: 0.9710 - val_loss: 0.0144 - val_acc: 0.9900\n",
      "Epoch 386/500\n",
      "1000/1000 - 0s - loss: 0.0150 - acc: 0.9800 - val_loss: 0.0145 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9750 - val_loss: 0.0145 - val_acc: 0.9600\n",
      "Epoch 388/500\n",
      "1000/1000 - 0s - loss: 0.0152 - acc: 0.9740 - val_loss: 0.0145 - val_acc: 0.9800\n",
      "Epoch 389/500\n",
      "1000/1000 - 0s - loss: 0.0152 - acc: 0.9740 - val_loss: 0.0145 - val_acc: 0.9900\n",
      "Epoch 390/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9760 - val_loss: 0.0144 - val_acc: 0.9700\n",
      "Epoch 391/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9730 - val_loss: 0.0144 - val_acc: 0.9900\n",
      "Epoch 392/500\n",
      "1000/1000 - 0s - loss: 0.0150 - acc: 0.9730 - val_loss: 0.0147 - val_acc: 0.9800\n",
      "Epoch 393/500\n",
      "1000/1000 - 0s - loss: 0.0153 - acc: 0.9640 - val_loss: 0.0144 - val_acc: 0.9800\n",
      "Epoch 394/500\n",
      "1000/1000 - 0s - loss: 0.0151 - acc: 0.9740 - val_loss: 0.0144 - val_acc: 0.9800\n",
      "Epoch 395/500\n",
      "1000/1000 - 0s - loss: 0.0147 - acc: 0.9810 - val_loss: 0.0143 - val_acc: 0.9900\n",
      "Epoch 396/500\n",
      "1000/1000 - 0s - loss: 0.0147 - acc: 0.9810 - val_loss: 0.0142 - val_acc: 0.9900\n",
      "Epoch 397/500\n",
      "1000/1000 - 0s - loss: 0.0147 - acc: 0.9780 - val_loss: 0.0141 - val_acc: 0.9900\n",
      "Epoch 398/500\n",
      "1000/1000 - 0s - loss: 0.0149 - acc: 0.9750 - val_loss: 0.0142 - val_acc: 0.9700\n",
      "Epoch 399/500\n",
      "1000/1000 - 0s - loss: 0.0148 - acc: 0.9830 - val_loss: 0.0141 - val_acc: 0.9800\n",
      "Epoch 400/500\n",
      "1000/1000 - 0s - loss: 0.0149 - acc: 0.9680 - val_loss: 0.0142 - val_acc: 0.9900\n",
      "Epoch 401/500\n",
      "1000/1000 - 0s - loss: 0.0147 - acc: 0.9730 - val_loss: 0.0141 - val_acc: 0.9700\n",
      "Epoch 402/500\n",
      "1000/1000 - 0s - loss: 0.0145 - acc: 0.9800 - val_loss: 0.0140 - val_acc: 0.9900\n",
      "Epoch 403/500\n",
      "1000/1000 - 0s - loss: 0.0145 - acc: 0.9840 - val_loss: 0.0140 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "1000/1000 - 0s - loss: 0.0144 - acc: 0.9820 - val_loss: 0.0140 - val_acc: 0.9900\n",
      "Epoch 405/500\n",
      "1000/1000 - 0s - loss: 0.0143 - acc: 0.9840 - val_loss: 0.0139 - val_acc: 0.9800\n",
      "Epoch 406/500\n",
      "1000/1000 - 0s - loss: 0.0144 - acc: 0.9810 - val_loss: 0.0138 - val_acc: 0.9900\n",
      "Epoch 407/500\n",
      "1000/1000 - 0s - loss: 0.0144 - acc: 0.9800 - val_loss: 0.0140 - val_acc: 0.9800\n",
      "Epoch 408/500\n",
      "1000/1000 - 0s - loss: 0.0143 - acc: 0.9820 - val_loss: 0.0138 - val_acc: 0.9900\n",
      "Epoch 409/500\n",
      "1000/1000 - 0s - loss: 0.0144 - acc: 0.9780 - val_loss: 0.0139 - val_acc: 0.9800\n",
      "Epoch 410/500\n",
      "1000/1000 - 0s - loss: 0.0143 - acc: 0.9810 - val_loss: 0.0138 - val_acc: 0.9900\n",
      "Epoch 411/500\n",
      "1000/1000 - 0s - loss: 0.0143 - acc: 0.9770 - val_loss: 0.0139 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "1000/1000 - 0s - loss: 0.0144 - acc: 0.9750 - val_loss: 0.0139 - val_acc: 0.9900\n",
      "Epoch 413/500\n",
      "1000/1000 - 0s - loss: 0.0142 - acc: 0.9760 - val_loss: 0.0137 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "1000/1000 - 0s - loss: 0.0143 - acc: 0.9730 - val_loss: 0.0139 - val_acc: 0.9700\n",
      "Epoch 415/500\n",
      "1000/1000 - 0s - loss: 0.0142 - acc: 0.9740 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "1000/1000 - 0s - loss: 0.0141 - acc: 0.9790 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "1000/1000 - 0s - loss: 0.0141 - acc: 0.9740 - val_loss: 0.0136 - val_acc: 0.9900\n",
      "Epoch 418/500\n",
      "1000/1000 - 0s - loss: 0.0141 - acc: 0.9830 - val_loss: 0.0136 - val_acc: 0.9800\n",
      "Epoch 419/500\n",
      "1000/1000 - 0s - loss: 0.0141 - acc: 0.9770 - val_loss: 0.0136 - val_acc: 0.9800\n",
      "Epoch 420/500\n",
      "1000/1000 - 0s - loss: 0.0139 - acc: 0.9860 - val_loss: 0.0136 - val_acc: 0.9900\n",
      "Epoch 421/500\n",
      "1000/1000 - 0s - loss: 0.0140 - acc: 0.9770 - val_loss: 0.0136 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "1000/1000 - 0s - loss: 0.0140 - acc: 0.9800 - val_loss: 0.0135 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "1000/1000 - 0s - loss: 0.0139 - acc: 0.9770 - val_loss: 0.0135 - val_acc: 0.9900\n",
      "Epoch 424/500\n",
      "1000/1000 - 0s - loss: 0.0141 - acc: 0.9790 - val_loss: 0.0137 - val_acc: 0.9900\n",
      "Epoch 425/500\n",
      "1000/1000 - 0s - loss: 0.0142 - acc: 0.9740 - val_loss: 0.0135 - val_acc: 0.9800\n",
      "Epoch 426/500\n",
      "1000/1000 - 0s - loss: 0.0139 - acc: 0.9790 - val_loss: 0.0134 - val_acc: 0.9900\n",
      "Epoch 427/500\n",
      "1000/1000 - 0s - loss: 0.0140 - acc: 0.9770 - val_loss: 0.0135 - val_acc: 0.9900\n",
      "Epoch 428/500\n",
      "1000/1000 - 0s - loss: 0.0142 - acc: 0.9790 - val_loss: 0.0134 - val_acc: 0.9900\n",
      "Epoch 429/500\n",
      "1000/1000 - 0s - loss: 0.0142 - acc: 0.9770 - val_loss: 0.0135 - val_acc: 0.9900\n",
      "Epoch 430/500\n",
      "1000/1000 - 0s - loss: 0.0140 - acc: 0.9800 - val_loss: 0.0135 - val_acc: 0.9900\n",
      "Epoch 431/500\n",
      "1000/1000 - 0s - loss: 0.0139 - acc: 0.9790 - val_loss: 0.0133 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "1000/1000 - 0s - loss: 0.0138 - acc: 0.9790 - val_loss: 0.0135 - val_acc: 0.9900\n",
      "Epoch 433/500\n",
      "1000/1000 - 0s - loss: 0.0137 - acc: 0.9800 - val_loss: 0.0133 - val_acc: 0.9800\n",
      "Epoch 434/500\n",
      "1000/1000 - 0s - loss: 0.0136 - acc: 0.9820 - val_loss: 0.0132 - val_acc: 0.9900\n",
      "Epoch 435/500\n",
      "1000/1000 - 0s - loss: 0.0137 - acc: 0.9790 - val_loss: 0.0134 - val_acc: 0.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 436/500\n",
      "1000/1000 - 0s - loss: 0.0139 - acc: 0.9770 - val_loss: 0.0131 - val_acc: 0.9800\n",
      "Epoch 437/500\n",
      "1000/1000 - 0s - loss: 0.0138 - acc: 0.9790 - val_loss: 0.0132 - val_acc: 0.9800\n",
      "Epoch 438/500\n",
      "1000/1000 - 0s - loss: 0.0136 - acc: 0.9800 - val_loss: 0.0132 - val_acc: 0.9800\n",
      "Epoch 439/500\n",
      "1000/1000 - 0s - loss: 0.0134 - acc: 0.9800 - val_loss: 0.0131 - val_acc: 0.9900\n",
      "Epoch 440/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9820 - val_loss: 0.0131 - val_acc: 0.9700\n",
      "Epoch 441/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9830 - val_loss: 0.0132 - val_acc: 0.9700\n",
      "Epoch 442/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9810 - val_loss: 0.0130 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "1000/1000 - 0s - loss: 0.0136 - acc: 0.9810 - val_loss: 0.0130 - val_acc: 0.9800\n",
      "Epoch 444/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9830 - val_loss: 0.0130 - val_acc: 0.9900\n",
      "Epoch 445/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9770 - val_loss: 0.0131 - val_acc: 0.9800\n",
      "Epoch 446/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9730 - val_loss: 0.0131 - val_acc: 0.9800\n",
      "Epoch 447/500\n",
      "1000/1000 - 0s - loss: 0.0134 - acc: 0.9800 - val_loss: 0.0129 - val_acc: 0.9800\n",
      "Epoch 448/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9800 - val_loss: 0.0130 - val_acc: 0.9700\n",
      "Epoch 449/500\n",
      "1000/1000 - 0s - loss: 0.0134 - acc: 0.9710 - val_loss: 0.0131 - val_acc: 0.9700\n",
      "Epoch 450/500\n",
      "1000/1000 - 0s - loss: 0.0133 - acc: 0.9840 - val_loss: 0.0129 - val_acc: 0.9900\n",
      "Epoch 451/500\n",
      "1000/1000 - 0s - loss: 0.0134 - acc: 0.9790 - val_loss: 0.0129 - val_acc: 0.9900\n",
      "Epoch 452/500\n",
      "1000/1000 - 0s - loss: 0.0133 - acc: 0.9780 - val_loss: 0.0129 - val_acc: 0.9900\n",
      "Epoch 453/500\n",
      "1000/1000 - 0s - loss: 0.0133 - acc: 0.9750 - val_loss: 0.0129 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "1000/1000 - 0s - loss: 0.0134 - acc: 0.9750 - val_loss: 0.0128 - val_acc: 0.9900\n",
      "Epoch 455/500\n",
      "1000/1000 - 0s - loss: 0.0133 - acc: 0.9750 - val_loss: 0.0130 - val_acc: 0.9700\n",
      "Epoch 456/500\n",
      "1000/1000 - 0s - loss: 0.0135 - acc: 0.9700 - val_loss: 0.0127 - val_acc: 0.9800\n",
      "Epoch 457/500\n",
      "1000/1000 - 0s - loss: 0.0133 - acc: 0.9760 - val_loss: 0.0130 - val_acc: 0.9600\n",
      "Epoch 458/500\n",
      "1000/1000 - 0s - loss: 0.0132 - acc: 0.9800 - val_loss: 0.0126 - val_acc: 0.9900\n",
      "Epoch 459/500\n",
      "1000/1000 - 0s - loss: 0.0132 - acc: 0.9790 - val_loss: 0.0127 - val_acc: 0.9900\n",
      "Epoch 460/500\n",
      "1000/1000 - 0s - loss: 0.0132 - acc: 0.9790 - val_loss: 0.0127 - val_acc: 0.9900\n",
      "Epoch 461/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9820 - val_loss: 0.0127 - val_acc: 0.9900\n",
      "Epoch 462/500\n",
      "1000/1000 - 0s - loss: 0.0132 - acc: 0.9770 - val_loss: 0.0126 - val_acc: 0.9800\n",
      "Epoch 463/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9770 - val_loss: 0.0127 - val_acc: 0.9900\n",
      "Epoch 464/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9760 - val_loss: 0.0126 - val_acc: 0.9700\n",
      "Epoch 465/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9830 - val_loss: 0.0127 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "1000/1000 - 0s - loss: 0.0130 - acc: 0.9810 - val_loss: 0.0126 - val_acc: 0.9900\n",
      "Epoch 467/500\n",
      "1000/1000 - 0s - loss: 0.0129 - acc: 0.9850 - val_loss: 0.0125 - val_acc: 0.9900\n",
      "Epoch 468/500\n",
      "1000/1000 - 0s - loss: 0.0130 - acc: 0.9810 - val_loss: 0.0125 - val_acc: 0.9900\n",
      "Epoch 469/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9780 - val_loss: 0.0125 - val_acc: 0.9800\n",
      "Epoch 470/500\n",
      "1000/1000 - 0s - loss: 0.0129 - acc: 0.9810 - val_loss: 0.0125 - val_acc: 0.9800\n",
      "Epoch 471/500\n",
      "1000/1000 - 0s - loss: 0.0129 - acc: 0.9760 - val_loss: 0.0125 - val_acc: 0.9800\n",
      "Epoch 472/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9840 - val_loss: 0.0124 - val_acc: 0.9700\n",
      "Epoch 473/500\n",
      "1000/1000 - 0s - loss: 0.0129 - acc: 0.9810 - val_loss: 0.0124 - val_acc: 0.9900\n",
      "Epoch 474/500\n",
      "1000/1000 - 0s - loss: 0.0128 - acc: 0.9790 - val_loss: 0.0124 - val_acc: 0.9900\n",
      "Epoch 475/500\n",
      "1000/1000 - 0s - loss: 0.0128 - acc: 0.9790 - val_loss: 0.0125 - val_acc: 0.9900\n",
      "Epoch 476/500\n",
      "1000/1000 - 0s - loss: 0.0131 - acc: 0.9770 - val_loss: 0.0129 - val_acc: 0.9800\n",
      "Epoch 477/500\n",
      "1000/1000 - 0s - loss: 0.0138 - acc: 0.9690 - val_loss: 0.0124 - val_acc: 0.9600\n",
      "Epoch 478/500\n",
      "1000/1000 - 0s - loss: 0.0129 - acc: 0.9810 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "1000/1000 - 0s - loss: 0.0129 - acc: 0.9830 - val_loss: 0.0123 - val_acc: 0.9900\n",
      "Epoch 480/500\n",
      "1000/1000 - 0s - loss: 0.0128 - acc: 0.9770 - val_loss: 0.0123 - val_acc: 0.9900\n",
      "Epoch 481/500\n",
      "1000/1000 - 0s - loss: 0.0127 - acc: 0.9860 - val_loss: 0.0122 - val_acc: 0.9900\n",
      "Epoch 482/500\n",
      "1000/1000 - 0s - loss: 0.0126 - acc: 0.9810 - val_loss: 0.0122 - val_acc: 0.9800\n",
      "Epoch 483/500\n",
      "1000/1000 - 0s - loss: 0.0126 - acc: 0.9840 - val_loss: 0.0123 - val_acc: 0.9900\n",
      "Epoch 484/500\n",
      "1000/1000 - 0s - loss: 0.0127 - acc: 0.9760 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "1000/1000 - 0s - loss: 0.0126 - acc: 0.9810 - val_loss: 0.0122 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "1000/1000 - 0s - loss: 0.0125 - acc: 0.9830 - val_loss: 0.0122 - val_acc: 0.9800\n",
      "Epoch 487/500\n",
      "1000/1000 - 0s - loss: 0.0127 - acc: 0.9750 - val_loss: 0.0123 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "1000/1000 - 0s - loss: 0.0126 - acc: 0.9800 - val_loss: 0.0121 - val_acc: 0.9800\n",
      "Epoch 489/500\n",
      "1000/1000 - 0s - loss: 0.0125 - acc: 0.9820 - val_loss: 0.0121 - val_acc: 0.9800\n",
      "Epoch 490/500\n",
      "1000/1000 - 0s - loss: 0.0125 - acc: 0.9820 - val_loss: 0.0121 - val_acc: 0.9900\n",
      "Epoch 491/500\n",
      "1000/1000 - 0s - loss: 0.0126 - acc: 0.9800 - val_loss: 0.0120 - val_acc: 0.9800\n",
      "Epoch 492/500\n",
      "1000/1000 - 0s - loss: 0.0126 - acc: 0.9770 - val_loss: 0.0120 - val_acc: 0.9900\n",
      "Epoch 493/500\n",
      "1000/1000 - 0s - loss: 0.0125 - acc: 0.9780 - val_loss: 0.0121 - val_acc: 0.9900\n",
      "Epoch 494/500\n",
      "1000/1000 - 0s - loss: 0.0125 - acc: 0.9820 - val_loss: 0.0120 - val_acc: 0.9800\n",
      "Epoch 495/500\n",
      "1000/1000 - 0s - loss: 0.0124 - acc: 0.9850 - val_loss: 0.0120 - val_acc: 0.9900\n",
      "Epoch 496/500\n",
      "1000/1000 - 0s - loss: 0.0124 - acc: 0.9810 - val_loss: 0.0119 - val_acc: 0.9900\n",
      "Epoch 497/500\n",
      "1000/1000 - 0s - loss: 0.0125 - acc: 0.9830 - val_loss: 0.0120 - val_acc: 0.9800\n",
      "Epoch 498/500\n",
      "1000/1000 - 0s - loss: 0.0128 - acc: 0.9750 - val_loss: 0.0125 - val_acc: 0.9900\n",
      "Epoch 499/500\n",
      "1000/1000 - 0s - loss: 0.0150 - acc: 0.9550 - val_loss: 0.0127 - val_acc: 0.9700\n",
      "Epoch 500/500\n",
      "1000/1000 - 0s - loss: 0.0139 - acc: 0.9690 - val_loss: 0.0121 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def get_model():\n",
    "    # Create a simple model.\n",
    "    model = tf.keras.Sequential()\n",
    "    dense = tf.keras.layers.Dense(200, input_shape= (8,) , activation=\"tanh\" , kernel_regularizer=l2(0.01))\n",
    "    model.add(dense)  # , input_shape=(train_X.shape[1], train_X.shape[2]))\n",
    "    dense = tf.keras.layers.Dense(200, activation = \"tanh\" , kernel_regularizer=l2(0.01))\n",
    "    model.add(dense) \n",
    "    model.add(tf.keras.layers.Dense(5))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\",  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Train the model.\n",
    "train_input = Xtrain\n",
    "train_target = Ytrain\n",
    "history = model.fit(X_train_scaled, Y_train_scaled,  epochs=500, batch_size=100, \n",
    "              shuffle=True, validation_data=(X_test_scaled, Y_test_scaled) , verbose=2\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:51:04.405697Z",
     "start_time": "2021-01-18T10:51:04.370130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.58577243e+01 2.21886226e+00 1.92316328e-01 2.08395093e-02\n",
      " 9.24537063e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.57833025e+01 2.24331203e+00 1.85352956e-01 3.60496852e-02\n",
      " 9.23335054e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.57325620e+01 2.31218953e+00 1.74549683e-01 5.04837091e-02\n",
      " 9.22405328e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.57149564e+01 2.28438753e+00 1.72796448e-01 6.43529475e-02\n",
      " 9.21756034e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.56342280e+01 2.24316975e+00 1.68323015e-01 7.79525924e-02\n",
      " 9.21384755e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.55077338e+01 2.16539872e+00 1.51677049e-01 9.06601250e-02\n",
      " 9.21278848e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [25.44076304  2.16444571  0.13799468  0.10217209 92.14203482]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [25.35803258  2.17141891  0.1209192   0.11243889 92.18050352]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [25.32113256  2.04483503  0.11294081  0.12176334 92.242704  ]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [25.24427067  2.05123682  0.10139178  0.13052124 92.32586452]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.51691611e+01 2.02951521e+00 8.42863691e-02 1.38005357e-01\n",
      " 9.24287662e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.50150959e+01 1.88183315e+00 6.00973313e-02 1.43631067e-01\n",
      " 9.25503637e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.49497975e+01 1.84035941e+00 4.38083209e-02 1.47961723e-01\n",
      " 9.26870395e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.48493892e+01 1.78259066e+00 2.44270210e-02 1.50617015e-01\n",
      " 9.28370804e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.48344861e+01 1.70811266e+00 1.26320308e-02 1.52045522e-01\n",
      " 9.29990146e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [2.48337775e+01 1.69110163e+00 6.95728513e-03 1.52847484e-01\n",
      " 9.31709832e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [ 2.47966586e+01  1.59996529e+00 -9.71649773e-03  1.52684156e-01\n",
      "  9.33518903e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [ 2.47820990e+01  1.45573653e+00 -8.23664656e-03  1.52051077e-01\n",
      "  9.35397015e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [ 2.51092954e+01  1.00720233e+00 -4.99844172e-01  3.69431025e-02\n",
      "  9.99993599e+01]\n",
      "***********\n",
      "predicted output [2.58534417e+01 2.17515583e+00 1.95552972e-01 2.59433261e-02\n",
      " 9.26166223e+01]\n",
      "target=  [ 2.53645471e+01  2.94049294e-01 -4.23744202e-01 -5.08525898e-04\n",
      "  9.99926474e+01]\n",
      "***********\n"
     ]
    }
   ],
   "source": [
    "y_predic = loaded_model.predict(X_test_scaled)\n",
    "for i in range(20):\n",
    "    print(\"predicted output\", np.multiply(scaler_y.scale_, y_predic[0]) + scaler_y.mean_)\n",
    "    #print(np.multiply(scaler_y.scale_, Y_train_scaled[0]) + scaler_y.mean_)\n",
    "    print(\"target= \" , Ytest[i])\n",
    "    print(\"***********\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(true_value, predicted_value, c='crimson')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "\n",
    "p1 = max(max(predicted_value), max(true_value))\n",
    "p2 = min(min(predicted_value), min(true_value))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.xlabel('True Values', fontsize=15)\n",
    "plt.ylabel('Predictions', fontsize=15)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:55:08.974489Z",
     "start_time": "2021-01-17T18:55:08.966361Z"
    }
   },
   "outputs": [],
   "source": [
    "ytrain = data.loc[:, [\"u\",\"w\",  \"q\",\"theta\", \"height\"]].iloc[:400, :]\n",
    "Xtrain = data.loc[:, [\"prev_u\",\"prev_w\",  \"prev_q\",\"prev_theta\", \"prev_height\",\"elevator\", \"tf1\",\"tf2\"]].iloc[:400, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T14:40:10.341587Z",
     "start_time": "2021-01-13T14:40:10.331744Z"
    }
   },
   "outputs": [],
   "source": [
    "ytest = data.loc[:,[\"elevator\",\"tf1\",\"tf2\",\"u\",\"w\",  \"q\",\"theta\", \"height\"]].iloc[100:200]\n",
    "Xtest = data.loc[:, [\"elevator\",\"tf1\",\"tf2\",\"u\",\"w\",  \"q\",\"theta\", \"height\"]].iloc[100:200, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:03:53.440801Z",
     "start_time": "2021-01-17T18:03:53.303047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(0., 14, 1)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T18:03:41.910697Z",
     "start_time": "2021-01-17T18:03:41.309832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([1, 2, 3, 4])\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T14:41:01.056979Z",
     "start_time": "2021-01-13T14:41:01.029283Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:52:12.823455Z",
     "start_time": "2021-01-13T15:52:12.810554Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_mlp(input_placeholder, output_size=5,  n_layers=2, size=250, activation=tf.tanh, output_activation=None):\n",
    "    \n",
    "    output_placeholder = input_placeholder\n",
    "\n",
    "    for _ in range(n_layers):\n",
    "        output_placeholder = tf.layers.dense(output_placeholder,units=size,activation=activation) # HINT: use tf.layers.dense (specify <input>, <size>, activation=<?>)\n",
    "    output_placeholder = tf.layers.dense(output_placeholder,units=output_size,activation=output_activation) # HINT: use tf.layers.dense (specify <input>, <size>, activation=<?>)\n",
    "    return output_placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, mean, std, eps=1e-8):\n",
    "    return (data-mean)/(std+eps)\n",
    "def unnormalize(data, mean, std):\n",
    "    return data*std+mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:52:16.883747Z",
     "start_time": "2021-01-13T15:52:16.513259Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "obs_pl = tf.placeholder(shape=[None, 8], name=\"ob\", dtype=tf.float32)\n",
    "delta_labels = tf.placeholder(shape=[None, 5], name=\"labels\", dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prefix delta is used in the variable below to denote changes in state, i.e. (s'-s)\n",
    "# self.delta_pred_normalized = build_mlp function and the concatenated_input above to  a neural network that\n",
    "# predicts unnormalized delta states (i.e. change in state)\n",
    "delta_pred_normalized = build_mlp(obs_pl)\n",
    "\n",
    "delta_labels_normalized = delta_labels\n",
    "# compared predicted deltas to labels (both should be normalized)\n",
    "loss = tf.losses.mean_squared_error(delta_labels_normalized,\n",
    "                                         delta_pred_normalized)  # a loss function that takes as input normalized versions of predicted change in state and ground truth change in state\n",
    "train_op = tf.train.AdamOptimizer(0.001).minimize(\n",
    "    loss)  # a train_op to minimize the loss d above. Adam optimizer will work well.\n",
    "\n",
    "    #############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T15:52:22.496574Z",
     "start_time": "2021-01-13T15:52:20.589063Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "feed_dict = {\n",
    "    obs_pl: Xtrain,\n",
    "    delta_labels: Ytrain\n",
    "}\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    _,losss =sess.run( [train_op ,loss] ,\n",
    "                        feed_dict=feed_dict)  # Run the train_op here, and also return the loss being optimized (on this batch of data)\n",
    "print(losss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T16:13:56.664908Z",
     "start_time": "2021-01-13T16:13:56.598960Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "feed_dict = {\n",
    "    obs_pl: Xtest,\n",
    "    delta_labels: Ytest\n",
    "}\n",
    "delta_pred = tf.placeholder(shape=[None, 5], name=\"ob\", dtype=tf.float32)\n",
    "loss = tf.losses.mean_squared_error(delta_labels,\n",
    "                                         delta_pred) \n",
    "\n",
    "\n",
    "pred = sess.run(delta_pred_normalized,feed_dict=feed_dict)\n",
    "                          # Run the train_op here, and also return the loss being optimized (on this batch of data)\n",
    "losses = sess.run(loss,feed_dict = {\n",
    "    delta_labels: Ytest,\n",
    "    delta_pred: pred\n",
    "})\n",
    "print(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T14:10:23.762583Z",
     "start_time": "2021-01-13T14:10:23.747089Z"
    }
   },
   "outputs": [],
   "source": [
    "losss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T16:34:29.634883Z",
     "start_time": "2021-01-13T16:34:27.359923Z"
    }
   },
   "outputs": [],
   "source": [
    "import gpflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T17:51:14.524887Z",
     "start_time": "2021-01-13T17:51:14.506508Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "N=20\n",
    "nd= 21\n",
    "data = np.random.normal(loc=0.5,scale=1,size=(N,nd))\n",
    "value = np.random.random((N,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T17:57:21.097408Z",
     "start_time": "2021-01-13T17:57:21.086993Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T17:58:53.840779Z",
     "start_time": "2021-01-13T17:58:52.058227Z"
    }
   },
   "outputs": [],
   "source": [
    "N=20\n",
    "nd=N+1\n",
    "np.random.seed(0)\n",
    "data = np.random.normal(loc=0.5, scale=1, size=(N, nd))\n",
    "value = np.random.random((N, 1))\n",
    "model = tf.keras.Sequential()\n",
    "dense = tf.keras.layers.Dense(10, input_shape= (nd,) )\n",
    "model.add(dense)  # , input_shape=(train_X.shape[1], train_X.shape[2]))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "model.fit(data, value,batch_size =10, epochs=50, verbose=0,\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T18:42:01.619581Z",
     "start_time": "2021-01-13T18:42:01.516971Z"
    }
   },
   "outputs": [],
   "source": [
    "init=tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "v= model(data)\n",
    "sess.run(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T18:41:32.257228Z",
     "start_time": "2021-01-13T18:41:32.225005Z"
    }
   },
   "outputs": [],
   "source": [
    "model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T18:41:33.239649Z",
     "start_time": "2021-01-13T18:41:33.218688Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T09:54:21.874036Z",
     "start_time": "2021-01-19T09:54:08.216864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/500\n",
      "1000/1000 - 0s - loss: 2094.5376 - acc: 0.7840 - val_loss: 1811.3003 - val_acc: 1.0000\n",
      "Epoch 2/500\n",
      "1000/1000 - 0s - loss: 1960.1203 - acc: 1.0000 - val_loss: 1770.2269 - val_acc: 1.0000\n",
      "Epoch 3/500\n",
      "1000/1000 - 0s - loss: 1928.6205 - acc: 1.0000 - val_loss: 1744.5854 - val_acc: 1.0000\n",
      "Epoch 4/500\n",
      "1000/1000 - 0s - loss: 1902.6224 - acc: 1.0000 - val_loss: 1719.8711 - val_acc: 1.0000\n",
      "Epoch 5/500\n",
      "1000/1000 - 0s - loss: 1877.4057 - acc: 1.0000 - val_loss: 1695.7321 - val_acc: 1.0000\n",
      "Epoch 6/500\n",
      "1000/1000 - 0s - loss: 1852.7222 - acc: 1.0000 - val_loss: 1672.1694 - val_acc: 1.0000\n",
      "Epoch 7/500\n",
      "1000/1000 - 0s - loss: 1828.5790 - acc: 1.0000 - val_loss: 1649.0892 - val_acc: 1.0000\n",
      "Epoch 8/500\n",
      "1000/1000 - 0s - loss: 1804.8833 - acc: 1.0000 - val_loss: 1626.4454 - val_acc: 1.0000\n",
      "Epoch 9/500\n",
      "1000/1000 - 0s - loss: 1781.6071 - acc: 1.0000 - val_loss: 1604.2017 - val_acc: 1.0000\n",
      "Epoch 10/500\n",
      "1000/1000 - 0s - loss: 1758.7366 - acc: 1.0000 - val_loss: 1582.3270 - val_acc: 1.0000\n",
      "Epoch 11/500\n",
      "1000/1000 - 0s - loss: 1736.2210 - acc: 1.0000 - val_loss: 1560.8042 - val_acc: 1.0000\n",
      "Epoch 12/500\n",
      "1000/1000 - 0s - loss: 1714.0732 - acc: 1.0000 - val_loss: 1539.6075 - val_acc: 1.0000\n",
      "Epoch 13/500\n",
      "1000/1000 - 0s - loss: 1692.2332 - acc: 1.0000 - val_loss: 1518.7443 - val_acc: 1.0000\n",
      "Epoch 14/500\n",
      "1000/1000 - 0s - loss: 1670.7195 - acc: 1.0000 - val_loss: 1498.1919 - val_acc: 1.0000\n",
      "Epoch 15/500\n",
      "1000/1000 - 0s - loss: 1649.5410 - acc: 1.0000 - val_loss: 1477.9291 - val_acc: 1.0000\n",
      "Epoch 16/500\n",
      "1000/1000 - 0s - loss: 1628.6087 - acc: 1.0000 - val_loss: 1457.9856 - val_acc: 1.0000\n",
      "Epoch 17/500\n",
      "1000/1000 - 0s - loss: 1608.0123 - acc: 1.0000 - val_loss: 1438.3167 - val_acc: 1.0000\n",
      "Epoch 18/500\n",
      "1000/1000 - 0s - loss: 1587.6927 - acc: 1.0000 - val_loss: 1418.9260 - val_acc: 1.0000\n",
      "Epoch 19/500\n",
      "1000/1000 - 0s - loss: 1567.6547 - acc: 1.0000 - val_loss: 1399.8025 - val_acc: 1.0000\n",
      "Epoch 20/500\n",
      "1000/1000 - 0s - loss: 1547.8746 - acc: 1.0000 - val_loss: 1380.9540 - val_acc: 1.0000\n",
      "Epoch 21/500\n",
      "1000/1000 - 0s - loss: 1528.3845 - acc: 1.0000 - val_loss: 1362.3564 - val_acc: 1.0000\n",
      "Epoch 22/500\n",
      "1000/1000 - 0s - loss: 1509.1293 - acc: 1.0000 - val_loss: 1344.0325 - val_acc: 1.0000\n",
      "Epoch 23/500\n",
      "1000/1000 - 0s - loss: 1490.1488 - acc: 1.0000 - val_loss: 1325.9495 - val_acc: 1.0000\n",
      "Epoch 24/500\n",
      "1000/1000 - 0s - loss: 1471.4069 - acc: 1.0000 - val_loss: 1308.1184 - val_acc: 1.0000\n",
      "Epoch 25/500\n",
      "1000/1000 - 0s - loss: 1452.9046 - acc: 1.0000 - val_loss: 1290.5367 - val_acc: 1.0000\n",
      "Epoch 26/500\n",
      "1000/1000 - 0s - loss: 1434.6839 - acc: 1.0000 - val_loss: 1273.1729 - val_acc: 1.0000\n",
      "Epoch 27/500\n",
      "1000/1000 - 0s - loss: 1416.6555 - acc: 1.0000 - val_loss: 1256.0593 - val_acc: 1.0000\n",
      "Epoch 28/500\n",
      "1000/1000 - 0s - loss: 1398.8898 - acc: 1.0000 - val_loss: 1239.1581 - val_acc: 1.0000\n",
      "Epoch 29/500\n",
      "1000/1000 - 0s - loss: 1381.3160 - acc: 1.0000 - val_loss: 1222.5093 - val_acc: 1.0000\n",
      "Epoch 30/500\n",
      "1000/1000 - 0s - loss: 1364.0078 - acc: 1.0000 - val_loss: 1206.0599 - val_acc: 1.0000\n",
      "Epoch 31/500\n",
      "1000/1000 - 0s - loss: 1346.9008 - acc: 1.0000 - val_loss: 1189.8347 - val_acc: 1.0000\n",
      "Epoch 32/500\n",
      "1000/1000 - 0s - loss: 1330.0212 - acc: 1.0000 - val_loss: 1173.8141 - val_acc: 1.0000\n",
      "Epoch 33/500\n",
      "1000/1000 - 0s - loss: 1313.3445 - acc: 1.0000 - val_loss: 1158.0079 - val_acc: 1.0000\n",
      "Epoch 34/500\n",
      "1000/1000 - 0s - loss: 1296.8694 - acc: 1.0000 - val_loss: 1142.4122 - val_acc: 1.0000\n",
      "Epoch 35/500\n",
      "1000/1000 - 0s - loss: 1280.6093 - acc: 1.0000 - val_loss: 1127.0210 - val_acc: 1.0000\n",
      "Epoch 36/500\n",
      "1000/1000 - 0s - loss: 1264.5442 - acc: 1.0000 - val_loss: 1111.8374 - val_acc: 1.0000\n",
      "Epoch 37/500\n",
      "1000/1000 - 0s - loss: 1248.7069 - acc: 1.0000 - val_loss: 1096.8322 - val_acc: 1.0000\n",
      "Epoch 38/500\n",
      "1000/1000 - 0s - loss: 1233.0320 - acc: 1.0000 - val_loss: 1082.0308 - val_acc: 1.0000\n",
      "Epoch 39/500\n",
      "1000/1000 - 0s - loss: 1217.5653 - acc: 1.0000 - val_loss: 1067.4058 - val_acc: 1.0000\n",
      "Epoch 40/500\n",
      "1000/1000 - 0s - loss: 1202.2686 - acc: 1.0000 - val_loss: 1052.9854 - val_acc: 1.0000\n",
      "Epoch 41/500\n",
      "1000/1000 - 0s - loss: 1187.1898 - acc: 1.0000 - val_loss: 1038.7307 - val_acc: 1.0000\n",
      "Epoch 42/500\n",
      "1000/1000 - 0s - loss: 1172.2818 - acc: 1.0000 - val_loss: 1024.6560 - val_acc: 1.0000\n",
      "Epoch 43/500\n",
      "1000/1000 - 0s - loss: 1157.5231 - acc: 1.0000 - val_loss: 1010.7695 - val_acc: 1.0000\n",
      "Epoch 44/500\n",
      "1000/1000 - 0s - loss: 1142.9784 - acc: 1.0000 - val_loss: 997.0438 - val_acc: 1.0000\n",
      "Epoch 45/500\n",
      "1000/1000 - 0s - loss: 1128.5943 - acc: 1.0000 - val_loss: 983.4966 - val_acc: 1.0000\n",
      "Epoch 46/500\n",
      "1000/1000 - 0s - loss: 1114.3635 - acc: 1.0000 - val_loss: 970.1307 - val_acc: 1.0000\n",
      "Epoch 47/500\n",
      "1000/1000 - 0s - loss: 1100.3331 - acc: 1.0000 - val_loss: 956.9122 - val_acc: 1.0000\n",
      "Epoch 48/500\n",
      "1000/1000 - 0s - loss: 1086.4414 - acc: 1.0000 - val_loss: 943.8589 - val_acc: 1.0000\n",
      "Epoch 49/500\n",
      "1000/1000 - 0s - loss: 1072.7309 - acc: 1.0000 - val_loss: 930.9656 - val_acc: 1.0000\n",
      "Epoch 50/500\n",
      "1000/1000 - 0s - loss: 1059.1615 - acc: 1.0000 - val_loss: 918.2371 - val_acc: 1.0000\n",
      "Epoch 51/500\n",
      "1000/1000 - 0s - loss: 1045.7753 - acc: 1.0000 - val_loss: 905.6454 - val_acc: 1.0000\n",
      "Epoch 52/500\n",
      "1000/1000 - 0s - loss: 1032.5216 - acc: 1.0000 - val_loss: 893.2104 - val_acc: 1.0000\n",
      "Epoch 53/500\n",
      "1000/1000 - 0s - loss: 1019.4017 - acc: 1.0000 - val_loss: 880.9419 - val_acc: 1.0000\n",
      "Epoch 54/500\n",
      "1000/1000 - 0s - loss: 1006.4763 - acc: 1.0000 - val_loss: 868.8102 - val_acc: 1.0000\n",
      "Epoch 55/500\n",
      "1000/1000 - 0s - loss: 993.6844 - acc: 1.0000 - val_loss: 856.8115 - val_acc: 1.0000\n",
      "Epoch 56/500\n",
      "1000/1000 - 0s - loss: 981.0106 - acc: 1.0000 - val_loss: 844.9775 - val_acc: 1.0000\n",
      "Epoch 57/500\n",
      "1000/1000 - 0s - loss: 968.5153 - acc: 1.0000 - val_loss: 833.2703 - val_acc: 1.0000\n",
      "Epoch 58/500\n",
      "1000/1000 - 0s - loss: 956.1337 - acc: 1.0000 - val_loss: 821.7040 - val_acc: 1.0000\n",
      "Epoch 59/500\n",
      "1000/1000 - 0s - loss: 943.9094 - acc: 1.0000 - val_loss: 810.2724 - val_acc: 1.0000\n",
      "Epoch 60/500\n",
      "1000/1000 - 0s - loss: 931.8083 - acc: 1.0000 - val_loss: 798.9775 - val_acc: 1.0000\n",
      "Epoch 61/500\n",
      "1000/1000 - 0s - loss: 919.8391 - acc: 1.0000 - val_loss: 787.8293 - val_acc: 1.0000\n",
      "Epoch 62/500\n",
      "1000/1000 - 0s - loss: 908.0378 - acc: 1.0000 - val_loss: 776.7810 - val_acc: 1.0000\n",
      "Epoch 63/500\n",
      "1000/1000 - 0s - loss: 896.3215 - acc: 1.0000 - val_loss: 765.8781 - val_acc: 1.0000\n",
      "Epoch 64/500\n",
      "1000/1000 - 0s - loss: 884.7624 - acc: 1.0000 - val_loss: 755.0950 - val_acc: 1.0000\n",
      "Epoch 65/500\n",
      "1000/1000 - 0s - loss: 873.3136 - acc: 1.0000 - val_loss: 744.4390 - val_acc: 1.0000\n",
      "Epoch 66/500\n",
      "1000/1000 - 0s - loss: 861.9859 - acc: 1.0000 - val_loss: 733.9174 - val_acc: 1.0000\n",
      "Epoch 67/500\n",
      "1000/1000 - 0s - loss: 850.8070 - acc: 1.0000 - val_loss: 723.5200 - val_acc: 1.0000\n",
      "Epoch 68/500\n",
      "1000/1000 - 0s - loss: 839.7471 - acc: 1.0000 - val_loss: 713.2186 - val_acc: 1.0000\n",
      "Epoch 69/500\n",
      "1000/1000 - 0s - loss: 828.7695 - acc: 1.0000 - val_loss: 703.0551 - val_acc: 1.0000\n",
      "Epoch 70/500\n",
      "1000/1000 - 0s - loss: 817.9533 - acc: 1.0000 - val_loss: 692.9999 - val_acc: 1.0000\n",
      "Epoch 71/500\n",
      "1000/1000 - 0s - loss: 807.2451 - acc: 1.0000 - val_loss: 683.0608 - val_acc: 1.0000\n",
      "Epoch 72/500\n",
      "1000/1000 - 0s - loss: 796.6371 - acc: 1.0000 - val_loss: 673.2297 - val_acc: 1.0000\n",
      "Epoch 73/500\n",
      "1000/1000 - 0s - loss: 786.1559 - acc: 1.0000 - val_loss: 663.5247 - val_acc: 1.0000\n",
      "Epoch 74/500\n",
      "1000/1000 - 0s - loss: 775.7824 - acc: 1.0000 - val_loss: 653.9183 - val_acc: 1.0000\n",
      "Epoch 75/500\n",
      "1000/1000 - 0s - loss: 765.5215 - acc: 1.0000 - val_loss: 644.4363 - val_acc: 1.0000\n",
      "Epoch 76/500\n",
      "1000/1000 - 0s - loss: 755.3657 - acc: 1.0000 - val_loss: 635.0538 - val_acc: 1.0000\n",
      "Epoch 77/500\n",
      "1000/1000 - 0s - loss: 745.3522 - acc: 1.0000 - val_loss: 625.7692 - val_acc: 1.0000\n",
      "Epoch 78/500\n",
      "1000/1000 - 0s - loss: 735.4136 - acc: 1.0000 - val_loss: 616.5918 - val_acc: 1.0000\n",
      "Epoch 79/500\n",
      "1000/1000 - 0s - loss: 725.5818 - acc: 1.0000 - val_loss: 607.5262 - val_acc: 1.0000\n",
      "Epoch 80/500\n",
      "1000/1000 - 0s - loss: 715.8578 - acc: 1.0000 - val_loss: 598.5597 - val_acc: 1.0000\n",
      "Epoch 81/500\n",
      "1000/1000 - 0s - loss: 706.2484 - acc: 1.0000 - val_loss: 589.6987 - val_acc: 1.0000\n",
      "Epoch 82/500\n",
      "1000/1000 - 0s - loss: 696.7220 - acc: 1.0000 - val_loss: 580.9465 - val_acc: 1.0000\n",
      "Epoch 83/500\n",
      "1000/1000 - 0s - loss: 687.3332 - acc: 1.0000 - val_loss: 572.2856 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "1000/1000 - 0s - loss: 678.0103 - acc: 1.0000 - val_loss: 563.7312 - val_acc: 1.0000\n",
      "Epoch 85/500\n",
      "1000/1000 - 0s - loss: 668.8377 - acc: 1.0000 - val_loss: 555.2585 - val_acc: 1.0000\n",
      "Epoch 86/500\n",
      "1000/1000 - 0s - loss: 659.6938 - acc: 1.0000 - val_loss: 546.8859 - val_acc: 1.0000\n",
      "Epoch 87/500\n",
      "1000/1000 - 0s - loss: 650.6854 - acc: 1.0000 - val_loss: 538.6213 - val_acc: 1.0000\n",
      "Epoch 88/500\n",
      "1000/1000 - 0s - loss: 641.7891 - acc: 1.0000 - val_loss: 530.4529 - val_acc: 1.0000\n",
      "Epoch 89/500\n",
      "1000/1000 - 0s - loss: 632.9631 - acc: 1.0000 - val_loss: 522.3687 - val_acc: 1.0000\n",
      "Epoch 90/500\n",
      "1000/1000 - 0s - loss: 624.2415 - acc: 1.0000 - val_loss: 514.3818 - val_acc: 1.0000\n",
      "Epoch 91/500\n",
      "1000/1000 - 0s - loss: 615.6200 - acc: 1.0000 - val_loss: 506.4893 - val_acc: 1.0000\n",
      "Epoch 92/500\n",
      "1000/1000 - 0s - loss: 607.0967 - acc: 1.0000 - val_loss: 498.6748 - val_acc: 1.0000\n",
      "Epoch 93/500\n",
      "1000/1000 - 0s - loss: 598.6483 - acc: 1.0000 - val_loss: 490.9588 - val_acc: 1.0000\n",
      "Epoch 94/500\n",
      "1000/1000 - 0s - loss: 590.2970 - acc: 1.0000 - val_loss: 483.3372 - val_acc: 1.0000\n",
      "Epoch 95/500\n",
      "1000/1000 - 0s - loss: 582.0409 - acc: 1.0000 - val_loss: 475.8176 - val_acc: 1.0000\n",
      "Epoch 96/500\n",
      "1000/1000 - 0s - loss: 573.8832 - acc: 1.0000 - val_loss: 468.3681 - val_acc: 1.0000\n",
      "Epoch 97/500\n",
      "1000/1000 - 0s - loss: 565.8213 - acc: 1.0000 - val_loss: 461.0017 - val_acc: 1.0000\n",
      "Epoch 98/500\n",
      "1000/1000 - 0s - loss: 557.8129 - acc: 1.0000 - val_loss: 453.7293 - val_acc: 1.0000\n",
      "Epoch 99/500\n",
      "1000/1000 - 0s - loss: 549.9372 - acc: 1.0000 - val_loss: 446.5366 - val_acc: 1.0000\n",
      "Epoch 100/500\n",
      "1000/1000 - 0s - loss: 542.1026 - acc: 1.0000 - val_loss: 439.4403 - val_acc: 1.0000\n",
      "Epoch 101/500\n",
      "1000/1000 - 0s - loss: 534.3931 - acc: 1.0000 - val_loss: 432.4304 - val_acc: 1.0000\n",
      "Epoch 102/500\n",
      "1000/1000 - 0s - loss: 526.7643 - acc: 1.0000 - val_loss: 425.4921 - val_acc: 1.0000\n",
      "Epoch 103/500\n",
      "1000/1000 - 0s - loss: 519.2068 - acc: 1.0000 - val_loss: 418.6345 - val_acc: 1.0000\n",
      "Epoch 104/500\n",
      "1000/1000 - 0s - loss: 511.7467 - acc: 1.0000 - val_loss: 411.8625 - val_acc: 1.0000\n",
      "Epoch 105/500\n",
      "1000/1000 - 0s - loss: 504.3528 - acc: 1.0000 - val_loss: 405.1752 - val_acc: 1.0000\n",
      "Epoch 106/500\n",
      "1000/1000 - 0s - loss: 497.0415 - acc: 1.0000 - val_loss: 398.5880 - val_acc: 1.0000\n",
      "Epoch 107/500\n",
      "1000/1000 - 0s - loss: 489.8607 - acc: 1.0000 - val_loss: 392.0570 - val_acc: 1.0000\n",
      "Epoch 108/500\n",
      "1000/1000 - 0s - loss: 482.7064 - acc: 1.0000 - val_loss: 385.6045 - val_acc: 1.0000\n",
      "Epoch 109/500\n",
      "1000/1000 - 0s - loss: 475.6485 - acc: 1.0000 - val_loss: 379.2426 - val_acc: 1.0000\n",
      "Epoch 110/500\n",
      "1000/1000 - 0s - loss: 468.6745 - acc: 1.0000 - val_loss: 372.9651 - val_acc: 1.0000\n",
      "Epoch 111/500\n",
      "1000/1000 - 0s - loss: 461.7960 - acc: 1.0000 - val_loss: 366.7519 - val_acc: 1.0000\n",
      "Epoch 112/500\n",
      "1000/1000 - 0s - loss: 454.9825 - acc: 1.0000 - val_loss: 360.6220 - val_acc: 1.0000\n",
      "Epoch 113/500\n",
      "1000/1000 - 0s - loss: 448.2634 - acc: 1.0000 - val_loss: 354.5516 - val_acc: 1.0000\n",
      "Epoch 114/500\n",
      "1000/1000 - 0s - loss: 441.5876 - acc: 1.0000 - val_loss: 348.5686 - val_acc: 1.0000\n",
      "Epoch 115/500\n",
      "1000/1000 - 0s - loss: 434.9961 - acc: 1.0000 - val_loss: 342.6756 - val_acc: 1.0000\n",
      "Epoch 116/500\n",
      "1000/1000 - 0s - loss: 428.5305 - acc: 1.0000 - val_loss: 336.8411 - val_acc: 1.0000\n",
      "Epoch 117/500\n",
      "1000/1000 - 0s - loss: 422.0924 - acc: 1.0000 - val_loss: 331.0800 - val_acc: 1.0000\n",
      "Epoch 118/500\n",
      "1000/1000 - 0s - loss: 415.7404 - acc: 1.0000 - val_loss: 325.3970 - val_acc: 1.0000\n",
      "Epoch 119/500\n",
      "1000/1000 - 0s - loss: 409.4762 - acc: 1.0000 - val_loss: 319.7891 - val_acc: 1.0000\n",
      "Epoch 120/500\n",
      "1000/1000 - 0s - loss: 403.2704 - acc: 1.0000 - val_loss: 314.2433 - val_acc: 1.0000\n",
      "Epoch 121/500\n",
      "1000/1000 - 0s - loss: 397.1574 - acc: 1.0000 - val_loss: 308.7686 - val_acc: 1.0000\n",
      "Epoch 122/500\n",
      "1000/1000 - 0s - loss: 391.1034 - acc: 1.0000 - val_loss: 303.3757 - val_acc: 1.0000\n",
      "Epoch 123/500\n",
      "1000/1000 - 0s - loss: 385.1280 - acc: 1.0000 - val_loss: 298.0391 - val_acc: 1.0000\n",
      "Epoch 124/500\n",
      "1000/1000 - 0s - loss: 379.2172 - acc: 1.0000 - val_loss: 292.7812 - val_acc: 1.0000\n",
      "Epoch 125/500\n",
      "1000/1000 - 0s - loss: 373.3789 - acc: 1.0000 - val_loss: 287.6040 - val_acc: 1.0000\n",
      "Epoch 126/500\n",
      "1000/1000 - 0s - loss: 367.6197 - acc: 1.0000 - val_loss: 282.4824 - val_acc: 1.0000\n",
      "Epoch 127/500\n",
      "1000/1000 - 0s - loss: 361.9301 - acc: 1.0000 - val_loss: 277.4348 - val_acc: 1.0000\n",
      "Epoch 128/500\n",
      "1000/1000 - 0s - loss: 356.3112 - acc: 1.0000 - val_loss: 272.4461 - val_acc: 1.0000\n",
      "Epoch 129/500\n",
      "1000/1000 - 0s - loss: 350.7587 - acc: 1.0000 - val_loss: 267.5273 - val_acc: 1.0000\n",
      "Epoch 130/500\n",
      "1000/1000 - 0s - loss: 345.2851 - acc: 1.0000 - val_loss: 262.6774 - val_acc: 1.0000\n",
      "Epoch 131/500\n",
      "1000/1000 - 0s - loss: 339.8457 - acc: 1.0000 - val_loss: 257.8943 - val_acc: 1.0000\n",
      "Epoch 132/500\n",
      "1000/1000 - 0s - loss: 334.5201 - acc: 1.0000 - val_loss: 253.1647 - val_acc: 1.0000\n",
      "Epoch 133/500\n",
      "1000/1000 - 0s - loss: 329.2284 - acc: 1.0000 - val_loss: 248.5142 - val_acc: 1.0000\n",
      "Epoch 134/500\n",
      "1000/1000 - 0s - loss: 324.0334 - acc: 1.0000 - val_loss: 243.9138 - val_acc: 1.0000\n",
      "Epoch 135/500\n",
      "1000/1000 - 0s - loss: 318.8715 - acc: 1.0000 - val_loss: 239.3845 - val_acc: 1.0000\n",
      "Epoch 136/500\n",
      "1000/1000 - 0s - loss: 313.7948 - acc: 1.0000 - val_loss: 234.9152 - val_acc: 1.0000\n",
      "Epoch 137/500\n",
      "1000/1000 - 0s - loss: 308.7846 - acc: 1.0000 - val_loss: 230.5098 - val_acc: 1.0000\n",
      "Epoch 138/500\n",
      "1000/1000 - 0s - loss: 303.8325 - acc: 1.0000 - val_loss: 226.1538 - val_acc: 1.0000\n",
      "Epoch 139/500\n",
      "1000/1000 - 0s - loss: 298.9348 - acc: 1.0000 - val_loss: 221.8714 - val_acc: 1.0000\n",
      "Epoch 140/500\n",
      "1000/1000 - 0s - loss: 294.1110 - acc: 1.0000 - val_loss: 217.6613 - val_acc: 1.0000\n",
      "Epoch 141/500\n",
      "1000/1000 - 0s - loss: 289.3445 - acc: 1.0000 - val_loss: 213.5027 - val_acc: 1.0000\n",
      "Epoch 142/500\n",
      "1000/1000 - 0s - loss: 284.6495 - acc: 1.0000 - val_loss: 209.4026 - val_acc: 1.0000\n",
      "Epoch 143/500\n",
      "1000/1000 - 0s - loss: 280.0293 - acc: 1.0000 - val_loss: 205.3515 - val_acc: 1.0000\n",
      "Epoch 144/500\n",
      "1000/1000 - 0s - loss: 275.4348 - acc: 1.0000 - val_loss: 201.3665 - val_acc: 1.0000\n",
      "Epoch 145/500\n",
      "1000/1000 - 0s - loss: 270.9245 - acc: 1.0000 - val_loss: 197.4395 - val_acc: 1.0000\n",
      "Epoch 146/500\n",
      "1000/1000 - 0s - loss: 266.4563 - acc: 1.0000 - val_loss: 193.5764 - val_acc: 1.0000\n",
      "Epoch 147/500\n",
      "1000/1000 - 0s - loss: 262.0770 - acc: 1.0000 - val_loss: 189.7679 - val_acc: 1.0000\n",
      "Epoch 148/500\n",
      "1000/1000 - 0s - loss: 257.7435 - acc: 1.0000 - val_loss: 186.0028 - val_acc: 1.0000\n",
      "Epoch 149/500\n",
      "1000/1000 - 0s - loss: 253.4589 - acc: 1.0000 - val_loss: 182.3025 - val_acc: 1.0000\n",
      "Epoch 150/500\n",
      "1000/1000 - 0s - loss: 249.2347 - acc: 1.0000 - val_loss: 178.6549 - val_acc: 1.0000\n",
      "Epoch 151/500\n",
      "1000/1000 - 0s - loss: 245.0776 - acc: 1.0000 - val_loss: 175.0686 - val_acc: 1.0000\n",
      "Epoch 152/500\n",
      "1000/1000 - 0s - loss: 240.9681 - acc: 1.0000 - val_loss: 171.5351 - val_acc: 1.0000\n",
      "Epoch 153/500\n",
      "1000/1000 - 0s - loss: 236.9253 - acc: 1.0000 - val_loss: 168.0486 - val_acc: 1.0000\n",
      "Epoch 154/500\n",
      "1000/1000 - 0s - loss: 232.9219 - acc: 1.0000 - val_loss: 164.6218 - val_acc: 1.0000\n",
      "Epoch 155/500\n",
      "1000/1000 - 0s - loss: 228.9980 - acc: 1.0000 - val_loss: 161.2483 - val_acc: 1.0000\n",
      "Epoch 156/500\n",
      "1000/1000 - 0s - loss: 225.1099 - acc: 1.0000 - val_loss: 157.9195 - val_acc: 1.0000\n",
      "Epoch 157/500\n",
      "1000/1000 - 0s - loss: 221.2797 - acc: 1.0000 - val_loss: 154.6439 - val_acc: 1.0000\n",
      "Epoch 158/500\n",
      "1000/1000 - 0s - loss: 217.5075 - acc: 1.0000 - val_loss: 151.4268 - val_acc: 1.0000\n",
      "Epoch 159/500\n",
      "1000/1000 - 0s - loss: 213.7877 - acc: 1.0000 - val_loss: 148.2594 - val_acc: 1.0000\n",
      "Epoch 160/500\n",
      "1000/1000 - 0s - loss: 210.1242 - acc: 1.0000 - val_loss: 145.1307 - val_acc: 1.0000\n",
      "Epoch 161/500\n",
      "1000/1000 - 0s - loss: 206.5105 - acc: 1.0000 - val_loss: 142.0589 - val_acc: 1.0000\n",
      "Epoch 162/500\n",
      "1000/1000 - 0s - loss: 202.9589 - acc: 1.0000 - val_loss: 139.0263 - val_acc: 1.0000\n",
      "Epoch 163/500\n",
      "1000/1000 - 0s - loss: 199.4273 - acc: 1.0000 - val_loss: 136.0507 - val_acc: 1.0000\n",
      "Epoch 164/500\n",
      "1000/1000 - 0s - loss: 195.9597 - acc: 1.0000 - val_loss: 133.1297 - val_acc: 1.0000\n",
      "Epoch 165/500\n",
      "1000/1000 - 0s - loss: 192.5706 - acc: 1.0000 - val_loss: 130.2556 - val_acc: 1.0000\n",
      "Epoch 166/500\n",
      "1000/1000 - 0s - loss: 189.2046 - acc: 1.0000 - val_loss: 127.4231 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/500\n",
      "1000/1000 - 0s - loss: 185.8906 - acc: 1.0000 - val_loss: 124.6425 - val_acc: 1.0000\n",
      "Epoch 168/500\n",
      "1000/1000 - 0s - loss: 182.6313 - acc: 1.0000 - val_loss: 121.8950 - val_acc: 1.0000\n",
      "Epoch 169/500\n",
      "1000/1000 - 0s - loss: 179.4194 - acc: 1.0000 - val_loss: 119.2029 - val_acc: 1.0000\n",
      "Epoch 170/500\n",
      "1000/1000 - 0s - loss: 176.2616 - acc: 1.0000 - val_loss: 116.5630 - val_acc: 1.0000\n",
      "Epoch 171/500\n",
      "1000/1000 - 0s - loss: 173.1376 - acc: 1.0000 - val_loss: 113.9458 - val_acc: 1.0000\n",
      "Epoch 172/500\n",
      "1000/1000 - 0s - loss: 170.0640 - acc: 1.0000 - val_loss: 111.3933 - val_acc: 1.0000\n",
      "Epoch 173/500\n",
      "1000/1000 - 0s - loss: 167.0404 - acc: 1.0000 - val_loss: 108.8772 - val_acc: 1.0000\n",
      "Epoch 174/500\n",
      "1000/1000 - 0s - loss: 164.0556 - acc: 1.0000 - val_loss: 106.4158 - val_acc: 1.0000\n",
      "Epoch 175/500\n",
      "1000/1000 - 0s - loss: 161.1457 - acc: 1.0000 - val_loss: 103.9789 - val_acc: 1.0000\n",
      "Epoch 176/500\n",
      "1000/1000 - 0s - loss: 158.2462 - acc: 1.0000 - val_loss: 101.5862 - val_acc: 1.0000\n",
      "Epoch 177/500\n",
      "1000/1000 - 0s - loss: 155.4037 - acc: 1.0000 - val_loss: 99.2541 - val_acc: 1.0000\n",
      "Epoch 178/500\n",
      "1000/1000 - 0s - loss: 152.6064 - acc: 1.0000 - val_loss: 96.9441 - val_acc: 1.0000\n",
      "Epoch 179/500\n",
      "1000/1000 - 0s - loss: 149.8555 - acc: 1.0000 - val_loss: 94.6787 - val_acc: 1.0000\n",
      "Epoch 180/500\n",
      "1000/1000 - 0s - loss: 147.1426 - acc: 1.0000 - val_loss: 92.4628 - val_acc: 1.0000\n",
      "Epoch 181/500\n",
      "1000/1000 - 0s - loss: 144.4746 - acc: 1.0000 - val_loss: 90.2741 - val_acc: 1.0000\n",
      "Epoch 182/500\n",
      "1000/1000 - 0s - loss: 141.8567 - acc: 1.0000 - val_loss: 88.1272 - val_acc: 1.0000\n",
      "Epoch 183/500\n",
      "1000/1000 - 0s - loss: 139.2709 - acc: 1.0000 - val_loss: 86.0269 - val_acc: 1.0000\n",
      "Epoch 184/500\n",
      "1000/1000 - 0s - loss: 136.7167 - acc: 1.0000 - val_loss: 83.9578 - val_acc: 1.0000\n",
      "Epoch 185/500\n",
      "1000/1000 - 0s - loss: 134.2280 - acc: 1.0000 - val_loss: 81.9335 - val_acc: 1.0000\n",
      "Epoch 186/500\n",
      "1000/1000 - 0s - loss: 131.7627 - acc: 1.0000 - val_loss: 79.9454 - val_acc: 1.0000\n",
      "Epoch 187/500\n",
      "1000/1000 - 0s - loss: 129.3443 - acc: 1.0000 - val_loss: 77.9870 - val_acc: 1.0000\n",
      "Epoch 188/500\n",
      "1000/1000 - 0s - loss: 126.9614 - acc: 1.0000 - val_loss: 76.0732 - val_acc: 1.0000\n",
      "Epoch 189/500\n",
      "1000/1000 - 0s - loss: 124.6208 - acc: 1.0000 - val_loss: 74.1941 - val_acc: 1.0000\n",
      "Epoch 190/500\n",
      "1000/1000 - 0s - loss: 122.3121 - acc: 1.0000 - val_loss: 72.3538 - val_acc: 1.0000\n",
      "Epoch 191/500\n",
      "1000/1000 - 0s - loss: 120.0583 - acc: 1.0000 - val_loss: 70.5497 - val_acc: 1.0000\n",
      "Epoch 192/500\n",
      "1000/1000 - 0s - loss: 117.8386 - acc: 1.0000 - val_loss: 68.7683 - val_acc: 1.0000\n",
      "Epoch 193/500\n",
      "1000/1000 - 0s - loss: 115.6401 - acc: 1.0000 - val_loss: 67.0242 - val_acc: 1.0000\n",
      "Epoch 194/500\n",
      "1000/1000 - 0s - loss: 113.4896 - acc: 1.0000 - val_loss: 65.3227 - val_acc: 1.0000\n",
      "Epoch 195/500\n",
      "1000/1000 - 0s - loss: 111.3671 - acc: 1.0000 - val_loss: 63.6536 - val_acc: 1.0000\n",
      "Epoch 196/500\n",
      "1000/1000 - 0s - loss: 109.2957 - acc: 1.0000 - val_loss: 62.0155 - val_acc: 1.0000\n",
      "Epoch 197/500\n",
      "1000/1000 - 0s - loss: 107.2457 - acc: 1.0000 - val_loss: 60.4055 - val_acc: 1.0000\n",
      "Epoch 198/500\n",
      "1000/1000 - 0s - loss: 105.2449 - acc: 1.0000 - val_loss: 58.8359 - val_acc: 1.0000\n",
      "Epoch 199/500\n",
      "1000/1000 - 0s - loss: 103.2660 - acc: 1.0000 - val_loss: 57.2940 - val_acc: 1.0000\n",
      "Epoch 200/500\n",
      "1000/1000 - 0s - loss: 101.3319 - acc: 1.0000 - val_loss: 55.7934 - val_acc: 1.0000\n",
      "Epoch 201/500\n",
      "1000/1000 - 0s - loss: 99.4219 - acc: 1.0000 - val_loss: 54.3048 - val_acc: 1.0000\n",
      "Epoch 202/500\n",
      "1000/1000 - 0s - loss: 97.5555 - acc: 1.0000 - val_loss: 52.8666 - val_acc: 1.0000\n",
      "Epoch 203/500\n",
      "1000/1000 - 0s - loss: 95.7141 - acc: 1.0000 - val_loss: 51.4469 - val_acc: 1.0000\n",
      "Epoch 204/500\n",
      "1000/1000 - 0s - loss: 93.9118 - acc: 1.0000 - val_loss: 50.0624 - val_acc: 1.0000\n",
      "Epoch 205/500\n",
      "1000/1000 - 0s - loss: 92.1408 - acc: 1.0000 - val_loss: 48.7032 - val_acc: 1.0000\n",
      "Epoch 206/500\n",
      "1000/1000 - 0s - loss: 90.4000 - acc: 1.0000 - val_loss: 47.3654 - val_acc: 1.0000\n",
      "Epoch 207/500\n",
      "1000/1000 - 0s - loss: 88.6842 - acc: 1.0000 - val_loss: 46.0700 - val_acc: 1.0000\n",
      "Epoch 208/500\n",
      "1000/1000 - 0s - loss: 87.0062 - acc: 1.0000 - val_loss: 44.7948 - val_acc: 1.0000\n",
      "Epoch 209/500\n",
      "1000/1000 - 0s - loss: 85.3643 - acc: 1.0000 - val_loss: 43.5554 - val_acc: 1.0000\n",
      "Epoch 210/500\n",
      "1000/1000 - 0s - loss: 83.7520 - acc: 1.0000 - val_loss: 42.3425 - val_acc: 1.0000\n",
      "Epoch 211/500\n",
      "1000/1000 - 0s - loss: 82.1595 - acc: 1.0000 - val_loss: 41.1419 - val_acc: 1.0000\n",
      "Epoch 212/500\n",
      "1000/1000 - 0s - loss: 80.5947 - acc: 1.0000 - val_loss: 39.9774 - val_acc: 1.0000\n",
      "Epoch 213/500\n",
      "1000/1000 - 0s - loss: 79.0718 - acc: 1.0000 - val_loss: 38.8465 - val_acc: 1.0000\n",
      "Epoch 214/500\n",
      "1000/1000 - 0s - loss: 77.5703 - acc: 1.0000 - val_loss: 37.7390 - val_acc: 1.0000\n",
      "Epoch 215/500\n",
      "1000/1000 - 0s - loss: 76.1002 - acc: 1.0000 - val_loss: 36.6599 - val_acc: 1.0000\n",
      "Epoch 216/500\n",
      "1000/1000 - 0s - loss: 74.6606 - acc: 1.0000 - val_loss: 35.5921 - val_acc: 1.0000\n",
      "Epoch 217/500\n",
      "1000/1000 - 0s - loss: 73.2398 - acc: 1.0000 - val_loss: 34.5534 - val_acc: 1.0000\n",
      "Epoch 218/500\n",
      "1000/1000 - 0s - loss: 71.8502 - acc: 1.0000 - val_loss: 33.5484 - val_acc: 1.0000\n",
      "Epoch 219/500\n",
      "1000/1000 - 0s - loss: 70.4972 - acc: 1.0000 - val_loss: 32.5610 - val_acc: 1.0000\n",
      "Epoch 220/500\n",
      "1000/1000 - 0s - loss: 69.1678 - acc: 1.0000 - val_loss: 31.6022 - val_acc: 1.0000\n",
      "Epoch 221/500\n",
      "1000/1000 - 0s - loss: 67.8443 - acc: 1.0000 - val_loss: 30.6520 - val_acc: 1.0000\n",
      "Epoch 222/500\n",
      "1000/1000 - 0s - loss: 66.5587 - acc: 1.0000 - val_loss: 29.7385 - val_acc: 1.0000\n",
      "Epoch 223/500\n",
      "1000/1000 - 0s - loss: 65.3185 - acc: 1.0000 - val_loss: 28.8461 - val_acc: 1.0000\n",
      "Epoch 224/500\n",
      "1000/1000 - 0s - loss: 64.0717 - acc: 1.0000 - val_loss: 27.9717 - val_acc: 1.0000\n",
      "Epoch 225/500\n",
      "1000/1000 - 0s - loss: 62.8660 - acc: 1.0000 - val_loss: 27.1245 - val_acc: 1.0000\n",
      "Epoch 226/500\n",
      "1000/1000 - 0s - loss: 61.6868 - acc: 1.0000 - val_loss: 26.2974 - val_acc: 1.0000\n",
      "Epoch 227/500\n",
      "1000/1000 - 0s - loss: 60.5281 - acc: 1.0000 - val_loss: 25.4834 - val_acc: 1.0000\n",
      "Epoch 228/500\n",
      "1000/1000 - 0s - loss: 59.3886 - acc: 1.0000 - val_loss: 24.6947 - val_acc: 1.0000\n",
      "Epoch 229/500\n",
      "1000/1000 - 0s - loss: 58.2803 - acc: 1.0000 - val_loss: 23.9363 - val_acc: 1.0000\n",
      "Epoch 230/500\n",
      "1000/1000 - 0s - loss: 57.1884 - acc: 1.0000 - val_loss: 23.1810 - val_acc: 1.0000\n",
      "Epoch 231/500\n",
      "1000/1000 - 0s - loss: 56.1147 - acc: 1.0000 - val_loss: 22.4517 - val_acc: 1.0000\n",
      "Epoch 232/500\n",
      "1000/1000 - 0s - loss: 55.0717 - acc: 1.0000 - val_loss: 21.7510 - val_acc: 1.0000\n",
      "Epoch 233/500\n",
      "1000/1000 - 0s - loss: 54.0617 - acc: 1.0000 - val_loss: 21.0614 - val_acc: 1.0000\n",
      "Epoch 234/500\n",
      "1000/1000 - 0s - loss: 53.0470 - acc: 1.0000 - val_loss: 20.3856 - val_acc: 1.0000\n",
      "Epoch 235/500\n",
      "1000/1000 - 0s - loss: 52.0681 - acc: 1.0000 - val_loss: 19.7354 - val_acc: 1.0000\n",
      "Epoch 236/500\n",
      "1000/1000 - 0s - loss: 51.1115 - acc: 1.0000 - val_loss: 19.1066 - val_acc: 1.0000\n",
      "Epoch 237/500\n",
      "1000/1000 - 0s - loss: 50.1764 - acc: 1.0000 - val_loss: 18.4810 - val_acc: 1.0000\n",
      "Epoch 238/500\n",
      "1000/1000 - 0s - loss: 49.2493 - acc: 1.0000 - val_loss: 17.8940 - val_acc: 1.0000\n",
      "Epoch 239/500\n",
      "1000/1000 - 0s - loss: 48.3546 - acc: 1.0000 - val_loss: 17.3138 - val_acc: 1.0000\n",
      "Epoch 240/500\n",
      "1000/1000 - 0s - loss: 47.4778 - acc: 1.0000 - val_loss: 16.7389 - val_acc: 1.0000\n",
      "Epoch 241/500\n",
      "1000/1000 - 0s - loss: 46.6139 - acc: 1.0000 - val_loss: 16.1931 - val_acc: 1.0000\n",
      "Epoch 242/500\n",
      "1000/1000 - 0s - loss: 45.7710 - acc: 1.0000 - val_loss: 15.6635 - val_acc: 1.0000\n",
      "Epoch 243/500\n",
      "1000/1000 - 0s - loss: 44.9589 - acc: 1.0000 - val_loss: 15.1469 - val_acc: 1.0000\n",
      "Epoch 244/500\n",
      "1000/1000 - 0s - loss: 44.1408 - acc: 1.0000 - val_loss: 14.6404 - val_acc: 1.0000\n",
      "Epoch 245/500\n",
      "1000/1000 - 0s - loss: 43.3611 - acc: 1.0000 - val_loss: 14.1563 - val_acc: 1.0000\n",
      "Epoch 246/500\n",
      "1000/1000 - 0s - loss: 42.5953 - acc: 1.0000 - val_loss: 13.6945 - val_acc: 1.0000\n",
      "Epoch 247/500\n",
      "1000/1000 - 0s - loss: 41.8463 - acc: 1.0000 - val_loss: 13.2358 - val_acc: 1.0000\n",
      "Epoch 248/500\n",
      "1000/1000 - 0s - loss: 41.1074 - acc: 1.0000 - val_loss: 12.7878 - val_acc: 1.0000\n",
      "Epoch 249/500\n",
      "1000/1000 - 0s - loss: 40.3832 - acc: 1.0000 - val_loss: 12.3690 - val_acc: 1.0000\n",
      "Epoch 250/500\n",
      "1000/1000 - 0s - loss: 39.6918 - acc: 1.0000 - val_loss: 11.9564 - val_acc: 1.0000\n",
      "Epoch 251/500\n",
      "1000/1000 - 0s - loss: 39.0118 - acc: 1.0000 - val_loss: 11.5674 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 252/500\n",
      "1000/1000 - 0s - loss: 38.3407 - acc: 1.0000 - val_loss: 11.1737 - val_acc: 1.0000\n",
      "Epoch 253/500\n",
      "1000/1000 - 0s - loss: 37.6897 - acc: 1.0000 - val_loss: 10.7930 - val_acc: 1.0000\n",
      "Epoch 254/500\n",
      "1000/1000 - 0s - loss: 37.0540 - acc: 1.0000 - val_loss: 10.4350 - val_acc: 1.0000\n",
      "Epoch 255/500\n",
      "1000/1000 - 0s - loss: 36.4225 - acc: 1.0000 - val_loss: 10.0815 - val_acc: 1.0000\n",
      "Epoch 256/500\n",
      "1000/1000 - 0s - loss: 35.8222 - acc: 1.0000 - val_loss: 9.7425 - val_acc: 1.0000\n",
      "Epoch 257/500\n",
      "1000/1000 - 0s - loss: 35.2262 - acc: 1.0000 - val_loss: 9.4247 - val_acc: 1.0000\n",
      "Epoch 258/500\n",
      "1000/1000 - 0s - loss: 34.6465 - acc: 1.0000 - val_loss: 9.1114 - val_acc: 1.0000\n",
      "Epoch 259/500\n",
      "1000/1000 - 0s - loss: 34.0840 - acc: 1.0000 - val_loss: 8.8124 - val_acc: 1.0000\n",
      "Epoch 260/500\n",
      "1000/1000 - 0s - loss: 33.5350 - acc: 1.0000 - val_loss: 8.5278 - val_acc: 1.0000\n",
      "Epoch 261/500\n",
      "1000/1000 - 0s - loss: 32.9987 - acc: 1.0000 - val_loss: 8.2379 - val_acc: 1.0000\n",
      "Epoch 262/500\n",
      "1000/1000 - 0s - loss: 32.4775 - acc: 1.0000 - val_loss: 7.9704 - val_acc: 1.0000\n",
      "Epoch 263/500\n",
      "1000/1000 - 0s - loss: 31.9653 - acc: 1.0000 - val_loss: 7.7132 - val_acc: 1.0000\n",
      "Epoch 264/500\n",
      "1000/1000 - 0s - loss: 31.4684 - acc: 1.0000 - val_loss: 7.4682 - val_acc: 1.0000\n",
      "Epoch 265/500\n",
      "1000/1000 - 0s - loss: 30.9813 - acc: 1.0000 - val_loss: 7.2250 - val_acc: 1.0000\n",
      "Epoch 266/500\n",
      "1000/1000 - 0s - loss: 30.5036 - acc: 1.0000 - val_loss: 6.9928 - val_acc: 1.0000\n",
      "Epoch 267/500\n",
      "1000/1000 - 0s - loss: 30.0463 - acc: 1.0000 - val_loss: 6.7817 - val_acc: 1.0000\n",
      "Epoch 268/500\n",
      "1000/1000 - 0s - loss: 29.5971 - acc: 1.0000 - val_loss: 6.5709 - val_acc: 1.0000\n",
      "Epoch 269/500\n",
      "1000/1000 - 0s - loss: 29.1619 - acc: 1.0000 - val_loss: 6.3689 - val_acc: 1.0000\n",
      "Epoch 270/500\n",
      "1000/1000 - 0s - loss: 28.7368 - acc: 1.0000 - val_loss: 6.1780 - val_acc: 1.0000\n",
      "Epoch 271/500\n",
      "1000/1000 - 0s - loss: 28.3176 - acc: 1.0000 - val_loss: 5.9860 - val_acc: 1.0000\n",
      "Epoch 272/500\n",
      "1000/1000 - 0s - loss: 27.9113 - acc: 1.0000 - val_loss: 5.8150 - val_acc: 1.0000\n",
      "Epoch 273/500\n",
      "1000/1000 - 0s - loss: 27.5175 - acc: 1.0000 - val_loss: 5.6541 - val_acc: 1.0000\n",
      "Epoch 274/500\n",
      "1000/1000 - 0s - loss: 27.1394 - acc: 1.0000 - val_loss: 5.4864 - val_acc: 1.0000\n",
      "Epoch 275/500\n",
      "1000/1000 - 0s - loss: 26.7611 - acc: 1.0000 - val_loss: 5.3342 - val_acc: 1.0000\n",
      "Epoch 276/500\n",
      "1000/1000 - 0s - loss: 26.4014 - acc: 1.0000 - val_loss: 5.1992 - val_acc: 1.0000\n",
      "Epoch 277/500\n",
      "1000/1000 - 0s - loss: 26.0507 - acc: 1.0000 - val_loss: 5.0592 - val_acc: 1.0000\n",
      "Epoch 278/500\n",
      "1000/1000 - 0s - loss: 25.7040 - acc: 1.0000 - val_loss: 4.9279 - val_acc: 1.0000\n",
      "Epoch 279/500\n",
      "1000/1000 - 0s - loss: 25.3700 - acc: 1.0000 - val_loss: 4.7999 - val_acc: 1.0000\n",
      "Epoch 280/500\n",
      "1000/1000 - 0s - loss: 25.0434 - acc: 1.0000 - val_loss: 4.6858 - val_acc: 1.0000\n",
      "Epoch 281/500\n",
      "1000/1000 - 0s - loss: 24.7284 - acc: 1.0000 - val_loss: 4.5735 - val_acc: 1.0000\n",
      "Epoch 282/500\n",
      "1000/1000 - 0s - loss: 24.4196 - acc: 1.0000 - val_loss: 4.4771 - val_acc: 1.0000\n",
      "Epoch 283/500\n",
      "1000/1000 - 0s - loss: 24.1248 - acc: 1.0000 - val_loss: 4.3804 - val_acc: 1.0000\n",
      "Epoch 284/500\n",
      "1000/1000 - 0s - loss: 23.8348 - acc: 1.0000 - val_loss: 4.2863 - val_acc: 1.0000\n",
      "Epoch 285/500\n",
      "1000/1000 - 0s - loss: 23.5495 - acc: 1.0000 - val_loss: 4.1931 - val_acc: 1.0000\n",
      "Epoch 286/500\n",
      "1000/1000 - 0s - loss: 23.2762 - acc: 1.0000 - val_loss: 4.1214 - val_acc: 1.0000\n",
      "Epoch 287/500\n",
      "1000/1000 - 0s - loss: 23.0057 - acc: 1.0000 - val_loss: 4.0484 - val_acc: 1.0000\n",
      "Epoch 288/500\n",
      "1000/1000 - 0s - loss: 22.7514 - acc: 1.0000 - val_loss: 3.9831 - val_acc: 1.0000\n",
      "Epoch 289/500\n",
      "1000/1000 - 0s - loss: 22.4991 - acc: 1.0000 - val_loss: 3.9087 - val_acc: 1.0000\n",
      "Epoch 290/500\n",
      "1000/1000 - 0s - loss: 22.2622 - acc: 1.0000 - val_loss: 3.8611 - val_acc: 1.0000\n",
      "Epoch 291/500\n",
      "1000/1000 - 0s - loss: 22.0179 - acc: 1.0000 - val_loss: 3.7972 - val_acc: 1.0000\n",
      "Epoch 292/500\n",
      "1000/1000 - 0s - loss: 21.7880 - acc: 1.0000 - val_loss: 3.7533 - val_acc: 1.0000\n",
      "Epoch 293/500\n",
      "1000/1000 - 0s - loss: 21.5639 - acc: 1.0000 - val_loss: 3.6979 - val_acc: 1.0000\n",
      "Epoch 294/500\n",
      "1000/1000 - 0s - loss: 21.3577 - acc: 1.0000 - val_loss: 3.6745 - val_acc: 1.0000\n",
      "Epoch 295/500\n",
      "1000/1000 - 0s - loss: 21.1392 - acc: 1.0000 - val_loss: 3.6242 - val_acc: 1.0000\n",
      "Epoch 296/500\n",
      "1000/1000 - 0s - loss: 20.9382 - acc: 1.0000 - val_loss: 3.5994 - val_acc: 1.0000\n",
      "Epoch 297/500\n",
      "1000/1000 - 0s - loss: 20.7373 - acc: 1.0000 - val_loss: 3.5699 - val_acc: 1.0000\n",
      "Epoch 298/500\n",
      "1000/1000 - 0s - loss: 20.5474 - acc: 1.0000 - val_loss: 3.5397 - val_acc: 1.0000\n",
      "Epoch 299/500\n",
      "1000/1000 - 0s - loss: 20.3593 - acc: 1.0000 - val_loss: 3.5199 - val_acc: 1.0000\n",
      "Epoch 300/500\n",
      "1000/1000 - 0s - loss: 20.1833 - acc: 1.0000 - val_loss: 3.5122 - val_acc: 1.0000\n",
      "Epoch 301/500\n",
      "1000/1000 - 0s - loss: 20.0065 - acc: 1.0000 - val_loss: 3.4874 - val_acc: 1.0000\n",
      "Epoch 302/500\n",
      "1000/1000 - 0s - loss: 19.8406 - acc: 1.0000 - val_loss: 3.4773 - val_acc: 1.0000\n",
      "Epoch 303/500\n",
      "1000/1000 - 0s - loss: 19.6727 - acc: 1.0000 - val_loss: 3.4683 - val_acc: 1.0000\n",
      "Epoch 304/500\n",
      "1000/1000 - 0s - loss: 19.5160 - acc: 1.0000 - val_loss: 3.4734 - val_acc: 1.0000\n",
      "Epoch 305/500\n",
      "1000/1000 - 0s - loss: 19.3632 - acc: 1.0000 - val_loss: 3.4665 - val_acc: 1.0000\n",
      "Epoch 306/500\n",
      "1000/1000 - 0s - loss: 19.2158 - acc: 1.0000 - val_loss: 3.4693 - val_acc: 1.0000\n",
      "Epoch 307/500\n",
      "1000/1000 - 0s - loss: 19.0700 - acc: 1.0000 - val_loss: 3.4667 - val_acc: 1.0000\n",
      "Epoch 308/500\n",
      "1000/1000 - 0s - loss: 18.9276 - acc: 1.0000 - val_loss: 3.4765 - val_acc: 1.0000\n",
      "Epoch 309/500\n",
      "1000/1000 - 0s - loss: 18.7968 - acc: 1.0000 - val_loss: 3.4834 - val_acc: 1.0000\n",
      "Epoch 310/500\n",
      "1000/1000 - 0s - loss: 18.6649 - acc: 1.0000 - val_loss: 3.4936 - val_acc: 1.0000\n",
      "Epoch 311/500\n",
      "1000/1000 - 0s - loss: 18.5411 - acc: 1.0000 - val_loss: 3.5099 - val_acc: 1.0000\n",
      "Epoch 312/500\n",
      "1000/1000 - 0s - loss: 18.4180 - acc: 1.0000 - val_loss: 3.5272 - val_acc: 1.0000\n",
      "Epoch 313/500\n",
      "1000/1000 - 0s - loss: 18.2971 - acc: 1.0000 - val_loss: 3.5397 - val_acc: 1.0000\n",
      "Epoch 314/500\n",
      "1000/1000 - 0s - loss: 18.1855 - acc: 1.0000 - val_loss: 3.5625 - val_acc: 1.0000\n",
      "Epoch 315/500\n",
      "1000/1000 - 0s - loss: 18.0767 - acc: 1.0000 - val_loss: 3.5784 - val_acc: 1.0000\n",
      "Epoch 316/500\n",
      "1000/1000 - 0s - loss: 17.9675 - acc: 1.0000 - val_loss: 3.5985 - val_acc: 1.0000\n",
      "Epoch 317/500\n",
      "1000/1000 - 0s - loss: 17.8658 - acc: 1.0000 - val_loss: 3.6273 - val_acc: 1.0000\n",
      "Epoch 318/500\n",
      "1000/1000 - 0s - loss: 17.7724 - acc: 1.0000 - val_loss: 3.6703 - val_acc: 1.0000\n",
      "Epoch 319/500\n",
      "1000/1000 - 0s - loss: 17.6718 - acc: 1.0000 - val_loss: 3.6756 - val_acc: 1.0000\n",
      "Epoch 320/500\n",
      "1000/1000 - 0s - loss: 17.5785 - acc: 1.0000 - val_loss: 3.7074 - val_acc: 1.0000\n",
      "Epoch 321/500\n",
      "1000/1000 - 0s - loss: 17.4879 - acc: 1.0000 - val_loss: 3.7405 - val_acc: 1.0000\n",
      "Epoch 322/500\n",
      "1000/1000 - 0s - loss: 17.4024 - acc: 1.0000 - val_loss: 3.7737 - val_acc: 1.0000\n",
      "Epoch 323/500\n",
      "1000/1000 - 0s - loss: 17.3186 - acc: 1.0000 - val_loss: 3.8048 - val_acc: 1.0000\n",
      "Epoch 324/500\n",
      "1000/1000 - 0s - loss: 17.2398 - acc: 1.0000 - val_loss: 3.8406 - val_acc: 1.0000\n",
      "Epoch 325/500\n",
      "1000/1000 - 0s - loss: 17.1598 - acc: 1.0000 - val_loss: 3.8708 - val_acc: 1.0000\n",
      "Epoch 326/500\n",
      "1000/1000 - 0s - loss: 17.0862 - acc: 1.0000 - val_loss: 3.9113 - val_acc: 1.0000\n",
      "Epoch 327/500\n",
      "1000/1000 - 0s - loss: 17.0151 - acc: 1.0000 - val_loss: 3.9508 - val_acc: 1.0000\n",
      "Epoch 328/500\n",
      "1000/1000 - 0s - loss: 16.9461 - acc: 1.0000 - val_loss: 3.9882 - val_acc: 1.0000\n",
      "Epoch 329/500\n",
      "1000/1000 - 0s - loss: 16.8751 - acc: 1.0000 - val_loss: 4.0194 - val_acc: 1.0000\n",
      "Epoch 330/500\n",
      "1000/1000 - 0s - loss: 16.8127 - acc: 1.0000 - val_loss: 4.0545 - val_acc: 1.0000\n",
      "Epoch 331/500\n",
      "1000/1000 - 0s - loss: 16.7483 - acc: 1.0000 - val_loss: 4.1001 - val_acc: 1.0000\n",
      "Epoch 332/500\n",
      "1000/1000 - 0s - loss: 16.6901 - acc: 1.0000 - val_loss: 4.1412 - val_acc: 1.0000\n",
      "Epoch 333/500\n",
      "1000/1000 - 0s - loss: 16.6324 - acc: 1.0000 - val_loss: 4.1824 - val_acc: 1.0000\n",
      "Epoch 334/500\n",
      "1000/1000 - 0s - loss: 16.5814 - acc: 1.0000 - val_loss: 4.2172 - val_acc: 1.0000\n",
      "Epoch 335/500\n",
      "1000/1000 - 0s - loss: 16.5229 - acc: 1.0000 - val_loss: 4.2680 - val_acc: 1.0000\n",
      "Epoch 336/500\n",
      "1000/1000 - 0s - loss: 16.4714 - acc: 1.0000 - val_loss: 4.3102 - val_acc: 1.0000\n",
      "Epoch 337/500\n",
      "1000/1000 - 0s - loss: 16.4249 - acc: 1.0000 - val_loss: 4.3426 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/500\n",
      "1000/1000 - 0s - loss: 16.3769 - acc: 1.0000 - val_loss: 4.3977 - val_acc: 1.0000\n",
      "Epoch 339/500\n",
      "1000/1000 - 0s - loss: 16.3284 - acc: 1.0000 - val_loss: 4.4373 - val_acc: 1.0000\n",
      "Epoch 340/500\n",
      "1000/1000 - 0s - loss: 16.2833 - acc: 1.0000 - val_loss: 4.4713 - val_acc: 1.0000\n",
      "Epoch 341/500\n",
      "1000/1000 - 0s - loss: 16.2423 - acc: 1.0000 - val_loss: 4.5170 - val_acc: 1.0000\n",
      "Epoch 342/500\n",
      "1000/1000 - 0s - loss: 16.1936 - acc: 1.0000 - val_loss: 4.4921 - val_acc: 1.0000\n",
      "Epoch 343/500\n",
      "1000/1000 - 0s - loss: 16.5056 - acc: 1.0000 - val_loss: 4.6363 - val_acc: 1.0000\n",
      "Epoch 344/500\n",
      "1000/1000 - 0s - loss: 16.1395 - acc: 1.0000 - val_loss: 4.6914 - val_acc: 1.0000\n",
      "Epoch 345/500\n",
      "1000/1000 - 0s - loss: 16.1115 - acc: 1.0000 - val_loss: 4.7370 - val_acc: 1.0000\n",
      "Epoch 346/500\n",
      "1000/1000 - 0s - loss: 16.0808 - acc: 1.0000 - val_loss: 4.7938 - val_acc: 1.0000\n",
      "Epoch 347/500\n",
      "1000/1000 - 0s - loss: 16.0429 - acc: 1.0000 - val_loss: 4.8248 - val_acc: 1.0000\n",
      "Epoch 348/500\n",
      "1000/1000 - 0s - loss: 16.0096 - acc: 1.0000 - val_loss: 4.8648 - val_acc: 1.0000\n",
      "Epoch 349/500\n",
      "1000/1000 - 0s - loss: 15.9745 - acc: 1.0000 - val_loss: 4.9090 - val_acc: 1.0000\n",
      "Epoch 350/500\n",
      "1000/1000 - 0s - loss: 15.9420 - acc: 1.0000 - val_loss: 4.9463 - val_acc: 1.0000\n",
      "Epoch 351/500\n",
      "1000/1000 - 0s - loss: 15.9147 - acc: 1.0000 - val_loss: 4.9944 - val_acc: 1.0000\n",
      "Epoch 352/500\n",
      "1000/1000 - 0s - loss: 15.8822 - acc: 1.0000 - val_loss: 5.0187 - val_acc: 1.0000\n",
      "Epoch 353/500\n",
      "1000/1000 - 0s - loss: 15.8566 - acc: 1.0000 - val_loss: 5.0622 - val_acc: 1.0000\n",
      "Epoch 354/500\n",
      "1000/1000 - 0s - loss: 15.8294 - acc: 1.0000 - val_loss: 5.1087 - val_acc: 1.0000\n",
      "Epoch 355/500\n",
      "1000/1000 - 0s - loss: 15.8045 - acc: 1.0000 - val_loss: 5.1519 - val_acc: 1.0000\n",
      "Epoch 356/500\n",
      "1000/1000 - 0s - loss: 15.7800 - acc: 1.0000 - val_loss: 5.1892 - val_acc: 1.0000\n",
      "Epoch 357/500\n",
      "1000/1000 - 0s - loss: 15.7580 - acc: 1.0000 - val_loss: 5.2270 - val_acc: 1.0000\n",
      "Epoch 358/500\n",
      "1000/1000 - 0s - loss: 15.7355 - acc: 1.0000 - val_loss: 5.2686 - val_acc: 1.0000\n",
      "Epoch 359/500\n",
      "1000/1000 - 0s - loss: 15.7160 - acc: 1.0000 - val_loss: 5.3031 - val_acc: 1.0000\n",
      "Epoch 360/500\n",
      "1000/1000 - 0s - loss: 15.6980 - acc: 1.0000 - val_loss: 5.3527 - val_acc: 1.0000\n",
      "Epoch 361/500\n",
      "1000/1000 - 0s - loss: 15.6761 - acc: 1.0000 - val_loss: 5.3861 - val_acc: 1.0000\n",
      "Epoch 362/500\n",
      "1000/1000 - 0s - loss: 15.6577 - acc: 1.0000 - val_loss: 5.4161 - val_acc: 1.0000\n",
      "Epoch 363/500\n",
      "1000/1000 - 0s - loss: 15.6404 - acc: 1.0000 - val_loss: 5.4561 - val_acc: 1.0000\n",
      "Epoch 364/500\n",
      "1000/1000 - 0s - loss: 15.6244 - acc: 1.0000 - val_loss: 5.5044 - val_acc: 1.0000\n",
      "Epoch 365/500\n",
      "1000/1000 - 0s - loss: 15.6082 - acc: 1.0000 - val_loss: 5.5317 - val_acc: 1.0000\n",
      "Epoch 366/500\n",
      "1000/1000 - 0s - loss: 15.5922 - acc: 1.0000 - val_loss: 5.5731 - val_acc: 1.0000\n",
      "Epoch 367/500\n",
      "1000/1000 - 0s - loss: 15.5797 - acc: 1.0000 - val_loss: 5.6155 - val_acc: 1.0000\n",
      "Epoch 368/500\n",
      "1000/1000 - 0s - loss: 15.5654 - acc: 1.0000 - val_loss: 5.6373 - val_acc: 1.0000\n",
      "Epoch 369/500\n",
      "1000/1000 - 0s - loss: 15.5486 - acc: 1.0000 - val_loss: 5.6742 - val_acc: 1.0000\n",
      "Epoch 370/500\n",
      "1000/1000 - 0s - loss: 15.5358 - acc: 1.0000 - val_loss: 5.7147 - val_acc: 1.0000\n",
      "Epoch 371/500\n",
      "1000/1000 - 0s - loss: 15.5236 - acc: 1.0000 - val_loss: 5.7474 - val_acc: 1.0000\n",
      "Epoch 372/500\n",
      "1000/1000 - 0s - loss: 15.5114 - acc: 1.0000 - val_loss: 5.7821 - val_acc: 1.0000\n",
      "Epoch 373/500\n",
      "1000/1000 - 0s - loss: 15.4997 - acc: 1.0000 - val_loss: 5.8136 - val_acc: 1.0000\n",
      "Epoch 374/500\n",
      "1000/1000 - 0s - loss: 15.4888 - acc: 1.0000 - val_loss: 5.8479 - val_acc: 1.0000\n",
      "Epoch 375/500\n",
      "1000/1000 - 0s - loss: 15.4773 - acc: 1.0000 - val_loss: 5.8658 - val_acc: 1.0000\n",
      "Epoch 376/500\n",
      "1000/1000 - 0s - loss: 15.4646 - acc: 1.0000 - val_loss: 5.8890 - val_acc: 1.0000\n",
      "Epoch 377/500\n",
      "1000/1000 - 0s - loss: 15.4511 - acc: 1.0000 - val_loss: 5.8889 - val_acc: 1.0000\n",
      "Epoch 378/500\n",
      "1000/1000 - 0s - loss: 15.4277 - acc: 1.0000 - val_loss: 5.7051 - val_acc: 1.0000\n",
      "Epoch 379/500\n",
      "1000/1000 - 0s - loss: 14.9473 - acc: 1.0000 - val_loss: 4.8962 - val_acc: 1.0000\n",
      "Epoch 380/500\n",
      "1000/1000 - 0s - loss: 14.3923 - acc: 1.0000 - val_loss: 3.0750 - val_acc: 1.0000\n",
      "Epoch 381/500\n",
      "1000/1000 - 0s - loss: 13.2920 - acc: 1.0000 - val_loss: 5.1650 - val_acc: 1.0000\n",
      "Epoch 382/500\n",
      "1000/1000 - 0s - loss: 12.6076 - acc: 1.0000 - val_loss: 4.4722 - val_acc: 1.0000\n",
      "Epoch 383/500\n",
      "1000/1000 - 0s - loss: 12.3328 - acc: 1.0000 - val_loss: 5.3009 - val_acc: 1.0000\n",
      "Epoch 384/500\n",
      "1000/1000 - 0s - loss: 12.1474 - acc: 1.0000 - val_loss: 4.5620 - val_acc: 1.0000\n",
      "Epoch 385/500\n",
      "1000/1000 - 0s - loss: 11.8693 - acc: 1.0000 - val_loss: 5.0689 - val_acc: 1.0000\n",
      "Epoch 386/500\n",
      "1000/1000 - 0s - loss: 11.7524 - acc: 1.0000 - val_loss: 4.8267 - val_acc: 1.0000\n",
      "Epoch 387/500\n",
      "1000/1000 - 0s - loss: 11.5930 - acc: 1.0000 - val_loss: 4.9766 - val_acc: 1.0000\n",
      "Epoch 388/500\n",
      "1000/1000 - 0s - loss: 11.4920 - acc: 1.0000 - val_loss: 4.8667 - val_acc: 1.0000\n",
      "Epoch 389/500\n",
      "1000/1000 - 0s - loss: 11.3384 - acc: 1.0000 - val_loss: 5.4989 - val_acc: 1.0000\n",
      "Epoch 390/500\n",
      "1000/1000 - 0s - loss: 11.2294 - acc: 1.0000 - val_loss: 5.1503 - val_acc: 1.0000\n",
      "Epoch 391/500\n",
      "1000/1000 - 0s - loss: 11.1533 - acc: 1.0000 - val_loss: 5.3608 - val_acc: 1.0000\n",
      "Epoch 392/500\n",
      "1000/1000 - 0s - loss: 11.0309 - acc: 1.0000 - val_loss: 5.8169 - val_acc: 1.0000\n",
      "Epoch 393/500\n",
      "1000/1000 - 0s - loss: 10.9260 - acc: 1.0000 - val_loss: 5.6935 - val_acc: 1.0000\n",
      "Epoch 394/500\n",
      "1000/1000 - 0s - loss: 10.8181 - acc: 1.0000 - val_loss: 5.6399 - val_acc: 1.0000\n",
      "Epoch 395/500\n",
      "1000/1000 - 0s - loss: 10.7646 - acc: 1.0000 - val_loss: 6.2399 - val_acc: 1.0000\n",
      "Epoch 396/500\n",
      "1000/1000 - 0s - loss: 10.6319 - acc: 1.0000 - val_loss: 5.5300 - val_acc: 1.0000\n",
      "Epoch 397/500\n",
      "1000/1000 - 0s - loss: 10.5070 - acc: 1.0000 - val_loss: 5.8214 - val_acc: 1.0000\n",
      "Epoch 398/500\n",
      "1000/1000 - 0s - loss: 10.4117 - acc: 1.0000 - val_loss: 5.4811 - val_acc: 1.0000\n",
      "Epoch 399/500\n",
      "1000/1000 - 0s - loss: 10.3376 - acc: 1.0000 - val_loss: 5.3487 - val_acc: 1.0000\n",
      "Epoch 400/500\n",
      "1000/1000 - 0s - loss: 10.2476 - acc: 1.0000 - val_loss: 5.7372 - val_acc: 1.0000\n",
      "Epoch 401/500\n",
      "1000/1000 - 0s - loss: 10.1364 - acc: 1.0000 - val_loss: 6.0602 - val_acc: 1.0000\n",
      "Epoch 402/500\n",
      "1000/1000 - 0s - loss: 10.0575 - acc: 1.0000 - val_loss: 5.6421 - val_acc: 1.0000\n",
      "Epoch 403/500\n",
      "1000/1000 - 0s - loss: 9.9325 - acc: 1.0000 - val_loss: 5.9079 - val_acc: 1.0000\n",
      "Epoch 404/500\n",
      "1000/1000 - 0s - loss: 9.8451 - acc: 1.0000 - val_loss: 6.1735 - val_acc: 1.0000\n",
      "Epoch 405/500\n",
      "1000/1000 - 0s - loss: 9.7610 - acc: 1.0000 - val_loss: 6.6370 - val_acc: 1.0000\n",
      "Epoch 406/500\n",
      "1000/1000 - 0s - loss: 9.7043 - acc: 1.0000 - val_loss: 6.4206 - val_acc: 1.0000\n",
      "Epoch 407/500\n",
      "1000/1000 - 0s - loss: 9.5602 - acc: 1.0000 - val_loss: 6.0864 - val_acc: 1.0000\n",
      "Epoch 408/500\n",
      "1000/1000 - 0s - loss: 9.4758 - acc: 1.0000 - val_loss: 6.7626 - val_acc: 1.0000\n",
      "Epoch 409/500\n",
      "1000/1000 - 0s - loss: 9.3860 - acc: 1.0000 - val_loss: 5.7331 - val_acc: 1.0000\n",
      "Epoch 410/500\n",
      "1000/1000 - 0s - loss: 9.3067 - acc: 1.0000 - val_loss: 6.0083 - val_acc: 1.0000\n",
      "Epoch 411/500\n",
      "1000/1000 - 0s - loss: 9.2214 - acc: 1.0000 - val_loss: 5.9133 - val_acc: 1.0000\n",
      "Epoch 412/500\n",
      "1000/1000 - 0s - loss: 9.1291 - acc: 1.0000 - val_loss: 6.6231 - val_acc: 1.0000\n",
      "Epoch 413/500\n",
      "1000/1000 - 0s - loss: 9.0242 - acc: 1.0000 - val_loss: 6.0142 - val_acc: 1.0000\n",
      "Epoch 414/500\n",
      "1000/1000 - 0s - loss: 8.9382 - acc: 1.0000 - val_loss: 6.1940 - val_acc: 1.0000\n",
      "Epoch 415/500\n",
      "1000/1000 - 0s - loss: 8.8369 - acc: 1.0000 - val_loss: 6.3776 - val_acc: 1.0000\n",
      "Epoch 416/500\n",
      "1000/1000 - 0s - loss: 8.7503 - acc: 1.0000 - val_loss: 5.8960 - val_acc: 1.0000\n",
      "Epoch 417/500\n",
      "1000/1000 - 0s - loss: 8.6524 - acc: 1.0000 - val_loss: 6.7504 - val_acc: 1.0000\n",
      "Epoch 418/500\n",
      "1000/1000 - 0s - loss: 8.5918 - acc: 1.0000 - val_loss: 5.2400 - val_acc: 1.0000\n",
      "Epoch 419/500\n",
      "1000/1000 - 0s - loss: 8.4960 - acc: 1.0000 - val_loss: 6.3499 - val_acc: 1.0000\n",
      "Epoch 420/500\n",
      "1000/1000 - 0s - loss: 8.3859 - acc: 1.0000 - val_loss: 6.4813 - val_acc: 1.0000\n",
      "Epoch 421/500\n",
      "1000/1000 - 0s - loss: 8.3101 - acc: 1.0000 - val_loss: 5.8418 - val_acc: 1.0000\n",
      "Epoch 422/500\n",
      "1000/1000 - 0s - loss: 8.2189 - acc: 1.0000 - val_loss: 6.8013 - val_acc: 1.0000\n",
      "Epoch 423/500\n",
      "1000/1000 - 0s - loss: 8.1368 - acc: 1.0000 - val_loss: 5.9542 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 424/500\n",
      "1000/1000 - 0s - loss: 8.0698 - acc: 1.0000 - val_loss: 7.1858 - val_acc: 1.0000\n",
      "Epoch 425/500\n",
      "1000/1000 - 0s - loss: 8.0023 - acc: 1.0000 - val_loss: 5.7693 - val_acc: 1.0000\n",
      "Epoch 426/500\n",
      "1000/1000 - 0s - loss: 7.9373 - acc: 1.0000 - val_loss: 6.9199 - val_acc: 1.0000\n",
      "Epoch 427/500\n",
      "1000/1000 - 0s - loss: 7.8182 - acc: 1.0000 - val_loss: 5.9713 - val_acc: 1.0000\n",
      "Epoch 428/500\n",
      "1000/1000 - 0s - loss: 7.7451 - acc: 1.0000 - val_loss: 7.6220 - val_acc: 1.0000\n",
      "Epoch 429/500\n",
      "1000/1000 - 0s - loss: 7.6971 - acc: 1.0000 - val_loss: 5.9120 - val_acc: 1.0000\n",
      "Epoch 430/500\n",
      "1000/1000 - 0s - loss: 7.6050 - acc: 1.0000 - val_loss: 6.5503 - val_acc: 1.0000\n",
      "Epoch 431/500\n",
      "1000/1000 - 0s - loss: 7.5192 - acc: 1.0000 - val_loss: 7.7880 - val_acc: 1.0000\n",
      "Epoch 432/500\n",
      "1000/1000 - 0s - loss: 7.4636 - acc: 1.0000 - val_loss: 6.5891 - val_acc: 1.0000\n",
      "Epoch 433/500\n",
      "1000/1000 - 0s - loss: 7.3866 - acc: 1.0000 - val_loss: 6.3086 - val_acc: 1.0000\n",
      "Epoch 434/500\n",
      "1000/1000 - 0s - loss: 7.2920 - acc: 1.0000 - val_loss: 7.2210 - val_acc: 1.0000\n",
      "Epoch 435/500\n",
      "1000/1000 - 0s - loss: 7.2240 - acc: 1.0000 - val_loss: 6.8773 - val_acc: 1.0000\n",
      "Epoch 436/500\n",
      "1000/1000 - 0s - loss: 7.1587 - acc: 1.0000 - val_loss: 6.9179 - val_acc: 1.0000\n",
      "Epoch 437/500\n",
      "1000/1000 - 0s - loss: 7.0950 - acc: 1.0000 - val_loss: 6.5122 - val_acc: 1.0000\n",
      "Epoch 438/500\n",
      "1000/1000 - 0s - loss: 7.0434 - acc: 1.0000 - val_loss: 6.7492 - val_acc: 1.0000\n",
      "Epoch 439/500\n",
      "1000/1000 - 0s - loss: 6.9765 - acc: 1.0000 - val_loss: 7.4399 - val_acc: 1.0000\n",
      "Epoch 440/500\n",
      "1000/1000 - 0s - loss: 6.9351 - acc: 1.0000 - val_loss: 6.6118 - val_acc: 1.0000\n",
      "Epoch 441/500\n",
      "1000/1000 - 0s - loss: 6.8724 - acc: 1.0000 - val_loss: 6.2918 - val_acc: 1.0000\n",
      "Epoch 442/500\n",
      "1000/1000 - 0s - loss: 6.7869 - acc: 1.0000 - val_loss: 6.9609 - val_acc: 1.0000\n",
      "Epoch 443/500\n",
      "1000/1000 - 0s - loss: 6.7311 - acc: 1.0000 - val_loss: 7.9021 - val_acc: 1.0000\n",
      "Epoch 444/500\n",
      "1000/1000 - 0s - loss: 6.6739 - acc: 1.0000 - val_loss: 7.3226 - val_acc: 1.0000\n",
      "Epoch 445/500\n",
      "1000/1000 - 0s - loss: 6.6123 - acc: 1.0000 - val_loss: 6.7872 - val_acc: 1.0000\n",
      "Epoch 446/500\n",
      "1000/1000 - 0s - loss: 6.5633 - acc: 1.0000 - val_loss: 7.5357 - val_acc: 1.0000\n",
      "Epoch 447/500\n",
      "1000/1000 - 0s - loss: 6.5050 - acc: 1.0000 - val_loss: 7.1532 - val_acc: 1.0000\n",
      "Epoch 448/500\n",
      "1000/1000 - 0s - loss: 6.4592 - acc: 1.0000 - val_loss: 7.5776 - val_acc: 1.0000\n",
      "Epoch 449/500\n",
      "1000/1000 - 0s - loss: 6.4135 - acc: 1.0000 - val_loss: 7.2653 - val_acc: 1.0000\n",
      "Epoch 450/500\n",
      "1000/1000 - 0s - loss: 6.3669 - acc: 1.0000 - val_loss: 6.6924 - val_acc: 1.0000\n",
      "Epoch 451/500\n",
      "1000/1000 - 0s - loss: 6.3020 - acc: 1.0000 - val_loss: 7.8128 - val_acc: 1.0000\n",
      "Epoch 452/500\n",
      "1000/1000 - 0s - loss: 6.2307 - acc: 1.0000 - val_loss: 6.8044 - val_acc: 1.0000\n",
      "Epoch 453/500\n",
      "1000/1000 - 0s - loss: 6.1762 - acc: 1.0000 - val_loss: 7.6827 - val_acc: 1.0000\n",
      "Epoch 454/500\n",
      "1000/1000 - 0s - loss: 6.1235 - acc: 1.0000 - val_loss: 7.3283 - val_acc: 1.0000\n",
      "Epoch 455/500\n",
      "1000/1000 - 0s - loss: 6.0727 - acc: 1.0000 - val_loss: 7.3798 - val_acc: 1.0000\n",
      "Epoch 456/500\n",
      "1000/1000 - 0s - loss: 6.0189 - acc: 1.0000 - val_loss: 7.7885 - val_acc: 1.0000\n",
      "Epoch 457/500\n",
      "1000/1000 - 0s - loss: 6.0114 - acc: 1.0000 - val_loss: 7.3416 - val_acc: 1.0000\n",
      "Epoch 458/500\n",
      "1000/1000 - 0s - loss: 5.9681 - acc: 1.0000 - val_loss: 7.0871 - val_acc: 1.0000\n",
      "Epoch 459/500\n",
      "1000/1000 - 0s - loss: 5.8898 - acc: 1.0000 - val_loss: 8.2417 - val_acc: 1.0000\n",
      "Epoch 460/500\n",
      "1000/1000 - 0s - loss: 5.8281 - acc: 1.0000 - val_loss: 7.8405 - val_acc: 1.0000\n",
      "Epoch 461/500\n",
      "1000/1000 - 0s - loss: 5.7811 - acc: 1.0000 - val_loss: 7.9117 - val_acc: 1.0000\n",
      "Epoch 462/500\n",
      "1000/1000 - 0s - loss: 5.7311 - acc: 1.0000 - val_loss: 7.2664 - val_acc: 1.0000\n",
      "Epoch 463/500\n",
      "1000/1000 - 0s - loss: 5.6824 - acc: 1.0000 - val_loss: 7.7863 - val_acc: 1.0000\n",
      "Epoch 464/500\n",
      "1000/1000 - 0s - loss: 5.6631 - acc: 1.0000 - val_loss: 6.8606 - val_acc: 1.0000\n",
      "Epoch 465/500\n",
      "1000/1000 - 0s - loss: 5.6622 - acc: 1.0000 - val_loss: 8.0677 - val_acc: 1.0000\n",
      "Epoch 466/500\n",
      "1000/1000 - 0s - loss: 5.6354 - acc: 1.0000 - val_loss: 7.9960 - val_acc: 1.0000\n",
      "Epoch 467/500\n",
      "1000/1000 - 0s - loss: 5.5659 - acc: 1.0000 - val_loss: 6.4769 - val_acc: 1.0000\n",
      "Epoch 468/500\n",
      "1000/1000 - 0s - loss: 5.4776 - acc: 1.0000 - val_loss: 8.5217 - val_acc: 1.0000\n",
      "Epoch 469/500\n",
      "1000/1000 - 0s - loss: 5.4518 - acc: 1.0000 - val_loss: 6.2860 - val_acc: 1.0000\n",
      "Epoch 470/500\n",
      "1000/1000 - 0s - loss: 5.3884 - acc: 1.0000 - val_loss: 7.9114 - val_acc: 1.0000\n",
      "Epoch 471/500\n",
      "1000/1000 - 0s - loss: 5.3154 - acc: 1.0000 - val_loss: 6.7219 - val_acc: 1.0000\n",
      "Epoch 472/500\n",
      "1000/1000 - 0s - loss: 5.2870 - acc: 1.0000 - val_loss: 7.3488 - val_acc: 1.0000\n",
      "Epoch 473/500\n",
      "1000/1000 - 0s - loss: 5.2399 - acc: 1.0000 - val_loss: 7.6201 - val_acc: 1.0000\n",
      "Epoch 474/500\n",
      "1000/1000 - 0s - loss: 5.2020 - acc: 1.0000 - val_loss: 7.3242 - val_acc: 1.0000\n",
      "Epoch 475/500\n",
      "1000/1000 - 0s - loss: 5.1535 - acc: 1.0000 - val_loss: 7.6251 - val_acc: 1.0000\n",
      "Epoch 476/500\n",
      "1000/1000 - 0s - loss: 5.1084 - acc: 1.0000 - val_loss: 6.7381 - val_acc: 1.0000\n",
      "Epoch 477/500\n",
      "1000/1000 - 0s - loss: 5.0784 - acc: 1.0000 - val_loss: 6.7518 - val_acc: 1.0000\n",
      "Epoch 478/500\n",
      "1000/1000 - 0s - loss: 5.0403 - acc: 1.0000 - val_loss: 7.1580 - val_acc: 1.0000\n",
      "Epoch 479/500\n",
      "1000/1000 - 0s - loss: 4.9832 - acc: 1.0000 - val_loss: 6.1241 - val_acc: 1.0000\n",
      "Epoch 480/500\n",
      "1000/1000 - 0s - loss: 4.9703 - acc: 1.0000 - val_loss: 6.9272 - val_acc: 1.0000\n",
      "Epoch 481/500\n",
      "1000/1000 - 0s - loss: 4.9150 - acc: 1.0000 - val_loss: 7.1635 - val_acc: 1.0000\n",
      "Epoch 482/500\n",
      "1000/1000 - 0s - loss: 4.9137 - acc: 1.0000 - val_loss: 8.0194 - val_acc: 1.0000\n",
      "Epoch 483/500\n",
      "1000/1000 - 0s - loss: 4.8698 - acc: 1.0000 - val_loss: 6.4097 - val_acc: 1.0000\n",
      "Epoch 484/500\n",
      "1000/1000 - 0s - loss: 4.8352 - acc: 1.0000 - val_loss: 6.3408 - val_acc: 1.0000\n",
      "Epoch 485/500\n",
      "1000/1000 - 0s - loss: 4.7795 - acc: 1.0000 - val_loss: 6.7950 - val_acc: 1.0000\n",
      "Epoch 486/500\n",
      "1000/1000 - 0s - loss: 4.7260 - acc: 1.0000 - val_loss: 7.4791 - val_acc: 1.0000\n",
      "Epoch 487/500\n",
      "1000/1000 - 0s - loss: 4.6955 - acc: 1.0000 - val_loss: 5.9063 - val_acc: 1.0000\n",
      "Epoch 488/500\n",
      "1000/1000 - 0s - loss: 4.6651 - acc: 1.0000 - val_loss: 6.5756 - val_acc: 1.0000\n",
      "Epoch 489/500\n",
      "1000/1000 - 0s - loss: 4.6325 - acc: 1.0000 - val_loss: 7.7009 - val_acc: 1.0000\n",
      "Epoch 490/500\n",
      "1000/1000 - 0s - loss: 4.6426 - acc: 1.0000 - val_loss: 5.4622 - val_acc: 1.0000\n",
      "Epoch 491/500\n",
      "1000/1000 - 0s - loss: 4.6062 - acc: 1.0000 - val_loss: 7.9125 - val_acc: 1.0000\n",
      "Epoch 492/500\n",
      "1000/1000 - 0s - loss: 4.5399 - acc: 1.0000 - val_loss: 6.6849 - val_acc: 1.0000\n",
      "Epoch 493/500\n",
      "1000/1000 - 0s - loss: 4.5014 - acc: 1.0000 - val_loss: 5.7563 - val_acc: 1.0000\n",
      "Epoch 494/500\n",
      "1000/1000 - 0s - loss: 4.4525 - acc: 1.0000 - val_loss: 6.7988 - val_acc: 1.0000\n",
      "Epoch 495/500\n",
      "1000/1000 - 0s - loss: 4.3925 - acc: 1.0000 - val_loss: 8.0743 - val_acc: 1.0000\n",
      "Epoch 496/500\n",
      "1000/1000 - 0s - loss: 4.3767 - acc: 1.0000 - val_loss: 6.2470 - val_acc: 1.0000\n",
      "Epoch 497/500\n",
      "1000/1000 - 0s - loss: 4.3481 - acc: 1.0000 - val_loss: 7.0209 - val_acc: 1.0000\n",
      "Epoch 498/500\n",
      "1000/1000 - 0s - loss: 4.3249 - acc: 1.0000 - val_loss: 7.6330 - val_acc: 1.0000\n",
      "Epoch 499/500\n",
      "1000/1000 - 0s - loss: 4.3026 - acc: 1.0000 - val_loss: 5.5828 - val_acc: 1.0000\n",
      "Epoch 500/500\n",
      "1000/1000 - 0s - loss: 4.2698 - acc: 1.0000 - val_loss: 5.5997 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "def get_model():\n",
    "    # Create a simple model.\n",
    "    model = tf.keras.Sequential()\n",
    "    dense = tf.keras.layers.Dense(200, input_shape= (8,) , activation=\"tanh\" , kernel_regularizer=l2(0.01))\n",
    "    model.add(dense)  # , input_shape=(train_X.shape[1], train_X.shape[2]))\n",
    "    dense = tf.keras.layers.Dense(50, activation = \"tanh\" , kernel_regularizer=l2(0.01))\n",
    "    model.add(dense) \n",
    "    model.add(tf.keras.layers.Dense(5))\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\",  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# Train the model.\n",
    "train_input = Xtrain\n",
    "train_target = Ytrain\n",
    "history = model.fit(train_input, train_target,  epochs=500, batch_size=100, \n",
    "              shuffle=True, validation_data=(Xtest, Ytest) , verbose=2\n",
    "                   )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T09:55:28.262737Z",
     "start_time": "2021-01-19T09:55:28.251796Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "hidden_layer_weights1  = weights[0]\n",
    "hidden_layer_bias1  = weights[1]\n",
    "hidden_layer_weights2 = weights[2]\n",
    "hidden_layer_bias2 = weights[3]\n",
    "output_layer_weights = weights[4]\n",
    "output_layer_bias = weights[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T09:55:40.036300Z",
     "start_time": "2021-01-19T09:55:40.018024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-19T07:30:15.659766Z",
     "start_time": "2021-01-19T07:30:15.538046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "from tensorflow.keras.models import model_from_json\n",
    "model_json = model.to_json()\n",
    "with open(\"model_next_simple.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_next_simple.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T10:44:55.718047Z",
     "start_time": "2021-01-18T10:44:55.494808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/prajwal-thakur/anaconda3/envs/vtol_project/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "json_file = open('model_next_simple.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_next_simple.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(optimizer=\"adam\", loss=\"mean_squared_error\",  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T20:53:55.532580Z",
     "start_time": "2021-01-13T20:53:55.510072Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T20:54:24.541643Z",
     "start_time": "2021-01-13T20:54:24.470872Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "loaded_model.predict(Xtrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:03:52.129473Z",
     "start_time": "2021-01-17T21:03:51.538215Z"
    }
   },
   "outputs": [],
   "source": [
    "from casadi import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:16:31.282894Z",
     "start_time": "2021-01-17T21:16:31.270512Z"
    }
   },
   "outputs": [],
   "source": [
    "opti = Opti()  # Optimization problem\n",
    "        # ---- decision variables ---------\n",
    "nout = 2  # outputs variables\n",
    "states = 2\n",
    "control_variables = 3\n",
    "X = opti.variable(states, 14 + 1)  # state trajectory\n",
    "U = opti.variable(states, 14 + 1)  # state trajectory\n",
    "x_nom = np.array([54,87])\n",
    "N=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:05:58.268088Z",
     "start_time": "2021-01-17T21:05:58.242129Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'MX' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a8b610e4a2a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m98\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'MX' and 'list'"
     ]
    }
   ],
   "source": [
    "X[:,1]-[32,98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:10:25.611966Z",
     "start_time": "2021-01-17T21:10:25.579404Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'DM' and 'MX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-0553758bcd2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mrepmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_nom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'DM' and 'MX'"
     ]
    }
   ],
   "source": [
    "x =  repmat(x_nom,1,N+1) + opti.variable(3,N+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:14:03.888321Z",
     "start_time": "2021-01-17T21:14:03.874217Z"
    }
   },
   "outputs": [],
   "source": [
    "A_lon = np.array([[-0.0475, 0.2383, -1.6000, -9.7900, 0],\n",
    "                       [-0.6016, -2.6981, 24.9000, -0.6264, 0.],\n",
    "                       [-0.0884, -0.5145, -0.8796, 0.0, 0.],\n",
    "                       [0., 0.0, 1.0, 0.0, 0.],\n",
    "                       [0.0639, -0.9980, 0.0, 25.000, 0.]])\n",
    "fxdot = lambda x, u: A_lon + x \n",
    "k1 = fxdot(X[:, 1], U[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:14:27.113269Z",
     "start_time": "2021-01-17T21:14:27.100789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MX((\n",
       "[[-0.0475, 0.2383, -1.6, -9.79, 0], \n",
       " [-0.6016, -2.6981, 24.9, -0.6264, 0], \n",
       " [-0.0884, -0.5145, -0.8796, 0, 0], \n",
       " [0, 0, 1, 0, 0], \n",
       " [0.0639, -0.998, 0, 25, 0]]+repmat(opti6_x_1[5:10], 5)))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_lon + X[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:17:26.910980Z",
     "start_time": "2021-01-17T21:17:26.903861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MX((([-0.0475, -0.6016]+opti7_x_1[2:4])/[34, 12]))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[-0.0475],\n",
    "                       [-0.6016]])\n",
    "(A + X[:,1])/np.array([[34],[12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-17T21:21:16.014178Z",
     "start_time": "2021-01-17T21:21:16.006825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MX((opti7_x_1[2:4]-[26.2094, 1.0793]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[ :, 1] - scaler_x.mean_[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.x_scale = np.array([3.31746745, 2.23230049, 0.45922519, 0.2943049, 7.69364428])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
